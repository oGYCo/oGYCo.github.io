---
layout: /src/layouts/MarkdownPostLayout.astro
title: KL散度
author: oGYCo
description: ""
image:
  url: ""
  alt: ""
pubDate: 2025-09-17
tags:
  [
    "machine learning"
  ]
languages: ["markdown"]
---

### **KL 散度 (Kullback-Leibler Divergence)**

#### **一、核心思想：KL散度是什么？**

KL散度，又称相对熵 (Relative Entropy)，是衡量**两个概率分布之间差异**的一种非对称度量。

> **核心问题**：如果我们用一个近似的概率分布 $Q$ 来描述一个真实的概率分布 $P$，我们需要付出多少“代价”或“损失多少信息”？

**通俗理解：信息编码的比喻**

想象一下，你负责为一个地区的每日天气（晴、阴、雨）设计一套二进制编码用于传输，目的是让平均传输成本（编码长度）最低。

1.  **真实分布 P (最优策略)**：你通过长期的气象数据得知，真实的天气概率分布是：
    * P(晴) = 70%
    * P(阴) = 20%
    * P(雨) = 10%

    根据信息论，最优编码是为高频事件分配短编码，低频事件分配长编码。例如：
    * 晴 → `0`
    * 阴 → `10`
    * 雨 → `11`
    这种编码方案下的平均编码长度是**理论最短的**，我们称之为该分布的**熵 (Entropy)**。

2.  **近似分布 Q (你的猜测)**：你没有历史数据，想当然地认为这三种天气等可能发生：
    * Q(晴) = 33.3%
    * Q(阴) = 33.3%
    * Q(雨) = 33.3%

    基于这个错误的假设，你可能会设计出这样的编码：
    * 晴 → `0`
    * 阴 → `10`
    * 雨 → `11` (也许是 `00`, `01`, `10`，但为了对比，我们用一个非最优但基于Q的方案)

**KL散度就是**：
> 用你那套基于错误假设 Q 的编码方案，去传输遵循真实分布 P 的天气信息时，所导致的**平均编码长度的增加量**。

这个“增加量”就是信息损失，是衡量你的猜测 Q 与真实 P 之间差异的量化指标。
* 如果你的猜测 Q 和真实 P 完全一样，那么编码长度没有增加，KL散度为 0。
* 如果你的猜测 Q 和真实 P 差异巨大，那么编码长度的浪费就会很严重，KL散度值就很大。

---

#### **二、数学定义与计算**

**1. 公式定义**

对于离散概率分布 P 和 Q，KL散度的定义如下：

$$D_{KL}(P || Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right)$$

* $P(i)$: 真实分布中事件 $i$ 的概率（作为权重）。
* $\log \left( \frac{P(i)}{Q(i)} \right)$: 在事件 $i$ 上，两个分布概率比值的对数，代表了在该点上的信息差异。

对于连续概率分布（概率密度函数为 $p(x)$ 和 $q(x)$），则用积分表示：

$$D_{KL}(P || Q) = \int p(x) \log \left( \frac{p(x)}{q(x)} \right) dx$$

**2. 计算示例**

假设一个骰子的真实投掷概率 P (不均匀) 和我们的猜测 Q (均匀) 如下：

| 点数 (i) | 真实分布 P(i) | 猜测分布 Q(i) |
| :------: | :-----------: | :-----------: |
|    1     |      0.5      |      1/6      |
|    2     |      0.1      |      1/6      |
|    3     |      0.1      |      1/6      |
|    4     |      0.1      |      1/6      |
|    5     |      0.1      |      1/6      |
|    6     |      0.1      |      1/6      |

我们来计算 $D_{KL}(P || Q)$ (使用自然对数 $\ln$)：

$D_{KL}(P || Q) = \underbrace{0.5 \ln\left(\frac{0.5}{1/6}\right)}_{\text{点数1的差异}} + \underbrace{0.1 \ln\left(\frac{0.1}{1/6}\right)}_{\text{点数2的差异}} + \dots + \underbrace{0.1 \ln\left(\frac{0.1}{1/6}\right)}_{\text{点数6的差异}}$

$= 0.5 \ln(3) + 5 \times 0.1 \ln(0.6)$
$\approx 0.5 \times 1.0986 + 0.5 \times (-0.5108)$
$\approx 0.5493 - 0.2554 = 0.2939$

这个值 `0.2939` (单位 nats) 就是用 Q 近似 P 所带来的信息损失。

---

#### **三、从信息论推导 KL 散度**

KL散度的公式不是凭空出现的，它与信息论中的**熵**和**交叉熵**密切相关。

**Step 1: 熵 (Entropy)**

熵 $H(P)$ 衡量了概率分布 P 的不确定性，也是对其进行**最优编码**所需的**平均信息量**（理论最短编码长度）。

$$H(P) = -\sum_i P(i) \log P(i)$$

**Step 2: 交叉熵 (Cross-Entropy)**

交叉熵 $H(P, Q)$ 衡量了使用基于**错误分布 Q 的编码方案**，来编码来自**真实分布 P 的事件**时，所需要的**平均信息量**。

$$H(P, Q) = -\sum_i P(i) \log Q(i)$$

由于 Q 是一个次优的近似，所以用它的编码方案必然会导致平均编码长度**不会短于**最优方案，即：$H(P, Q) \ge H(P)$。

**Step 3: KL 散度的推导**

KL散度正是这两种编码方案平均长度的**差值**，即信息损失量。

$$D_{KL}(P || Q) = H(P, Q) - H(P)$$

代入公式：
$$D_{KL}(P || Q) = \left(-\sum_i P(i) \log Q(i)\right) - \left(-\sum_i P(i) \log P(i)\right)$$

$$= \sum_i \left( P(i) \log P(i) - P(i) \log Q(i) \right)$$

$$= \sum_i P(i) \left( \log P(i) - \log Q(i) \right)$$

利用对数运算法则 $\log a - \log b = \log(a/b)$，我们得到：

$$D_{KL}(P || Q) = \sum_i P(i) \log\left(\frac{P(i)}{Q(i)}\right)$$

这个推导完美地将KL散度的公式与它在信息论中的直观含义联系了起来。

---

#### **四、重要特性与总结**

1.  **非负性 (Non-negativity)**
    * $D_{KL}(P || Q) \ge 0$。
    * 当且仅当 $P=Q$ 时，$D_{KL}(P || Q) = 0$。

2.  **不对称性 (Asymmetry)**
    * $D_{KL}(P || Q) \neq D_{KL}(Q || P)$。
    * 因此，KL散度不是一个真正的“距离”度量，而是一个“散度”。
    * **$D_{KL}(P || Q)$**：用 Q 近似 P 的信息损失。在机器学习中，P 通常是数据的真实分布，Q 是模型的预测分布。我们希望最小化 $D_{KL}(P || Q)$ 来让模型学习到真实分布。
    * **$D_{KL}(Q || P)$**：用 P 近似 Q 的信息损失。这有不同的应用场景，例如在某些类型的变分推断中。