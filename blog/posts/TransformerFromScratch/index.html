<!DOCTYPE html><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous"><script type="module">const o='<svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5 2V1H10V2H5ZM4.75 0C4.33579 0 4 0.335786 4 0.75V1H3.5C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H7V13H3.5C3.22386 13 3 12.7761 3 12.5V2.5C3 2.22386 3.22386 2 3.5 2H4V2.25C4 2.66421 4.33579 3 4.75 3H10.25C10.6642 3 11 2.66421 11 2.25V2H11.5C11.7761 2 12 2.22386 12 2.5V7H13V2.5C13 1.67157 12.3284 1 11.5 1H11V0.75C11 0.335786 10.6642 0 10.25 0H4.75ZM9 8.5C9 8.77614 8.77614 9 8.5 9C8.22386 9 8 8.77614 8 8.5C8 8.22386 8.22386 8 8.5 8C8.77614 8 9 8.22386 9 8.5ZM10.5 9C10.7761 9 11 8.77614 11 8.5C11 8.22386 10.7761 8 10.5 8C10.2239 8 10 8.22386 10 8.5C10 8.77614 10.2239 9 10.5 9ZM13 8.5C13 8.77614 12.7761 9 12.5 9C12.2239 9 12 8.77614 12 8.5C12 8.22386 12.2239 8 12.5 8C12.7761 8 13 8.22386 13 8.5ZM14.5 9C14.7761 9 15 8.77614 15 8.5C15 8.22386 14.7761 8 14.5 8C14.2239 8 14 8.22386 14 8.5C14 8.77614 14.2239 9 14.5 9ZM15 10.5C15 10.7761 14.7761 11 14.5 11C14.2239 11 14 10.7761 14 10.5C14 10.2239 14.2239 10 14.5 10C14.7761 10 15 10.2239 15 10.5ZM14.5 13C14.7761 13 15 12.7761 15 12.5C15 12.2239 14.7761 12 14.5 12C14.2239 12 14 12.2239 14 12.5C14 12.7761 14.2239 13 14.5 13ZM14.5 15C14.7761 15 15 14.7761 15 14.5C15 14.2239 14.7761 14 14.5 14C14.2239 14 14 14.2239 14 14.5C14 14.7761 14.2239 15 14.5 15ZM8.5 11C8.77614 11 9 10.7761 9 10.5C9 10.2239 8.77614 10 8.5 10C8.22386 10 8 10.2239 8 10.5C8 10.7761 8.22386 11 8.5 11ZM9 12.5C9 12.7761 8.77614 13 8.5 13C8.22386 13 8 12.7761 8 12.5C8 12.2239 8.22386 12 8.5 12C8.77614 12 9 12.2239 9 12.5ZM8.5 15C8.77614 15 9 14.7761 9 14.5C9 14.2239 8.77614 14 8.5 14C8.22386 14 8 14.2239 8 14.5C8 14.7761 8.22386 15 8.5 15ZM11 14.5C11 14.7761 10.7761 15 10.5 15C10.2239 15 10 14.7761 10 14.5C10 14.2239 10.2239 14 10.5 14C10.7761 14 11 14.2239 11 14.5ZM12.5 15C12.7761 15 13 14.7761 13 14.5C13 14.2239 12.7761 14 12.5 14C12.2239 14 12 14.2239 12 14.5C12 14.7761 12.2239 15 12.5 15Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>';let n=Array.from(document.querySelectorAll("pre"));for(let e of n){let C=document.createElement("div");C.style.position="relative",C.classList.add("code-block-efe","group");let t=document.createElement("button");t.className="copy-code",t.innerHTML=o,e.setAttribute("tabindex","0"),e.parentNode&&(e.parentNode.insertBefore(C,e),C.appendChild(e),C.appendChild(t)),t.addEventListener("click",async()=>{await r(e,t)})}async function r(e,C){let t=e.querySelector("code");if(!t)return;let l=t.innerText;await navigator.clipboard.writeText(l),C.innerHTML='<svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.4669 3.72684C11.7558 3.91574 11.8369 4.30308 11.648 4.59198L7.39799 11.092C7.29783 11.2452 7.13556 11.3467 6.95402 11.3699C6.77247 11.3931 6.58989 11.3355 6.45446 11.2124L3.70446 8.71241C3.44905 8.48022 3.43023 8.08494 3.66242 7.82953C3.89461 7.57412 4.28989 7.55529 4.5453 7.78749L6.75292 9.79441L10.6018 3.90792C10.7907 3.61902 11.178 3.53795 11.4669 3.72684Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>&nbsp;Copied',setTimeout(()=>{C.innerHTML=o},700)}</script><html lang="en" class="dark:bg-[#0E0E11] bg-mint-50/5 font-montserrat scroll-smooth"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><title>Transformer From Scratch</title><!-- Favicon for different platforms --><link rel="icon" href="/favicon-32x32.png" sizes="32x32"><link rel="icon" href="/favicon-16x16.png" sizes="16x16"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="icon" href="/android-chrome-192x192.png" sizes="192x192"><link rel="icon" href="/android-chrome-512x512.png" sizes="512x512"><!-- Web manifest --><link rel="manifest" href="/site.webmanifest"><meta name="description" content="A powerful architecture"><meta name="author" content="oGYCo"><meta name="robots" content="index, follow"><link rel="canonical" href="https://ogyco.github.io//blog/posts/TransformerFromScratch/"><!-- Open Graph / Facebook --><meta property="og:title" content="Transformer From Scratch"><meta property="og:description" content="A powerful architecture"><meta property="og:image" content="/images/posts/transformer.png"><meta property="og:image:alt" content="Transformer architecture diagram showing attention mechanisms and neural network layers"><meta property="og:url" content="https://ogyco.github.io"><meta property="og:type" content="website"><meta property="og:site_name" content="My Page | oGYCo"><meta property="og:locale" content="en_US"><!-- Twitter Cards --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Transformer From Scratch"><meta name="twitter:description" content="A powerful architecture"><meta name="twitter:image" content="https://ogyco.github.io/images/posts/transformer.png"><meta name="twitter:image:alt" content="Transformer architecture diagram showing attention mechanisms and neural network layers"><meta name="twitter:site" content><meta name="theme-color" content="#0E0E11"><!-- Sitemap --><link rel="sitemap" href="/sitemap-index.xml"><link rel="stylesheet" href="/_astro/about-me.wFIrXlMU.css"></head> <body class="overflow-x-hidden"> <vercel-speed-insights data-props="{}" data-params="{}" data-pathname="/blog/posts/TransformerFromScratch/"></vercel-speed-insights> <script type="module">var o="@vercel/speed-insights",u="1.2.0",f=()=>{window.si||(window.si=function(...r){(window.siq=window.siq||[]).push(r)})};function l(){return typeof window<"u"}function h(){try{const e="production"}catch{}return"production"}function d(){return h()==="development"}function v(e,r){if(!e||!r)return e;let n=e;try{const t=Object.entries(r);for(const[s,i]of t)if(!Array.isArray(i)){const a=c(i);a.test(n)&&(n=n.replace(a,`/[${s}]`))}for(const[s,i]of t)if(Array.isArray(i)){const a=c(i.join("/"));a.test(n)&&(n=n.replace(a,`/[...${s}]`))}return n}catch{return e}}function c(e){return new RegExp(`/${g(e)}(?=[/?#]|$)`)}function g(e){return e.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function m(e){return e.scriptSrc?e.scriptSrc:d()?"https://va.vercel-scripts.com/v1/speed-insights/script.debug.js":e.dsn?"https://va.vercel-scripts.com/v1/speed-insights/script.js":e.basePath?`${e.basePath}/speed-insights/script.js`:"/_vercel/speed-insights/script.js"}function w(e={}){var r;if(!l()||e.route===null)return null;f();const n=m(e);if(document.head.querySelector(`script[src*="${n}"]`))return null;e.beforeSend&&((r=window.si)==null||r.call(window,"beforeSend",e.beforeSend));const t=document.createElement("script");return t.src=n,t.defer=!0,t.dataset.sdkn=o+(e.framework?`/${e.framework}`:""),t.dataset.sdkv=u,e.sampleRate&&(t.dataset.sampleRate=e.sampleRate.toString()),e.route&&(t.dataset.route=e.route),e.endpoint?t.dataset.endpoint=e.endpoint:e.basePath&&(t.dataset.endpoint=`${e.basePath}/speed-insights/vitals`),e.dsn&&(t.dataset.dsn=e.dsn),d()&&e.debug===!1&&(t.dataset.debug="false"),t.onerror=()=>{console.log(`[Vercel Speed Insights] Failed to load script from ${n}. Please check if any content blockers are enabled and try again.`)},document.head.appendChild(t),{setRoute:s=>{t.dataset.route=s??void 0}}}function p(){try{return}catch{}}customElements.define("vercel-speed-insights",class extends HTMLElement{constructor(){super();try{const r=JSON.parse(this.dataset.props??"{}"),n=JSON.parse(this.dataset.params??"{}"),t=v(this.dataset.pathname??"",n);w({route:t,...r,framework:"astro",basePath:p(),beforeSend:window.speedInsightsBeforeSend})}catch(r){throw new Error(`Failed to parse SpeedInsights properties: ${r}`)}}});</script> <div class="blur-circle"></div> <div style="  background:linear-gradient(45deg, rgba(96, 250, 155, 0) 10.79%, rgba(96, 250, 170, 0.03) 40.92%, rgba(96, 250, 155, 0) 90.35%)" class="fixed top-0 left-0 w-full h-full pointer-events-none -z-1"></div> <header role="banner" aria-label="Main navigation" class="sticky top-0 z-50 w-full p-4 font-medium text-blacktext dark:text-zinc-300 dark:bg-[#0E0E11]/80 dark:border-b dark:border-zinc-800 bg-white/90 backdrop-blur-xs dark:backdrop-blur-xs max-md:z-50 max-md:px-0 transition-all"> <div class="relative mx-auto flex max-w-7xl flex-row items-center justify-between max-xl:px-6"> <a href="/" aria-label="Go to home"> <svg width="80" height="24" viewBox="0 0 80 24" fill="none" xmlns="http://www.w3.org/2000/svg"> <defs> <linearGradient id="logoGradient" x1="0%" y1="0%" x2="100%" y2="0%"> <stop offset="0%" style="stop-color:#38bdf8;stop-opacity:1"></stop> <stop offset="100%" style="stop-color:#3b82f6;stop-opacity:1"></stop> </linearGradient> </defs> <text x="0" y="18" font-family="Arial, sans-serif" font-weight="bold" font-size="16" fill="url(#logoGradient)">
oGYCo
</text> </svg> </a> <script type="module">document.addEventListener("DOMContentLoaded",()=>{const h=window.location.pathname==="/",n={active:["text-mint-500","dark:text-mint-400","font-bold","[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)]"],inactive:["dark:text-zinc-300","text-blacktext"]};function c(t,e){e?(t.classList.add(...n.active),t.classList.remove(...n.inactive),t.setAttribute("aria-current","page")):(t.classList.remove(...n.active),t.classList.add(...n.inactive),t.removeAttribute("aria-current"))}function i(){const t=window.location.pathname,e=window.location.hash?`#${window.location.hash.substring(1)}`:"";document.querySelectorAll("nav a").forEach(o=>{const a=o.getAttribute("data-path");c(o,a===t||a===e)})}function l(){if(!h)return;const t=document.querySelectorAll("section[id]"),e=document.querySelectorAll("nav a"),o={root:null,rootMargin:"-50% 0px",threshold:0},a=new IntersectionObserver(s=>{s.forEach(r=>{if(r.isIntersecting){const d=r.target.getAttribute("id");d&&e.forEach(u=>{const v=u.getAttribute("data-path");c(u,v===`/#${d}`)})}})},o);t.forEach(s=>a.observe(s))}i(),l(),window.addEventListener("hashchange",i)});</script> <nav class="nav-links flex w-full justify-center gap-6 max-md:gap-3 max-md:py-6" role="navigation" aria-label="Main Navigation"> <a href="/#home" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/#home" aria-label="Home"> Home </a><div class="relative group flex items-center"> <a href="/blog/" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/blog/" aria-label="Blog"> Blog </a> <div class="absolute left-0 top-full hidden group-hover:block bg-white dark:bg-zinc-800 shadow-lg rounded-md py-2 min-w-[200px] z-50"> <a href="/blog/posts/" class="block px-4 py-2 text-sm hover:bg-mint-100 dark:hover:bg-zinc-700 transition-colors" data-path="/blog/posts/" aria-label="All Posts"> All Posts </a> </div> </div><a href="/about-me" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/about-me" aria-label="About Me"> About Me </a> <div class="flex items-center justify-center gap-5 md:hidden" role="group" aria-label="Social Media Links"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Link to github"> <svg width="1em" height="1em" aria-hidden="true" data-icon="github">   <symbol id="ai:local:github" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M7.5.25a7.25 7.25 0 0 0-2.292 14.13c.363.066.495-.158.495-.35 0-.172-.006-.628-.01-1.233-2.016.438-2.442-.972-2.442-.972-.33-.838-.805-1.06-.805-1.06-.658-.45.05-.441.05-.441.728.051 1.11.747 1.11.747.647 1.108 1.697.788 2.11.602.066-.468.254-.788.46-.969-1.61-.183-3.302-.805-3.302-3.583a2.8 2.8 0 0 1 .747-1.945c-.075-.184-.324-.92.07-1.92 0 0 .61-.194 1.994.744A7 7 0 0 1 7.5 3.756 7 7 0 0 1 9.315 4c1.384-.938 1.992-.743 1.992-.743.396.998.147 1.735.072 1.919.465.507.745 1.153.745 1.945 0 2.785-1.695 3.398-3.31 3.577.26.224.492.667.492 1.343 0 .97-.009 1.751-.009 1.989 0 .194.131.42.499.349A7.25 7.25 0 0 0 7.499.25" clip-rule="evenodd"/></symbol><use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Link to linkedin"> <svg width="1em" height="1em" aria-hidden="true" data-icon="linkedin">   <symbol id="ai:local:linkedin" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M2 1a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h11a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1zm1.05 5h1.9v6h-1.9zm2.025-1.995a1.075 1.075 0 1 1-2.15 0 1.075 1.075 0 0 1 2.15 0M12 8.357c0-1.805-1.167-2.507-2.326-2.507a2.2 2.2 0 0 0-1.095.231c-.257.13-.526.424-.734.938h-.053V6H6v6.005h1.906V8.81c-.027-.327.077-.75.291-1.001.215-.252.52-.312.753-.342h.073c.606 0 1.056.375 1.056 1.32v3.217h1.906z" clip-rule="evenodd"/></symbol><use href="#ai:local:linkedin"></use>  </svg> </a> </div> </nav> <div class="flex items-center justify-between gap-5 text-xl"> <div class="max-md:hidden flex items-center justify-center gap-5" role="list"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Link to github"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="github">   <use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Link to linkedin"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="linkedin">   <use href="#ai:local:linkedin"></use>  </svg> </a> </div> </div> <div class="flex items-center gap-5 text-xl md:pl-5"> <button id="themeToggle" class="hover:cursor-pointer hover:text-mint-400 transition-all" data-astro-cid-htzy5xbu> <svg width="1em" height="1em" class="sun" data-astro-cid-htzy5xbu="true" data-icon="sun">   <symbol id="ai:local:sun" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M7.5 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2a.5.5 0 0 1 .5-.5M2.197 2.197a.5.5 0 0 1 .707 0L4.318 3.61a.5.5 0 0 1-.707.707L2.197 2.904a.5.5 0 0 1 0-.707M.5 7a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1zm1.697 5.803a.5.5 0 0 1 0-.707l1.414-1.414a.5.5 0 1 1 .707.707l-1.414 1.414a.5.5 0 0 1-.707 0M12.5 7a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1zm-1.818-2.682a.5.5 0 0 1 0-.707l1.414-1.414a.5.5 0 1 1 .707.707L11.39 4.318a.5.5 0 0 1-.707 0M8 12.5a.5.5 0 0 0-1 0v2a.5.5 0 0 0 1 0zm2.682-1.818a.5.5 0 0 1 .707 0l1.414 1.414a.5.5 0 1 1-.707.707l-1.414-1.414a.5.5 0 0 1 0-.707M5.5 7.5a2 2 0 1 1 4 0 2 2 0 0 1-4 0m2-3a3 3 0 1 0 0 6 3 3 0 0 0 0-6" clip-rule="evenodd"/></symbol><use href="#ai:local:sun"></use>  </svg> <svg width="1em" height="1em" class="moon" data-astro-cid-htzy5xbu="true" data-icon="moon">   <symbol id="ai:local:moon" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M2.9.5a.4.4 0 0 0-.8 0v.6h-.6a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 1 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zm3 3a.4.4 0 1 0-.8 0v.6h-.6a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 1 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zm-4 3a.4.4 0 1 0-.8 0v.6H.5a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 0 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zM8.544.982l-.298-.04c-.213-.024-.34.224-.217.4q.211.305.389.632A6.602 6.602 0 0 1 2.96 11.69c-.215.012-.334.264-.184.417q.103.105.21.206l.072.066.26.226.188.148.121.09.187.131.176.115q.18.115.37.217l.264.135.26.12.303.122.244.086a6.6 6.6 0 0 0 1.103.26l.317.04.267.02q.19.011.384.011a6.6 6.6 0 0 0 6.56-7.339l-.038-.277a6.6 6.6 0 0 0-.384-1.415l-.113-.268-.077-.166-.074-.148a6.6 6.6 0 0 0-.546-.883l-.153-.2-.199-.24-.163-.18-.12-.124-.16-.158-.223-.2-.32-.26-.245-.177-.292-.19-.321-.186-.328-.165-.113-.052-.24-.101-.276-.104-.252-.082-.325-.09-.265-.06zm1.86 4.318a7.6 7.6 0 0 0-.572-2.894 5.601 5.601 0 1 1-4.748 10.146 7.6 7.6 0 0 0 3.66-2.51.749.749 0 0 0 1.355-.442.75.75 0 0 0-.584-.732q.093-.174.178-.355A1.25 1.25 0 1 0 10.35 6.2q.052-.442.052-.9" clip-rule="evenodd"/></symbol><use href="#ai:local:moon"></use>  </svg> </button>  <script>
    // Execute immediately before the page loads
    (function() {
        const theme = (() => {
            if (typeof localStorage !== "undefined" && localStorage.getItem("theme")) {
                return localStorage.getItem("theme");
            }
            if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                return "dark";
            }
            return "light";
        })();

        if (theme === "light") {
            document.documentElement.classList.remove("dark");
        } else {
            document.documentElement.classList.add("dark");
        }

        window.localStorage.setItem("theme", theme);
    })();

    // Listen for changes in system preferences
    window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change", (e) => {
        if (!localStorage.getItem("theme")) {
            if (e.matches) {
                document.documentElement.classList.add("dark");
            } else {
                document.documentElement.classList.remove("dark");
            }
        }
    });

    const handleToggleClick = () => {
        const element = document.documentElement;
        element.classList.toggle("dark");

        const isDark = element.classList.contains("dark");
        localStorage.setItem("theme", isDark ? "dark" : "light");
    };

    document
        .getElementById("themeToggle")
        .addEventListener("click", handleToggleClick);
</script> <button class="hamburger" aria-label="Open menu" aria-expanded="false" aria-controls="mobile-menu"> <svg width="1em" height="1em" class="hamburger-icon bars-icon" aria-hidden="true" data-icon="bars">   <symbol id="ai:local:bars" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M1.5 3a.5.5 0 0 0 0 1h12a.5.5 0 0 0 0-1zM1 7.5a.5.5 0 0 1 .5-.5h12a.5.5 0 0 1 0 1h-12a.5.5 0 0 1-.5-.5m0 4a.5.5 0 0 1 .5-.5h12a.5.5 0 0 1 0 1h-12a.5.5 0 0 1-.5-.5" clip-rule="evenodd"/></symbol><use href="#ai:local:bars"></use>  </svg> <svg width="1em" height="1em" class="hamburger-icon xmark-icon" aria-hidden="true" data-icon="xmark">   <symbol id="ai:local:xmark" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M12.854 2.854a.5.5 0 0 0-.708-.708L7.5 6.793 2.854 2.146a.5.5 0 1 0-.708.708L6.793 7.5l-4.647 4.646a.5.5 0 0 0 .708.708L7.5 8.207l4.646 4.647a.5.5 0 0 0 .708-.708L8.207 7.5z" clip-rule="evenodd"/></symbol><use href="#ai:local:xmark"></use>  </svg> </button> </div> </div> </header> <div class="min-h-[85vh] "> <section class="relative mx-auto px-8 max-sm:px-4 flex flex-row justify-center gap-6"><div class="w-64 max-xl:hidden"></div><article class="flex flex-col gap-8 max-w-3xl max-md:w-full pb-10 pt-8 mt-8 px-14 max-md:px-10 max-sm:px-4 dark:bg-transparent bg-white dark:border-0 border border-neutral-100 rounded-2xl"><header class="flex flex-col gap-4" id="start"><div class="flex gap-2 flex-wrap"><a class="cursor-pointer" href="/blog/techs/python" aria-label="View articles about Python" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Python icon"><svg width="1em" height="1em" class="w-full!" data-icon="python">   <symbol id="ai:local:python" viewBox="16 16 32 32"><g fill="none"><path fill="url(#a)" d="M31.885 16c-8.124 0-7.617 3.523-7.617 3.523l.01 3.65h7.752v1.095H21.197S16 23.678 16 31.876c0 8.196 4.537 7.906 4.537 7.906h2.708v-3.804s-.146-4.537 4.465-4.537h7.688s4.32.07 4.32-4.175v-7.019S40.374 16 31.885 16m-4.275 2.454a1.394 1.394 0 1 1 0 2.79 1.393 1.393 0 0 1-1.395-1.395c0-.771.624-1.395 1.395-1.395"/><path fill="url(#b)" d="M32.115 47.833c8.124 0 7.617-3.523 7.617-3.523l-.01-3.65H31.97v-1.095h10.832S48 40.155 48 31.958s-4.537-7.906-4.537-7.906h-2.708v3.803s.146 4.537-4.465 4.537h-7.688s-4.32-.07-4.32 4.175v7.019s-.656 4.247 7.833 4.247m4.275-2.454a1.393 1.393 0 0 1-1.395-1.395 1.394 1.394 0 1 1 1.395 1.395"/><defs><linearGradient id="a" x1="19.075" x2="34.898" y1="18.782" y2="34.658" gradientUnits="userSpaceOnUse"><stop stop-color="#387EB8"/><stop offset="1" stop-color="#366994"/></linearGradient><linearGradient id="b" x1="28.809" x2="45.803" y1="28.882" y2="45.163" gradientUnits="userSpaceOnUse"><stop stop-color="#FFE052"/><stop offset="1" stop-color="#FFC331"/></linearGradient></defs></g></symbol><use href="#ai:local:python"></use>  </svg></div>Python</span></a></div><div class="gap-3 flex flex-wrap justify-start items-center"><a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/Model Architecture" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Model Architecture </a></div><span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> July 20, 2025</span><h1 class="font-extrabold text-4xl max-xl:text-3xl dark:text-white text-blacktext">Transformer From Scratch</h1><div class="flex items-center max-sm:flex-col gap-2 max-sm:items-start justify-between"><div class="white flex items-center gap-4"><div aria-label="Sparkle" class="size-10 bg-cover bg-center rounded-full drop-shadow-lg" style="background-image: url(/images/1723118997692.png);"></div><a id="Transformer From Scratch" class="leading-0 text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50" href="/blog">oGYCo</a></div></div></header><figure><img class="w-full rounded-xl" src="/images/posts/transformer.png" alt="Transformer architecture diagram showing attention mechanisms and neural network layers"></figure><div class="markdown" id="content"><p>本文主要是对于transformer的每个部分的深入理解加上对于代码的部分关键信息进行解读和补充</p>
<p>关于代码的全部解释请见文章<a href="https://ogyco.github.io/blog/posts/CodeUnderstandingOfTransformer/">Transformer代码深入理解</a>，建议两者结合阅读，先看本文的<strong>非折叠内容</strong>理解整个架构的设计以及详细解释，然后再看代码理解的文章同时遇到关键的部分再回到本文的折叠区（即“工程代码解读部分”）看相关代码的关键部分的解释</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">图解transformer</a></li>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">哈弗-代码实现</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
</ul>
<h2 id="整体架构">整体架构</h2>
<ul>
<li>Transformer
<ul>
<li>Encoder * N
<ul>
<li>Self-Attention</li>
<li>Feed Forward Neural Network</li>
</ul>
</li>
<li>Decoder * N
<ul>
<li>Self-Attention(Masked)</li>
<li>Encoder-Decoder Attention(Cross-Attention)</li>
<li>Feed Forward Neural Network</li>
</ul>
</li>
</ul>
</li>
</ul>
<details>
<summary>工程代码解读</summary>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> EncoderDecoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"><span style="color:#9ECBFF">    A standard Encoder-Decoder architecture. Base for this and many</span></span>
<span class="line"><span style="color:#9ECBFF">    other models.</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, encoder, decoder, src_embed, tgt_embed, generator):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(EncoderDecoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.encoder </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> encoder</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.decoder </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> decoder</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.src_embed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> src_embed</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.tgt_embed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt_embed</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.generator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, src, tgt, src_mask, tgt_mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Take in and process masked src and target sequences."</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decode(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.encode(src, src_mask), src_mask, tgt, tgt_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> encode</span><span style="color:#E1E4E8">(self, src, src_mask):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.encoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_embed(src), src_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> decode</span><span style="color:#E1E4E8">(self, memory, src_mask, tgt, tgt_mask):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt_embed(tgt), memory, src_mask, tgt_mask)</span></span></code></pre>
<p>src和src_mask的区别：input是长短不一的，所以需要为input加上pad，即把短句子用一个特殊的“占位符”（比如 <pad>）补齐，让它们和最长的句子一样长src，但是在计算的过程中<strong>这些位置是不需要被关注</strong>的，所以需要对这些pad区域进行掩码，故而有了src_mask，格式类似于<code>[True, True, True, True, True, False, False, False]</code></pad></p>
<p>tgt和tgt_mask在src掩码的基础上还加入了不让前面的token看到后面token的掩码，pad掩码部分和src一样</p>
</details>
<h2 id="第一步tokenization--embedding--positional-encoding">第一步—Tokenization &#x26; Embedding &#x26; Positional Encoding</h2>
<h3 id="分词">分词</h3>
<h3 id="嵌入向量">嵌入向量</h3>
<p>就是用一个Embedding矩阵，将词表中的每个词和矩阵中的一个行向量联系起来</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 本质上是一个大的权重矩阵</span></span>
<span class="line"><span style="color:#E1E4E8">embedding_matrix.shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_size, d_model]  </span><span style="color:#6A737D"># [30000, 512]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 对于输入的词汇ID</span></span>
<span class="line"><span style="color:#E1E4E8">word_id </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1234</span></span>
<span class="line"><span style="color:#E1E4E8">word_vector </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> embedding_matrix[word_id]  </span><span style="color:#6A737D"># 取出对应行作为词向量</span></span></code></pre>
<h3 id="位置编码">位置编码</h3>
<p>如果只是有注意力和全连接层，一段序列的位置关系就会被忽略，所以需要引入位置编码
所谓的位置编码就是给embedding之后的向量加上了一个相同维度的代表这token位置信息的向量（遵循一定的模式），注意是两个向量相加，不是直接拼接在原本的向量后面，虽然应该也可以，但是那样会增加向量的维度导致后面需要的参数量也更多。</p>
<h4 id="遵循什么模式呢">遵循什么模式呢？</h4>
<p>位置编码会有不同的方式，通过不同的函数生成</p>
<h2 id="第二步encoder">第二步—Encoder</h2>
<p>过程：整个架构就是完成一个input序列到一个putput序列的任务，而encoder负责的就是input的部分。编码器会接受一个向量列表作为输入（最开始是嵌入向量和位置向量的和），然后经过注意力层和前馈层之后传递到下一个encoder，经过多个encoder最后得到了一个向量列表会用于decoder的cross-attention层</p>
<details>
<summary>工程代码解读</summary>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Encoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Core encoder is a stack of N layers"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, layer, N):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Encoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, N)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(layer.size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Pass the input (and mask) through each layer in turn."</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">            x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, mask)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p>这里与原论文采用的有一点略微的差别，是更现代的方式，原论文中是Post-LN结构，也就是先经过子层然后再残差连接与层归一化，而这里则用的是Pre-LN结构，也就是先进行层归一化，再通过子层，然后进行残差连接，然后在通过了N个decoder层之后最后再来一次层归一化，例如6层的结构也就是进行了13次层归一化操作，而原论文中则只会进行12次层归一化操作。这样做的原因是在后续的研究和实践中，大家发现将层归一化放在前面（即 Pre-LN）会让训练过程更稳定，尤其是在模型层数很深的时候。很多现代的 Transformer 实现（比如 GPT-2/3 和 BERT 的一些变体）都采用了这种结构。</p>
<blockquote>
<p>整个Encoder是6个EncoderLayer组成，每个EncoderLayer包含两个SublayerConnection，第一个是经过先层归一化然后经过注意力层然后进行dropout正则化再残差连接，第二是经过先层归一化然后经过FFN层然后进行dropout正则化再残差连接</p>
</blockquote>
</details>
<h3 id="注意力机制">注意力机制</h3>
<p>自注意力机制就是让每个单独的token学到语境信息并更新，即让每个token向量变成一个在语境信息中更加准确的向量，从而将每个token转化为该token在语境中的高维精确的表达。</p>
<p>例如：”<code>The animal didn't cross the street because it was too tired</code>”
比如该句子中的it指代的是the animal，但是一个简单的it的嵌入向量只能表达最基本的代词含义，现在需要让这个it“注意到”animal，从而将这两个词联系起来，即是计算注意力分数然后根据animal的注意力分数较高，会让之后由animal得到的V向量的权重较大，使得animal对it的更新效果更加显著。当然，一个token对自身的注意力权重通常都比较显著，以确保其核心身份信息不会丢失。但最高的注意力权重会动态地分配给当前上下文中最重要的token(s)，这其中可能包括它自己，也可能包括其他的token。例如对于 <code>it</code> 来说，<code>animal</code> 的上下文信息可能比 <code>it</code> 本身的（作为代词的）信息更重要。在这种情况下，模型可能会学到给 <code>animal</code> 分配比 <code>it</code> 自身更高的注意力权重。</p>
<ul>
<li>自注意力机制通过一个<strong>加权求和</strong>过程，让每个token的向量表示融合来自句子中其他token（尤其是最相关的token）的上下文信息。在这个过程中，通过自身的注意力权重和至关重要的<strong>残差连接（为什么要引入<a href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5">残差连接</a>）</strong>，它又能确保token不会“忘记”自己是谁，从而实现对原始信息的保留和对上下文信息的精确更新。</li>
<li><strong>自注意力机制的核心任务，就是计算出那个需要被加到原始向量上的“更新向量”或“残差向量”，然后与原始向量相加就得到了最后更新的向量</strong></li>
<li>具体过程
<ul>
<li>
<p>第一步：将原始向量与三个矩阵Wq、Wk、Wv分别相乘得到Q向量、K向量、V向量，即一层注意力的参数就是Wq、Wk、Wv三个矩阵</p>
<p><img src="/images/posts/t-1.png" alt="Transformer QKV矩阵计算"></p>
</li>
<li>
<p>第二步：然后得到了三个矩阵Q、K、V，再让Q矩阵和K矩阵相乘得到每个向量与其他每个向量的注意力分数，K矩阵需要转置，就得到了向量的点积矩阵（注意力分数矩阵），注意这里注意力分数还要进行除以根号下dk（向量的维度）（防止产生过大的注意力分数和梯度消失问题，让模型无法学习多样化的信息，即最后的softmax结果几乎是一个one-shot向量）和一个softmax操作（让权重和为1，且都是非负数，这样的权重可以被理解为一个<strong>概率分布</strong>，它告诉我们应该将100%的“注意力”如何分配给序列中的所有token）</p>
<p><img src="/images/posts/t-2.png" alt="注意力分数计算公式"></p>
<p><img src="/images/posts/t-3.png" alt="注意力机制详细流程"></p>
<p><img src="/images/posts/t-4.png" alt="Softmax函数应用"></p>
<ul>
<li>
<p>梯度消失的原因，当产生的数值特别大的时候，变化几乎不会导致结果的变化，也就是梯度为0，模型就会认为不需要再继续训练了</p>
<p><img src="/images/posts/t-5.png" alt="梯度消失问题示意图"></p>
</li>
</ul>
</li>
<li>
<p>第三步：然后再根据注意力分数对V向量进行加权求和</p>
<p><img src="/images/posts/t-6.png" alt="注意力加权求和计算"></p>
</li>
</ul>
</li>
<li>论文中所用的实际上是<a href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6">多头注意力</a>来计算，也就是算出来的q、k、v向量跟原本的向量不是同一个维度，例如原向量是512维的，现在算出来的q、k、v都是64维度的</li>
<li>在实际的计算过程中用到的是矩阵计算，如图所示：
<ul>
<li>算Q、K、V矩阵：例如2个512维的向量，就是一个2 * 512的矩阵，然后乘以一个512 * 64的矩阵得到了两个64维的矩阵
<img src="/images/posts/self-attention-matrix-calculation.png" alt=""></li>
<li>算注意力矩阵:Q为2 * 64的矩阵，乘以K的转置，也就是64 * 2的矩阵，也就得到一个2 * 2的注意力分数矩阵，这样算出来就是例如[1][1]的数值就是第一个向量对第一个向量的注意力,[1][2]就是第一个向量对第二个向量的注意力。然后除以根号下dk(key向量的维度)，然后再做softmax，显然这里的softmax的计算就是对注意力矩阵的每个行向量进行操作，即每个行向量是作为一组来操作的
<img src="/images/posts/self-attention-matrix-calculation-2.png" alt=""></li>
</ul>
</li>
</ul>
<h3 id="多头注意力机制">多头注意力机制</h3>
<h4 id="过程">过程</h4>
<p>所谓的多头注意力就是原本是将原向量乘以三个矩阵：Wq、Wk、Wv转（线性变换）成维度相同的向量然后就可以一次性完成一次的注意力层，然后多头注意力就是变成了将原向量线性变换为了低维度的3个向量，例如原向量是512维的，现在得到的q、k、v都是64维度的，然后就会有8组Wq、Wk、Wv矩阵完整8次转化，每一组最终都会得到一个包含64维度向量组成的矩阵n * 64，然后把8组的矩阵拼接起来就是一个包含512维度向量的矩阵n * 512，注意这样直接拼接起来并不好，因为信息相当于是割裂的，就需要再乘以一个512 * 512的矩阵相当于把8个64维度的<strong>信息片段</strong>的信息进行整合，重新变成一个<strong>完整</strong>的向量。
<img src="/images/posts/transformer_attention_heads_qkv.png" alt=""></p>
<h4 id="为什么这样做">为什么这样做</h4>
<p>为什么不直接将Q、K、V变换为512维度的呢？这样不是更加方便吗，分成8次的注意力计算有什么好处？最后还要再乘以一个矩阵来整合信息,还增加了参数量和计算量</p>
<ul>
<li>“It expands the model’s ability to focus on different positions.”
<ul>
<li>每个头关注的是不同的信息，然后这样就能学到更加丰富的内容，注意力是加权平均的操作：每个位置是看“所有其他位置”的加权和，所以这种“平均”会有可能模糊具体细节，尤其是当多个位置有相似内容时。多头注意力用多个“注意力头”同时从不同角度学习注意力权重，可以一定程度上保留更多细粒度信息。Transformer 能快速捕捉远程依赖，但注意力有平均化的副作用，作者通过多头注意力来缓解这个问题。</li>
</ul>
</li>
<li>“It gives the attention layer multiple ‘representation subspaces’”</li>
</ul>
<p>这么看的话，transformer就跟CNN很相似了，每个注意力头关注的是不同的地方，然后通过乘以一个大的变换矩阵就形成更大的特征提取与融合</p>
<h3 id="前馈神经网络">前馈神经网络</h3>
<p>这一部分受到了3Blue1Brown视频<a href="https://www.bilibili.com/video/BV1aTxMehEjK?spm_id_from=333.788.recommend_more_video.0&#x26;vd_source=085e39762801eff84f6eecf25e64a0a8">(bilibili)</a><a href="https://www.youtube.com/watch?v=9-Jl0dxWQs8">(youtube)</a>的指导，讲解的非常精彩，如果你也感兴趣，请务必去看一看，同时该系列视频的其他视频也值得一看</p>
<p>所谓前馈神经网络其实就是一个MLP（多层感知机），即让向量先通过一个线性层然后使用激活函数再经过线性层，至于具体的线性层层数这些是模型架构设计的一部分，每个线性层其实就是在做一次线性变换，所谓加权求和，两个神经元之间的连线上的权重其实就是这个线性变换对应矩阵的某个位置的一个值。我们把需要变换的向量看成一个列向量，第一个线性层就是让这个列向量左边乘以一个变换矩阵，变换的结果就是矩阵的每一行是一个行向量跟这个列向量做点积（矩阵看成行向量是因为行向量才是跟embedding处在同样的向量空间中也就是可以进行计算的），例如原本列向量代表的是Michel Jordan，矩阵的某个行向量代表First Name Michael，那么我们可以理解为<strong>原列向量蕴含了这个行向量的编码信息，也就是说列向量在行向量上的投影几乎就是行向量本身，那么可以简单理解维这两个向量做点积的结果就是1，也就是所谓的神经元被激活了</strong>。然后经过激活函数之后，我们来到了第二个线性层，这个线性层我们更好的方式是将权重矩阵看作是列向量（因为第二个层的列向量才是跟embedding处在同样的向量空间中，即维度是一样的），矩阵乘以列向量就是神经元对应的激活值乘以其对应位置的列向量然后求和，这里也就是<strong>被激活了的神经元在权重矩阵中对应的列向量的编码会被加到最后的结果中</strong></p>
<blockquote>
<p>由此可以见到，所谓的权重也可以是看成了储存在模型中的向量信息，每个向量也是有他所代表的信息，也就是模型储存了事实，第一个线性层的权重矩阵的每一行都看成是一个“查询”向量，查询向量与原向量做点积就可以得到这两个向量的关系，如果结果很大就是算出来的结果对应的神经元会被激活，第二个权重矩阵的每一列看成是一个事实向量，其对应的神经元被激活，这个向量就会被加到最后的结果中</p>
</blockquote>
<blockquote>
<p>第一个线性层的矩阵看成是行向量的组合，第二个线性层看成是列向量的组合，因为他们才都是和原本的embedding处在同一个向量空间中，我们将所有的语义信息都在这个预设好维度（即每个token对应embedding向量的维度）的向量空间中进行存储和计算，我们几乎所有的工作都是在使用这些存储在这个特定空间的向量进行计算，如算点积（计算相似度）、相加减（更新语义）等等等等</p>
</blockquote>
<blockquote>
<p>不过，以上只是一种简化的理解方式，实际上单个神经元对应的向量可能并不是代表想Michael Jordan这样的简单事实，而在科学上还提出过有关“叠加”态的说法</p>
</blockquote>
<blockquote>
<p>在N维的向量空间中最多只能有N个互相垂直的向量，但是,一旦我们将要求降低一点，让向量之间的夹角在89-91度之间，根据Johnson-Lindenstrauss Lemma，满足这样条件的向量的数量会随着维数的增大而呈指数级的增长，也就是向量空间所能表征的“概念”数量会指数级的增加，或许这也就能解释为什么模型的参数规模到了一定的水平之后会出现涌现的能力</p>
</blockquote>
<h3 id="残差连接">残差连接</h3>
<p>残差连接就是让经过了注意力层或者FFN生成的向量和原本的没有经过网络的向量直接相加之后再进行层归一化处理</p>
<p>理论上来说，更深的网络一定不会比浅的网络的性能差，因为深的网络只需要让后面的层变成恒等变换层就会与浅层网络一样，但是实践上发现要让网络拟合出一个恒等变换是很困难的，而网络拟合一个0函数就很简单，即让F(x)=0，因为只需要让权重全部变成0就可以了。于是就引入了残差连接，即让网络学习的是输入与输出之间的差函数，这样，一个恒等变换网络就等于一个残差为0的网络加上残差连接即可。而注意力层就是学习的残差函数</p>
<blockquote>
<p>残差连接改变了自注意力层的<strong>学习目标</strong></p>
</blockquote>
<ul>
<li><strong>没有残差连接时</strong>：自注意力层需要学习一个完整的变换函数 <code>H(x)</code>，直接输出最终的目标向量。这很难。</li>
<li><strong>有残差连接时</strong>：自注意力层只需要学习<strong>残差（Residual）</strong> <code>F(x) = H(x) - x</code>。也就是说，它只需要学习“<strong>需要做的改变量</strong>”或者说“<strong>更新量</strong>”。</li>
</ul>
<p>如果模型发现不需要做任何改变，它只需要让自注意力层输出一个零向量就行了，这比学习一个恒等变换（输入什么输出什么）要容易得多。
<img src="/images/posts/t-7.png" alt="残差网络的意义"></p>
<h3 id="层归一化">层归一化</h3>
<p>层归一化是一种技术，它针对单个样本的所有特征（即单个向量内部），通过调整其数值的分布，使其变得“整齐”（通常是均值为0，方差为1），从而让神经网络的训练过程更快速、更稳定</p>
<p>在 Transformer 的 Encoder 或 Decoder 层中，数据是以 [批次大小, 序列长度, 嵌入维度] 的形式流动的。层归一化（LN）作用在最后一个维度上。也就是说，它会独立地处理序列中的每一个词元（Token），对这个词元的整个 512 维的嵌入向量进行归一化操作</p>
<h2 id="第三步decoder">第三步—Decoder</h2>
<p>Decoder负责的是Output序列，整个过程就是不断的将decoder的输出作为新的输入给decoder然后让decoder不断生成下一个token直到生成一个特殊的结束token</p>
<blockquote>
<p>重要：所谓的生成下一个token就是每一个token经过decoder之后就会生成一个代表概率的向量，向量的某个位置的值代表着当前token对于下一个token在这个index下（例如向量的第5维度代表着词表中的某个特定的词）对应的词的概率，也就是说<strong>每个token经过decoder之后都会生成一个预测向量</strong>，只不过在推理阶段我们只需要取最后一个token的预测向量来生成下一个词而已，而在推理的时候则是会用到所有token生成的预测向量，看看这个预测向量代表的词和实际的标签是否相符合罢了</p>
</blockquote>
<p>在推理的时候，Decoder处理的第一个Token向量来自于一个人为添加的、代表“句子开始”的特殊标记，通常被称为 <sos> (Start of Sequence) 或 <bos> (Begin of Sequence)，然后模型就可以不断的进行生成了</bos></sos></p>
<p>在训练时，Transformer用了一个“障眼法”（Mask），让GPU可以同时为序列中的每一个位置都执行一次“一次只预测一个词”的任务，从而把原本需要N步的串行计算，变成了一步完成的并行计算，极大地加速了训练过程。这个设计正是Transformer能够处理长序列并在大规模数据上成功训练的关键之一</p>
<details>
<summary>工程代码解读</summary>
<blockquote>
<p>Decoder也是由6个DecoderLayer组成，每个DecoderLayer则是由三个SubLayerConnection做成，第一个是先经过层归一化然后再经过注意力层（这里需要用到特别的掩码即既有对pad的掩码，也有对未来token的掩码）然后再dropout正则化然后再残差连接，第二个是先经过层归一化然后再经过cross-attention层（即K、V来自encoder，Q来自上一个DecoderLayer的输出）然后dropout正则化然后再残差连接，第三个是先经过层归一化然后再经过FFN然后dropout正则化然后残差连接</p>
</blockquote>
</details>
<h3 id="掩码注意力">掩码注意力</h3>
<p>掩码的目的主要是用在模型训练的时候。
在训练时，我们已经拥有了完整的源句子和对应的目标句子，输入的准备如图
<img src="/images/posts/train-encoder.png" alt="">
也就是说，在训练的时候，输入了sos，然后让模型根据sos和原始的输入来预测下一个词也就是Wie，即前向传播一次，然后再反向传播进行梯度下降来修改参数让模型预测的更准确，然后输入sos、Wie让模型预测geht，以此类推，这样sos和eos的加入刚好让标签和实际输入错开了，从而保证了自回归生成。但是！！！注意了，实际训练过程，加上了sos的序列会被全部放进encoder中</p>
<p>为什么？不是一次只能预测一个词吗（对也不对）</p>
<p>实际上decoder是能够一次性预测所有的词的，只不过是推理的时候只用到了最后一个token的预测，而在训练的时候是会用到所有的token的预测的，也就是根据预测来反向传播调整参数</p>
<p>那么这个时候就需要引入掩码了，因为预测的时候不能让当前token看到他后面的token（也就是实际的“答案”）</p>
<h4 id="掩码操作">掩码操作</h4>
<p>例如，当计算第三个位置（“geht”）的输出时，自注意力机制会计算它与序列中所有其他词的“注意力分数”。Mask矩阵会强行将它与自己及之后位置（“geht”, “es”）的注意力分数设置为一个极大的负数（比如 -1e9）。也就是给计算出来的注意力分数矩阵加上了一个特别的矩阵，矩阵的右上方全是负无穷，左下角全是0，也就是让原本注意力分数矩阵的右上角全部变成负无穷（右上角的数值代表的是前面token对于他后面的token的注意力分数）</p>
<p>这样一来，在经过Softmax函数之后，这些位置的注意力权重就变成了0，也就不会让前面的token看到后面的答案了（后续在加权求和的时候就不会加上来自后面的token生成的V向量）</p>
<h3 id="cross-attentionencoder-decoder-attention">Cross-Attention(Encoder-Decoder Attention)</h3>
<p>这个是一个特殊的注意力层，与self attention唯一的不同就是K、V矩阵是来自encoder最后一次输出的向量生成的K和V，而Q则是由经过了掩码注意力层之后生成的向量生成的。在cross-attention模块中，来自答案的Q和来自encoder的K算出了注意力分数矩阵，这个注意力分数矩阵然后会和来自encoder的V矩阵相乘得到新的一组向量再和原向量进行了残差连接并进行了层归一化处理</p>
<p>这样做的原因就是让输出作为查询，而来自encoder的信息就是作为一份已经理解好的超级“文档”，让来自decoder的序列来注意到来自encoder的信息，从而实现信息的融合</p>
<blockquote>
<p>信息融合本身不是目的，它是一种手段。其最终目的，是让模型在预测下一个词时，能够做出一个更明智、更准确、几乎是唯一正确的选择。如果说Cross-Attention是“博览群书”（吸收信息），那么FFN就是“消化理解”（深度思考）。它会将收集到的信息进行提炼、组合和筛选，识别出更复杂的模式，最终输出一个更加精炼、意图更加明确的向量。“正因为融合了信息，这个向量才从一个‘充满可能性的模糊状态’，变成了一个‘指向正确答案的确定状态’。”</p>
</blockquote>
<h2 id="最后linear--softmax">最后—Linear &#x26; Softmax</h2>
<p>经过decoder最后模型会生成一堆向量，但是这个向量还不是最终的预测向量，这个向量还要经过一个线性层生成一个巨大的（维度跟词表中词的数量一样）每个维度包含其对应词的预测分数的向量，然后这个分数向量再经过softmax生成概率向量就完成了</p>
<h3 id="linear">Linear</h3>
<p>线性层就是一个简单的全连接神经网络将解码器产生的向量线性变换到了一个巨大的向量</p>
<h3 id="softmax">Softmax</h3>
<p>softmax没啥好说的，就是一个计算过程，取一组值然后生成一个概率分布，分数越大概率就越高，所有值对概率的和为1</p>
<h2 id="something-special">Something Special</h2>
<h3 id="损失函数">损失函数</h3>
<p>在训练的时候，我们假设我们的输出词汇表只包含六个单词（“a”、“am”、“i”、“thanks”、“student”和 “<eos>”，然后我们就可以定义词表：
<img src="/images/posts/wordlist.png" alt="">
即每个词对应了一个index值，然后我们还有one-shot编码
<img src="/images/posts/one-shot.png" alt="">
就是为每个词编码为当前第index维的值为1，其余都是0的一个向量，然后我们不是经过decoder之后就会有一个跟词表维度一样的预测向量吗，如果模型预测的准确，那么这个预测向量就应该是当前词对应的one-shot向量（这个维度的概率为1，其他维度的概率为0嘛）</eos></p>
<p>这样，我们就可以计算损失了，即预测向量与one-shot向量的<a href="https://colah.github.io/posts/2015-09-Visual-Information/">交叉熵</a>或者是<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">KL散度</a>等等</p>
<h3 id="并行">并行</h3>
<p>在进行注意力计算的时候，某个token的计算会依赖于其他token，但在实际的计算过程中是进行矩阵乘法，所以仍然是并行计算，不过在前馈层是没有token间的依赖关系的，可以单个token直接进行计算，所以可以并行计算（也是矩阵乘法）</p></div></article><div class="max-xl:hidden  "> <div id="nav-content" class="bg-white dark:bg-transparent sticky w-72 mt-8 rounded-2xl dark:border-0 border border-neutral-100 top-14 max-h-[calc(100svh-3.5rem)] overflow-x-hidden px-6 pt-8 pb-12"> <div class="flex flex-col gap-4 pl-0"> <div> <h3 class="dark:text-zinc-400 text-blacktext/90 font-black tracking-wide text-md uppercase">Table of Contents</h3> </div> <div class="flex flex-col gap-2 pr-6 text-neutral-500 dark:text-neutral-300 "> <ul id="toc-list" class="leading-loose text-base gap-2 border-l dark:border-neutral-500/20 border-blacktext/20"> <li class="leading-loose"> <a class="inline-block leading-5 pl-4 font-bold text-white border-l dark:border-white border-blacktext dark:hover:border-white hover:border-blacktext" href="#">Transformer From Scratch</a> </li> </ul> </div> </div> </div> </div> <script type="module">document.addEventListener("DOMContentLoaded",function(){const d=document.getElementById("toc-list"),c=document.getElementById("content");if(!d||!c)return;const s=c.querySelectorAll("h2, h3");let r=d;s.forEach((e,n)=>{e.id||(e.id=e.textContent?.trim().toLowerCase().replace(/\s+/g,"-")+"-"+n);const l=document.createElement("li"),t=document.createElement("a");if(t.href=`#${e.id}`,t.textContent=e.textContent?.trim()||e.id,t.classList.add("inline-block","leading-5","hover:text-mint-400","py-2","border-l","border-transparent","dark:hover:border-white","hover:border-blacktext"),t.classList.add(e.tagName==="H2"?"pl-6":"pl-12"),console.log("classes removed 2"),l.appendChild(t),e.tagName==="H2"){r=document.createElement("ul"),r.classList.add("border-neutral-400","dark:hover:border-white","hover:border-blacktext","pl-0"),console.log("classes removed 3");const o=document.createElement("li");o.appendChild(t),o.appendChild(r),d.appendChild(o)}else r.appendChild(l);t.addEventListener("click",function(o){o.preventDefault(),document.getElementById(e.id)?.scrollIntoView({behavior:"smooth",block:"start"})})});const i=new IntersectionObserver(e=>{e.forEach(n=>{const l=n.target.getAttribute("id"),t=document.querySelector(`a[href="#${l}"]`);n.isIntersecting&&(document.querySelectorAll("#toc-list a").forEach(o=>{o.classList.remove("font-semibold","dark:text-mint-300!","text-blacktext!","dark:border-white!","border-blacktext!"),o.classList.add("dark:text-neutral-300","text-neutral-500"),console.log("classes removed 4")}),t?.classList.add("font-semibold","dark:text-mint-300!","text-blacktext!","border-l","dark:border-white!","border-blacktext!"))})},{rootMargin:"-30% 0px -65% 0px",threshold:.1});s.forEach(e=>i.observe(e))});</script></section><div class="flex flex-col gap-6 max-w-4xl max-lg:py-2 py-3 max-xl:py-2 mx-auto"><div class="px-8"><nav class="mt-8 flex flex-row gap-2 w-full p-6 max-xl:p-3 max-lg:p-2"> <a href="/blog/posts/markdown-tutorial" style="width: -webkit-fill-available;" class="relative flex min-w-1/2 items-center justify-start gap-2 font-semibold dark:text-white text-blacktext text-left text-pretty max-sm:text-xs max-md:text-sm max-md:leading-4 hover:text-mint-300 hover:[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)] transition-all before:absolute before:-top-5 before:left-0 before:text-sm before:font-light before:content-['Previous_Post']"> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.85355 3.85355C7.04882 3.65829 7.04882 3.34171 6.85355 3.14645C6.65829 2.95118 6.34171 2.95118 6.14645 3.14645L2.14645 7.14645C1.95118 7.34171 1.95118 7.65829 2.14645 7.85355L6.14645 11.8536C6.34171 12.0488 6.65829 12.0488 6.85355 11.8536C7.04882 11.6583 7.04882 11.3417 6.85355 11.1464L3.20711 7.5L6.85355 3.85355ZM12.8536 3.85355C13.0488 3.65829 13.0488 3.34171 12.8536 3.14645C12.6583 2.95118 12.3417 2.95118 12.1464 3.14645L8.14645 7.14645C7.95118 7.34171 7.95118 7.65829 8.14645 7.85355L12.1464 11.8536C12.3417 12.0488 12.6583 12.0488 12.8536 11.8536C13.0488 11.6583 13.0488 11.3417 12.8536 11.1464L9.20711 7.5L12.8536 3.85355Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> The Complete Markdown Guide </a> <a href="/blog/posts/CodeUnderstandingOfTransformer" style="width: -webkit-fill-available;" class="relative flex min-w-1/2 items-center justify-end gap-2 font-semibold  dark:text-white text-blacktext  text-right text-pretty max-sm:text-xs max-md:text-sm max-md:leading-4 hover:text-mint-300 hover:[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)] transition-all before:absolute before:-top-5 before:right-0 before:text-sm before:font-light before:content-['Next_Post']"> Transformer代码深入理解 <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.14645 11.1464C1.95118 11.3417 1.95118 11.6583 2.14645 11.8536C2.34171 12.0488 2.65829 12.0488 2.85355 11.8536L6.85355 7.85355C7.04882 7.65829 7.04882 7.34171 6.85355 7.14645L2.85355 3.14645C2.65829 2.95118 2.34171 2.95118 2.14645 3.14645C1.95118 3.34171 1.95118 3.65829 2.14645 3.85355L5.79289 7.5L2.14645 11.1464ZM8.14645 11.1464C7.95118 11.3417 7.95118 11.6583 8.14645 11.8536C8.34171 12.0488 8.65829 12.0488 8.85355 11.8536L12.8536 7.85355C13.0488 7.65829 13.0488 7.34171 12.8536 7.14645L8.85355 3.14645C8.65829 2.95118 8.34171 2.95118 8.14645 3.14645C7.95118 3.34171 7.95118 3.65829 8.14645 3.85355L11.7929 7.5L8.14645 11.1464Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> </a> </nav><hr class="text-mint-300/50"></div><div class="px-8 max-sm:px-4"><section class="py-8 max-lg:px-4 max-md:px-8 max-sm:px-0 max-md:py-4 max-w-4xl mx-auto">   <div class="flex flex-col gap-8 w-full mx-auto"> <article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/nanochat/gpt.py" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> February 1, 2026</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">nanochat学习-模型架构</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" class="size-4 rotate-180" data-icon="arrow-left">   <symbol id="ai:local:arrow-left" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M6.854 3.146a.5.5 0 0 1 0 .708L3.707 7H12.5a.5.5 0 0 1 0 1H3.707l3.147 3.146a.5.5 0 0 1-.708.708l-4-4a.5.5 0 0 1 0-.708l4-4a.5.5 0 0 1 .708 0" clip-rule="evenodd"/></symbol><use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/python" aria-label="View articles about Python" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Python icon"><svg width="1em" height="1em" viewBox="16 16 32 32" class="w-full!" data-icon="python">   <use href="#ai:local:python"></use>  </svg></div>Python</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/Model Architecture" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Model Architecture </a><a href="/blog/tags/nanochat" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> nanochat </a> </div> </div> </div> <a href="/blog/posts/nanochat/gpt.py" style="background-image:url(/images/posts/nanochat.png)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/nanochat/start" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> February 1, 2026</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">nanochat学习-开头</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/python" aria-label="View articles about Python" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Python icon"><svg width="1em" height="1em" viewBox="16 16 32 32" class="w-full!" data-icon="python">   <use href="#ai:local:python"></use>  </svg></div>Python</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/nanochat" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> nanochat </a> </div> </div> </div> <a href="/blog/posts/nanochat/start" style="background-image:url(/images/posts/nanochat.png)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/KeYan/NL2SQL" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> October 30, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">论文-code生成</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/markdown" aria-label="View articles about Markdown" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Markdown icon"><svg width="1.63em" height="1em" class="w-full!" data-icon="markdown">   <symbol id="ai:local:markdown" viewBox="0 0 208 128"><path fill="none" stroke="#FFF" stroke-width="10" d="M15 5h178a10 10 0 0 1 10 10v98a10 10 0 0 1-10 10H15a10 10 0 0 1-10-10V15A10 10 0 0 1 15 5z"/><path fill="#FFF" d="M30 98V30h20l20 25 20-25h20v68H90V59L70 84 50 59v39zm125 0-30-33h20V30h20v35h20z"/></symbol><use href="#ai:local:markdown"></use>  </svg></div>Markdown</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/code generation" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> code generation </a> </div> </div> </div> <a href="/blog/posts/KeYan/NL2SQL" style="background-image:url(/images/posts/image-13.png)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/Postgresql" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> October 23, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">Postgresql 数据库</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/markdown" aria-label="View articles about Markdown" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Markdown icon"><svg width="1.63em" height="1em" viewBox="0 0 208 128" class="w-full!" data-icon="markdown">   <use href="#ai:local:markdown"></use>  </svg></div>Markdown</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/Database" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Database </a> </div> </div> </div> <a href="/blog/posts/Postgresql" style="background-image:url(/images/posts/126475690_p1.jpg)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article> </div> <div id="morePosts" class="w-full flex justify-center text-center my-12"> <a href="/blog/posts/" class="font-bold cursor-pointer text-mint-400 dark:text-mint-100 hover:text-mint-500 dark:hover:text-mint-300 transition-all">
View all posts...
</a> </div> </section></div></div> </div> <footer class="relative bottom-0 w-full px-4 py-8 font-medium text-blacktext dark:bg-transparent dark:border-b-2 dark:border-zinc-800 dark:text-zinc-300 max-lg:mt-3" role="contentinfo" aria-label="Site footer"> <nav class="mx-auto flex max-w-7xl flex-row items-center justify-between gap-4 text-xl max-xl:px-6 max-sm:flex-col" aria-label="Footer navigation"> <div class="relative h-6 cursor-pointer before:absolute before:left-1/2 before:top-1/2 before:h-full before:w-[40%] before:-translate-x-1/2 before:-translate-y-1/2 before:rounded-full before:bg-[#50fd8f25] before:blur-3xl before:opacity-80 before:-z-1 hover:text-mint-500 transition-all [text-shadow:0_1px_2px_#000]"> <a href="/" aria-label="Return to homepage"> <!-- This icon represents the logo --> <svg width="80" height="24" viewBox="0 0 80 24" fill="none" xmlns="http://www.w3.org/2000/svg"> <defs> <linearGradient id="logoGradient" x1="0%" y1="0%" x2="100%" y2="0%"> <stop offset="0%" style="stop-color:#38bdf8;stop-opacity:1"></stop> <stop offset="100%" style="stop-color:#3b82f6;stop-opacity:1"></stop> </linearGradient> </defs> <text x="0" y="18" font-family="Arial, sans-serif" font-weight="bold" font-size="16" fill="url(#logoGradient)">
oGYCo
</text> </svg> </a> </div> <div class="flex items-center justify-center gap-5" role="list" aria-label="Social media links"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="mailto:473815274@qq.com" rel="noopener noreferrer" aria-label="Send email to 473815274@qq.com"> <svg width="1em" height="1em" aria-hidden="true" data-icon="envelope">   <symbol id="ai:local:envelope" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M1 2a1 1 0 0 0-1 1v9a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1zm0 1h13v.925a.45.45 0 0 0-.241.07L7.5 7.967 1.241 3.995A.45.45 0 0 0 1 3.925zm0 1.908V12h13V4.908L7.741 8.88a.45.45 0 0 1-.482 0z" clip-rule="evenodd"/></symbol><use href="#ai:local:envelope"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://instagram.com/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on Instagram"> <svg width="1em" height="1em" aria-hidden="true" data-icon="instagram">   <symbol id="ai:local:instagram" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M12.91 12.909c.326-.327.582-.72.749-1.151.16-.414.27-.886.302-1.578s.04-.915.04-2.68-.008-1.987-.04-2.68c-.032-.692-.142-1.164-.302-1.578a3.2 3.2 0 0 0-.75-1.151 3.2 3.2 0 0 0-1.151-.75c-.414-.16-.886-.27-1.578-.302S9.265 1 7.5 1s-1.987.007-2.68.04c-.692.03-1.164.14-1.578.301a3.2 3.2 0 0 0-1.151.75 3.2 3.2 0 0 0-.75 1.151c-.16.414-.27.886-.302 1.578S1 5.735 1 7.5s.007 1.987.04 2.68c.03.692.14 1.164.301 1.578.164.434.42.826.75 1.151.325.33.718.586 1.151.75.414.16.886.27 1.578.302S5.735 14 7.5 14s1.987-.008 2.68-.04c.692-.03 1.164-.14 1.578-.301a3.3 3.3 0 0 0 1.151-.75M2 6.735v1.53c-.002.821-.002 1.034.02 1.5.026.586.058 1.016.156 1.34.094.312.199.63.543 1.012.344.383.675.556 1.097.684.423.127.954.154 1.415.175.522.024.73.024 1.826.024H8.24c.842.001 1.054.002 1.526-.02.585-.027 1.015-.059 1.34-.156.311-.094.629-.2 1.011-.543.383-.344.556-.676.684-1.098.127-.422.155-.953.176-1.414C13 9.247 13 9.04 13 7.947v-.89c0-1.096 0-1.303-.023-1.826-.021-.461-.049-.992-.176-1.414-.127-.423-.3-.754-.684-1.098-.383-.344-.7-.449-1.011-.543-.325-.097-.755-.13-1.34-.156A27 27 0 0 0 8.24 2H7.057c-1.096 0-1.304 0-1.826.023-.461.021-.992.049-1.415.176-.422.128-.753.301-1.097.684s-.45.7-.543 1.012c-.098.324-.13.754-.156 1.34-.022.466-.022.679-.02 1.5M7.5 5.25a2.25 2.25 0 1 0 0 4.5 2.25 2.25 0 0 0 0-4.5M4.25 7.5a3.25 3.25 0 1 1 6.5 0 3.25 3.25 0 0 1-6.5 0m6.72-2.72a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" clip-rule="evenodd"/></symbol><use href="#ai:local:instagram"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://youtube.com/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on YouTube"> <svg width="1em" height="1em" aria-hidden="true" data-icon="youtube">   <symbol id="ai:local:youtube" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M4.764 3.122A33 33 0 0 1 7.5 3c.94 0 1.868.049 2.736.122 1.044.088 1.72.148 2.236.27.47.111.733.258.959.489.024.025.06.063.082.09.2.23.33.518.405 1.062.08.583.082 1.343.082 2.492 0 1.135-.002 1.885-.082 2.46-.074.536-.204.821-.405 1.054l-.083.09c-.23.234-.49.379-.948.487-.507.12-1.168.178-2.194.264-.869.072-1.812.12-2.788.12s-1.92-.048-2.788-.12c-1.026-.086-1.687-.144-2.194-.264-.459-.108-.719-.253-.948-.487l-.083-.09c-.2-.233-.33-.518-.405-1.054C1.002 9.41 1 8.66 1 7.525c0-1.149.002-1.91.082-2.492.075-.544.205-.832.405-1.062.023-.027.058-.065.082-.09.226-.231.489-.378.959-.489.517-.122 1.192-.182 2.236-.27M0 7.525c0-2.242 0-3.363.73-4.208.036-.042.085-.095.124-.135.78-.799 1.796-.885 3.826-1.056C5.57 2.05 6.527 2 7.5 2s1.93.05 2.82.126c2.03.171 3.046.257 3.826 1.056.039.04.087.093.124.135.73.845.73 1.966.73 4.208 0 2.215 0 3.323-.731 4.168a3 3 0 0 1-.125.135c-.781.799-1.778.882-3.773 1.048C9.48 12.951 8.508 13 7.5 13s-1.98-.05-2.87-.124c-1.996-.166-2.993-.25-3.774-1.048a3 3 0 0 1-.125-.135C0 10.848 0 9.74 0 7.525m5.25-2.142a.25.25 0 0 1 .35-.23l4.828 2.118c.2.088.2.37 0 .458L5.6 9.846a.25.25 0 0 1-.35-.229z" clip-rule="evenodd"/></symbol><use href="#ai:local:youtube"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Visit oGYCo on GitHub"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="github">   <use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on LinkedIn"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="linkedin">   <use href="#ai:local:linkedin"></use>  </svg> </a> </div> </nav> </footer> <script type="module">document.querySelector(".hamburger").addEventListener("click",()=>{const e=document.querySelector(".nav-links"),t=document.querySelector(".hamburger");e.classList.toggle("expanded"),t.classList.toggle("active")});</script> </body> </html>