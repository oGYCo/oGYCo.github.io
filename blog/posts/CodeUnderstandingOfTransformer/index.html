<!DOCTYPE html><script type="module">const o='<svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5 2V1H10V2H5ZM4.75 0C4.33579 0 4 0.335786 4 0.75V1H3.5C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H7V13H3.5C3.22386 13 3 12.7761 3 12.5V2.5C3 2.22386 3.22386 2 3.5 2H4V2.25C4 2.66421 4.33579 3 4.75 3H10.25C10.6642 3 11 2.66421 11 2.25V2H11.5C11.7761 2 12 2.22386 12 2.5V7H13V2.5C13 1.67157 12.3284 1 11.5 1H11V0.75C11 0.335786 10.6642 0 10.25 0H4.75ZM9 8.5C9 8.77614 8.77614 9 8.5 9C8.22386 9 8 8.77614 8 8.5C8 8.22386 8.22386 8 8.5 8C8.77614 8 9 8.22386 9 8.5ZM10.5 9C10.7761 9 11 8.77614 11 8.5C11 8.22386 10.7761 8 10.5 8C10.2239 8 10 8.22386 10 8.5C10 8.77614 10.2239 9 10.5 9ZM13 8.5C13 8.77614 12.7761 9 12.5 9C12.2239 9 12 8.77614 12 8.5C12 8.22386 12.2239 8 12.5 8C12.7761 8 13 8.22386 13 8.5ZM14.5 9C14.7761 9 15 8.77614 15 8.5C15 8.22386 14.7761 8 14.5 8C14.2239 8 14 8.22386 14 8.5C14 8.77614 14.2239 9 14.5 9ZM15 10.5C15 10.7761 14.7761 11 14.5 11C14.2239 11 14 10.7761 14 10.5C14 10.2239 14.2239 10 14.5 10C14.7761 10 15 10.2239 15 10.5ZM14.5 13C14.7761 13 15 12.7761 15 12.5C15 12.2239 14.7761 12 14.5 12C14.2239 12 14 12.2239 14 12.5C14 12.7761 14.2239 13 14.5 13ZM14.5 15C14.7761 15 15 14.7761 15 14.5C15 14.2239 14.7761 14 14.5 14C14.2239 14 14 14.2239 14 14.5C14 14.7761 14.2239 15 14.5 15ZM8.5 11C8.77614 11 9 10.7761 9 10.5C9 10.2239 8.77614 10 8.5 10C8.22386 10 8 10.2239 8 10.5C8 10.7761 8.22386 11 8.5 11ZM9 12.5C9 12.7761 8.77614 13 8.5 13C8.22386 13 8 12.7761 8 12.5C8 12.2239 8.22386 12 8.5 12C8.77614 12 9 12.2239 9 12.5ZM8.5 15C8.77614 15 9 14.7761 9 14.5C9 14.2239 8.77614 14 8.5 14C8.22386 14 8 14.2239 8 14.5C8 14.7761 8.22386 15 8.5 15ZM11 14.5C11 14.7761 10.7761 15 10.5 15C10.2239 15 10 14.7761 10 14.5C10 14.2239 10.2239 14 10.5 14C10.7761 14 11 14.2239 11 14.5ZM12.5 15C12.7761 15 13 14.7761 13 14.5C13 14.2239 12.7761 14 12.5 14C12.2239 14 12 14.2239 12 14.5C12 14.7761 12.2239 15 12.5 15Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>';let n=Array.from(document.querySelectorAll("pre"));for(let e of n){let C=document.createElement("div");C.style.position="relative",C.classList.add("code-block-efe","group");let t=document.createElement("button");t.className="copy-code",t.innerHTML=o,e.setAttribute("tabindex","0"),e.parentNode&&(e.parentNode.insertBefore(C,e),C.appendChild(e),C.appendChild(t)),t.addEventListener("click",async()=>{await r(e,t)})}async function r(e,C){let t=e.querySelector("code");if(!t)return;let l=t.innerText;await navigator.clipboard.writeText(l),C.innerHTML='<svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.4669 3.72684C11.7558 3.91574 11.8369 4.30308 11.648 4.59198L7.39799 11.092C7.29783 11.2452 7.13556 11.3467 6.95402 11.3699C6.77247 11.3931 6.58989 11.3355 6.45446 11.2124L3.70446 8.71241C3.44905 8.48022 3.43023 8.08494 3.66242 7.82953C3.89461 7.57412 4.28989 7.55529 4.5453 7.78749L6.75292 9.79441L10.6018 3.90792C10.7907 3.61902 11.178 3.53795 11.4669 3.72684Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>&nbsp;Copied',setTimeout(()=>{C.innerHTML=o},700)}</script> <html lang="en" class="dark:bg-[#0E0E11] bg-mint-50/5 font-montserrat scroll-smooth"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><title>Transformer代码深入理解</title><!-- Favicon for different platforms --><link rel="icon" href="/favicon-32x32.png" sizes="32x32"><link rel="icon" href="/favicon-16x16.png" sizes="16x16"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="icon" href="/android-chrome-192x192.png" sizes="192x192"><link rel="icon" href="/android-chrome-512x512.png" sizes="512x512"><!-- Web manifest --><link rel="manifest" href="/site.webmanifest"><meta name="description" content="Annotated Transformer"><meta name="author" content="oGYCo"><meta name="robots" content="index, follow"><link rel="canonical" href="https://ogyco.github.io//blog/posts/CodeUnderstandingOfTransformer/"><!-- Open Graph / Facebook --><meta property="og:title" content="Transformer代码深入理解"><meta property="og:description" content="Annotated Transformer"><meta property="og:image" content="/images/posts/Annotated_Transformer.png"><meta property="og:image:alt" content="Harvard's code of transformer"><meta property="og:url" content="https://ogyco.github.io"><meta property="og:type" content="website"><meta property="og:site_name" content="My Page | oGYCo"><meta property="og:locale" content="en_US"><!-- Twitter Cards --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Transformer代码深入理解"><meta name="twitter:description" content="Annotated Transformer"><meta name="twitter:image" content="https://ogyco.github.io/images/posts/Annotated_Transformer.png"><meta name="twitter:image:alt" content="Harvard's code of transformer"><meta name="twitter:site" content><meta name="theme-color" content="#0E0E11"><!-- Sitemap --><link rel="sitemap" href="/sitemap-index.xml"><link rel="stylesheet" href="/_astro/about-me.CZDa078-.css"></head> <body class="overflow-x-hidden"> <vercel-speed-insights data-props="{}" data-params="{}" data-pathname="/blog/posts/CodeUnderstandingOfTransformer/"></vercel-speed-insights> <script type="module">var o="@vercel/speed-insights",u="1.2.0",f=()=>{window.si||(window.si=function(...r){(window.siq=window.siq||[]).push(r)})};function l(){return typeof window<"u"}function h(){try{const e="production"}catch{}return"production"}function d(){return h()==="development"}function v(e,r){if(!e||!r)return e;let n=e;try{const t=Object.entries(r);for(const[s,i]of t)if(!Array.isArray(i)){const a=c(i);a.test(n)&&(n=n.replace(a,`/[${s}]`))}for(const[s,i]of t)if(Array.isArray(i)){const a=c(i.join("/"));a.test(n)&&(n=n.replace(a,`/[...${s}]`))}return n}catch{return e}}function c(e){return new RegExp(`/${g(e)}(?=[/?#]|$)`)}function g(e){return e.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function m(e){return e.scriptSrc?e.scriptSrc:d()?"https://va.vercel-scripts.com/v1/speed-insights/script.debug.js":e.dsn?"https://va.vercel-scripts.com/v1/speed-insights/script.js":e.basePath?`${e.basePath}/speed-insights/script.js`:"/_vercel/speed-insights/script.js"}function w(e={}){var r;if(!l()||e.route===null)return null;f();const n=m(e);if(document.head.querySelector(`script[src*="${n}"]`))return null;e.beforeSend&&((r=window.si)==null||r.call(window,"beforeSend",e.beforeSend));const t=document.createElement("script");return t.src=n,t.defer=!0,t.dataset.sdkn=o+(e.framework?`/${e.framework}`:""),t.dataset.sdkv=u,e.sampleRate&&(t.dataset.sampleRate=e.sampleRate.toString()),e.route&&(t.dataset.route=e.route),e.endpoint?t.dataset.endpoint=e.endpoint:e.basePath&&(t.dataset.endpoint=`${e.basePath}/speed-insights/vitals`),e.dsn&&(t.dataset.dsn=e.dsn),d()&&e.debug===!1&&(t.dataset.debug="false"),t.onerror=()=>{console.log(`[Vercel Speed Insights] Failed to load script from ${n}. Please check if any content blockers are enabled and try again.`)},document.head.appendChild(t),{setRoute:s=>{t.dataset.route=s??void 0}}}function p(){try{return}catch{}}customElements.define("vercel-speed-insights",class extends HTMLElement{constructor(){super();try{const r=JSON.parse(this.dataset.props??"{}"),n=JSON.parse(this.dataset.params??"{}"),t=v(this.dataset.pathname??"",n);w({route:t,...r,framework:"astro",basePath:p(),beforeSend:window.speedInsightsBeforeSend})}catch(r){throw new Error(`Failed to parse SpeedInsights properties: ${r}`)}}});</script> <div class="blur-circle"></div> <div style="  background:linear-gradient(45deg, rgba(96, 250, 155, 0) 10.79%, rgba(96, 250, 170, 0.03) 40.92%, rgba(96, 250, 155, 0) 90.35%)" class="fixed top-0 left-0 w-full h-full pointer-events-none -z-1"></div> <header role="banner" aria-label="Main navigation" class="sticky top-0 z-50 w-full p-4 font-medium text-blacktext dark:text-zinc-300 dark:bg-[#0E0E11]/80 dark:border-b dark:border-zinc-800 bg-white/90 backdrop-blur-xs dark:backdrop-blur-xs max-md:z-50 max-md:px-0 transition-all"> <div class="relative mx-auto flex max-w-7xl flex-row items-center justify-between max-xl:px-6"> <a href="/" aria-label="Go to home"> <svg width="80" height="24" viewBox="0 0 80 24" fill="none" xmlns="http://www.w3.org/2000/svg"> <defs> <linearGradient id="logoGradient" x1="0%" y1="0%" x2="100%" y2="0%"> <stop offset="0%" style="stop-color:#38bdf8;stop-opacity:1"></stop> <stop offset="100%" style="stop-color:#3b82f6;stop-opacity:1"></stop> </linearGradient> </defs> <text x="0" y="18" font-family="Arial, sans-serif" font-weight="bold" font-size="16" fill="url(#logoGradient)">
oGYCo
</text> </svg> </a> <script type="module">document.addEventListener("DOMContentLoaded",()=>{const h=window.location.pathname==="/",n={active:["text-mint-500","dark:text-mint-400","font-bold","[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)]"],inactive:["dark:text-zinc-300","text-blacktext"]};function c(t,e){e?(t.classList.add(...n.active),t.classList.remove(...n.inactive),t.setAttribute("aria-current","page")):(t.classList.remove(...n.active),t.classList.add(...n.inactive),t.removeAttribute("aria-current"))}function i(){const t=window.location.pathname,e=window.location.hash?`#${window.location.hash.substring(1)}`:"";document.querySelectorAll("nav a").forEach(o=>{const a=o.getAttribute("data-path");c(o,a===t||a===e)})}function l(){if(!h)return;const t=document.querySelectorAll("section[id]"),e=document.querySelectorAll("nav a"),o={root:null,rootMargin:"-50% 0px",threshold:0},a=new IntersectionObserver(s=>{s.forEach(r=>{if(r.isIntersecting){const d=r.target.getAttribute("id");d&&e.forEach(u=>{const v=u.getAttribute("data-path");c(u,v===`/#${d}`)})}})},o);t.forEach(s=>a.observe(s))}i(),l(),window.addEventListener("hashchange",i)});</script> <nav class="nav-links flex w-full justify-center gap-6 max-md:gap-3 max-md:py-6" role="navigation" aria-label="Main Navigation"> <a href="/#home" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/#home" aria-label="Home"> Home </a><div class="relative group flex items-center"> <a href="/blog/" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/blog/" aria-label="Blog"> Blog </a> <div class="absolute left-0 top-full hidden group-hover:block bg-white dark:bg-zinc-800 shadow-lg rounded-md py-2 min-w-[200px] z-50"> <a href="/blog/posts/" class="block px-4 py-2 text-sm hover:bg-mint-100 dark:hover:bg-zinc-700 transition-colors" data-path="/blog/posts/" aria-label="All Posts"> All Posts </a> </div> </div><a href="/about-me" class="px-2 py-2 transition-all hover:text-mint-300 max-md:mx-auto max-md:w-full max-md:px-6 max-md:py-2 " data-path="/about-me" aria-label="About Me"> About Me </a> <div class="flex items-center justify-center gap-5 md:hidden" role="group" aria-label="Social Media Links"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Link to github"> <svg width="1em" height="1em" aria-hidden="true" data-icon="github">   <symbol id="ai:local:github" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M7.5.25a7.25 7.25 0 0 0-2.292 14.13c.363.066.495-.158.495-.35 0-.172-.006-.628-.01-1.233-2.016.438-2.442-.972-2.442-.972-.33-.838-.805-1.06-.805-1.06-.658-.45.05-.441.05-.441.728.051 1.11.747 1.11.747.647 1.108 1.697.788 2.11.602.066-.468.254-.788.46-.969-1.61-.183-3.302-.805-3.302-3.583a2.8 2.8 0 0 1 .747-1.945c-.075-.184-.324-.92.07-1.92 0 0 .61-.194 1.994.744A7 7 0 0 1 7.5 3.756 7 7 0 0 1 9.315 4c1.384-.938 1.992-.743 1.992-.743.396.998.147 1.735.072 1.919.465.507.745 1.153.745 1.945 0 2.785-1.695 3.398-3.31 3.577.26.224.492.667.492 1.343 0 .97-.009 1.751-.009 1.989 0 .194.131.42.499.349A7.25 7.25 0 0 0 7.499.25" clip-rule="evenodd"/></symbol><use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Link to linkedin"> <svg width="1em" height="1em" aria-hidden="true" data-icon="linkedin">   <symbol id="ai:local:linkedin" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M2 1a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h11a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1zm1.05 5h1.9v6h-1.9zm2.025-1.995a1.075 1.075 0 1 1-2.15 0 1.075 1.075 0 0 1 2.15 0M12 8.357c0-1.805-1.167-2.507-2.326-2.507a2.2 2.2 0 0 0-1.095.231c-.257.13-.526.424-.734.938h-.053V6H6v6.005h1.906V8.81c-.027-.327.077-.75.291-1.001.215-.252.52-.312.753-.342h.073c.606 0 1.056.375 1.056 1.32v3.217h1.906z" clip-rule="evenodd"/></symbol><use href="#ai:local:linkedin"></use>  </svg> </a> </div> </nav> <div class="flex items-center justify-between gap-5 text-xl"> <div class="max-md:hidden flex items-center justify-center gap-5" role="list"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Link to github"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="github">   <use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Link to linkedin"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="linkedin">   <use href="#ai:local:linkedin"></use>  </svg> </a> </div> </div> <div class="flex items-center gap-5 text-xl md:pl-5"> <button id="themeToggle" class="hover:cursor-pointer hover:text-mint-400 transition-all" data-astro-cid-htzy5xbu> <svg width="1em" height="1em" class="sun" data-astro-cid-htzy5xbu="true" data-icon="sun">   <symbol id="ai:local:sun" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M7.5 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2a.5.5 0 0 1 .5-.5M2.197 2.197a.5.5 0 0 1 .707 0L4.318 3.61a.5.5 0 0 1-.707.707L2.197 2.904a.5.5 0 0 1 0-.707M.5 7a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1zm1.697 5.803a.5.5 0 0 1 0-.707l1.414-1.414a.5.5 0 1 1 .707.707l-1.414 1.414a.5.5 0 0 1-.707 0M12.5 7a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1zm-1.818-2.682a.5.5 0 0 1 0-.707l1.414-1.414a.5.5 0 1 1 .707.707L11.39 4.318a.5.5 0 0 1-.707 0M8 12.5a.5.5 0 0 0-1 0v2a.5.5 0 0 0 1 0zm2.682-1.818a.5.5 0 0 1 .707 0l1.414 1.414a.5.5 0 1 1-.707.707l-1.414-1.414a.5.5 0 0 1 0-.707M5.5 7.5a2 2 0 1 1 4 0 2 2 0 0 1-4 0m2-3a3 3 0 1 0 0 6 3 3 0 0 0 0-6" clip-rule="evenodd"/></symbol><use href="#ai:local:sun"></use>  </svg> <svg width="1em" height="1em" class="moon" data-astro-cid-htzy5xbu="true" data-icon="moon">   <symbol id="ai:local:moon" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M2.9.5a.4.4 0 0 0-.8 0v.6h-.6a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 1 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zm3 3a.4.4 0 1 0-.8 0v.6h-.6a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 1 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zm-4 3a.4.4 0 1 0-.8 0v.6H.5a.4.4 0 1 0 0 .8h.6v.6a.4.4 0 0 0 .8 0v-.6h.6a.4.4 0 0 0 0-.8h-.6zM8.544.982l-.298-.04c-.213-.024-.34.224-.217.4q.211.305.389.632A6.602 6.602 0 0 1 2.96 11.69c-.215.012-.334.264-.184.417q.103.105.21.206l.072.066.26.226.188.148.121.09.187.131.176.115q.18.115.37.217l.264.135.26.12.303.122.244.086a6.6 6.6 0 0 0 1.103.26l.317.04.267.02q.19.011.384.011a6.6 6.6 0 0 0 6.56-7.339l-.038-.277a6.6 6.6 0 0 0-.384-1.415l-.113-.268-.077-.166-.074-.148a6.6 6.6 0 0 0-.546-.883l-.153-.2-.199-.24-.163-.18-.12-.124-.16-.158-.223-.2-.32-.26-.245-.177-.292-.19-.321-.186-.328-.165-.113-.052-.24-.101-.276-.104-.252-.082-.325-.09-.265-.06zm1.86 4.318a7.6 7.6 0 0 0-.572-2.894 5.601 5.601 0 1 1-4.748 10.146 7.6 7.6 0 0 0 3.66-2.51.749.749 0 0 0 1.355-.442.75.75 0 0 0-.584-.732q.093-.174.178-.355A1.25 1.25 0 1 0 10.35 6.2q.052-.442.052-.9" clip-rule="evenodd"/></symbol><use href="#ai:local:moon"></use>  </svg> </button>  <script>
    // Execute immediately before the page loads
    (function() {
        const theme = (() => {
            if (typeof localStorage !== "undefined" && localStorage.getItem("theme")) {
                return localStorage.getItem("theme");
            }
            if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                return "dark";
            }
            return "light";
        })();

        if (theme === "light") {
            document.documentElement.classList.remove("dark");
        } else {
            document.documentElement.classList.add("dark");
        }

        window.localStorage.setItem("theme", theme);
    })();

    // Listen for changes in system preferences
    window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change", (e) => {
        if (!localStorage.getItem("theme")) {
            if (e.matches) {
                document.documentElement.classList.add("dark");
            } else {
                document.documentElement.classList.remove("dark");
            }
        }
    });

    const handleToggleClick = () => {
        const element = document.documentElement;
        element.classList.toggle("dark");

        const isDark = element.classList.contains("dark");
        localStorage.setItem("theme", isDark ? "dark" : "light");
    };

    document
        .getElementById("themeToggle")
        .addEventListener("click", handleToggleClick);
</script> <button class="hamburger" aria-label="Open menu" aria-expanded="false" aria-controls="mobile-menu"> <svg width="1em" height="1em" class="hamburger-icon bars-icon" aria-hidden="true" data-icon="bars">   <symbol id="ai:local:bars" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M1.5 3a.5.5 0 0 0 0 1h12a.5.5 0 0 0 0-1zM1 7.5a.5.5 0 0 1 .5-.5h12a.5.5 0 0 1 0 1h-12a.5.5 0 0 1-.5-.5m0 4a.5.5 0 0 1 .5-.5h12a.5.5 0 0 1 0 1h-12a.5.5 0 0 1-.5-.5" clip-rule="evenodd"/></symbol><use href="#ai:local:bars"></use>  </svg> <svg width="1em" height="1em" class="hamburger-icon xmark-icon" aria-hidden="true" data-icon="xmark">   <symbol id="ai:local:xmark" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M12.854 2.854a.5.5 0 0 0-.708-.708L7.5 6.793 2.854 2.146a.5.5 0 1 0-.708.708L6.793 7.5l-4.647 4.646a.5.5 0 0 0 .708.708L7.5 8.207l4.646 4.647a.5.5 0 0 0 .708-.708L8.207 7.5z" clip-rule="evenodd"/></symbol><use href="#ai:local:xmark"></use>  </svg> </button> </div> </div> </header> <div class="min-h-[85vh] ">  <section class="relative mx-auto px-8 max-sm:px-4 flex flex-row justify-center gap-6"> <div class="w-64 max-xl:hidden"></div> <article class="flex flex-col gap-8 max-w-3xl max-md:w-full pb-10 pt-8 mt-8 px-14 max-md:px-10 max-sm:px-4 dark:bg-transparent bg-white dark:border-0 border border-neutral-100 rounded-2xl"> <header class="flex flex-col gap-4" id="start"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/python" aria-label="View articles about Python" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Python icon"><svg width="1em" height="1em" class="w-full!" data-icon="python">   <symbol id="ai:local:python" viewBox="16 16 32 32"><g fill="none"><path fill="url(#a)" d="M31.885 16c-8.124 0-7.617 3.523-7.617 3.523l.01 3.65h7.752v1.095H21.197S16 23.678 16 31.876c0 8.196 4.537 7.906 4.537 7.906h2.708v-3.804s-.146-4.537 4.465-4.537h7.688s4.32.07 4.32-4.175v-7.019S40.374 16 31.885 16m-4.275 2.454a1.394 1.394 0 1 1 0 2.79 1.393 1.393 0 0 1-1.395-1.395c0-.771.624-1.395 1.395-1.395"/><path fill="url(#b)" d="M32.115 47.833c8.124 0 7.617-3.523 7.617-3.523l-.01-3.65H31.97v-1.095h10.832S48 40.155 48 31.958s-4.537-7.906-4.537-7.906h-2.708v3.803s.146 4.537-4.465 4.537h-7.688s-4.32-.07-4.32 4.175v7.019s-.656 4.247 7.833 4.247m4.275-2.454a1.393 1.393 0 0 1-1.395-1.395 1.394 1.394 0 1 1 1.395 1.395"/><defs><linearGradient id="a" x1="19.075" x2="34.898" y1="18.782" y2="34.658" gradientUnits="userSpaceOnUse"><stop stop-color="#387EB8"/><stop offset="1" stop-color="#366994"/></linearGradient><linearGradient id="b" x1="28.809" x2="45.803" y1="28.882" y2="45.163" gradientUnits="userSpaceOnUse"><stop stop-color="#FFE052"/><stop offset="1" stop-color="#FFC331"/></linearGradient></defs></g></symbol><use href="#ai:local:python"></use>  </svg></div>Python</span></a> </div> <div class="gap-3 flex flex-wrap justify-start items-center"> <a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/Model Architecture" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Model Architecture </a><a href="/blog/tags/AIGC" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AIGC </a> </div> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> July 22, 2025</span> <h1 class="font-extrabold text-4xl max-xl:text-3xl dark:text-white text-blacktext">Transformer代码深入理解</h1> <div class="flex items-center max-sm:flex-col gap-2 max-sm:items-start justify-between"> <div class="white flex items-center gap-4"> <div aria-label="Sparkle" class="size-10 bg-cover bg-center rounded-full drop-shadow-lg" style="background-image: url(/images/1723118997692.png);"></div> <a id="Transformer代码深入理解" class="leading-0 text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50" href="/blog">oGYCo </a> </div> </div> </header> <figure> <img class="w-full rounded-xl" src="/images/posts/Annotated_Transformer.png" alt="Harvard's code of transformer"> </figure> <div class="markdown" id="content"> <p>全文主要由AI生成，是根据<a href="https://nlp.seas.harvard.edu/annotated-transformer/">哈弗Annotated Transformer代码实现</a>进行的代码解释</p>
<p>关于Transformer的内容理解见<a href="https://ogyco.github.io/blog/posts/TransformerFromScratch/">Transformer From Scratch</a></p>
<h2 id="单元格-6导入所有需要的库">单元格 6：导入所有需要的库</h2>
<h3 id="是什么-系统交互工具"><strong>是什么：</strong> 系统交互工具</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> os</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> os.path </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> exists</span></span></code></pre>
<p><strong>os 模块的本质：</strong></p>
<ul>
<li><code>os</code> 是”操作系统”的缩写，是 Python 标准库的一部分</li>
<li>这个模块提供了一套与操作系统交互的接口函数</li>
<li><code>exists</code> 函数专门用来检查文件或目录是否存在，返回布尔值</li>
</ul>
<h3 id="为什么需要-文件管理的必要性"><strong>为什么需要：</strong> 文件管理的必要性</h3>
<p>在深度学习项目中，我们经常需要：</p>
<ol>
<li><strong>检查模型文件是否存在</strong>：避免重复下载或训练</li>
<li><strong>创建保存目录</strong>：为训练结果和模型权重创建文件夹</li>
<li><strong>路径处理</strong>：在不同操作系统上正确处理文件路径</li>
<li><strong>环境检测</strong>：确认运行环境的配置</li>
</ol>
<h3 id="怎么做-具体应用场景"><strong>怎么做：</strong> 具体应用场景</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 典型使用示例</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#F97583"> not</span><span style="color:#E1E4E8"> exists(</span><span style="color:#9ECBFF">"model_weights.pt"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"模型文件不存在，开始训练..."</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"发现已保存的模型，加载中..."</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>实际用途：</strong>
在这个Transformer项目中，主要用于检查预训练模型、数据集文件是否存在，以及创建输出目录。</p>
<hr>
<h3 id="是什么-深度学习核心框架"><strong>是什么：</strong> 深度学习核心框架</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch.nn </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> nn</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torch.nn.functional </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> log_softmax, pad</span></span></code></pre>
<p><strong>PyTorch 框架体系：</strong></p>
<p><strong><code>torch</code> - 核心计算引擎：</strong></p>
<ul>
<li>PyTorch 是目前最流行的深度学习框架之一（由 Meta/Facebook 开发）</li>
<li>核心数据结构是 <strong>Tensor（张量）</strong>：多维数组，类似 numpy，但专为深度学习优化</li>
<li>提供<strong>自动微分</strong>：自动计算梯度，这是神经网络训练的核心</li>
<li>支持 <strong>GPU 加速</strong>：利用 CUDA 实现几十倍的计算加速</li>
</ul>
<p><strong><code>torch.nn</code> - 神经网络构建工具包：</strong></p>
<ul>
<li><code>nn</code> 是 “Neural Networks” 的缩写</li>
<li>包含构建神经网络的所有基本组件：
<ul>
<li><strong>层（Layers）</strong>：Linear（全连接）、Conv2d（卷积）、Embedding（嵌入）等</li>
<li><strong>激活函数</strong>：ReLU、Tanh、Sigmoid 等</li>
<li><strong>损失函数</strong>：CrossEntropyLoss、MSELoss 等</li>
<li><strong>容器</strong>：Sequential（顺序堆叠）、ModuleList（模块列表）等</li>
</ul>
</li>
</ul>
<p><strong><code>torch.nn.functional</code> - 函数式操作：</strong></p>
<ul>
<li>包含无状态的函数版本，不需要创建层对象</li>
<li><code>log_softmax</code>：对数softmax函数，用于多分类概率计算</li>
<li><code>pad</code>：张量填充函数，为序列添加padding</li>
</ul>
<h3 id="为什么选择这些-设计哲学与优势"><strong>为什么选择这些：</strong> 设计哲学与优势</h3>
<p><strong>为什么选择 PyTorch：</strong></p>
<ol>
<li><strong>动态计算图</strong>：更灵活，便于调试和实验</li>
<li><strong>Python原生</strong>：与Python生态无缝集成</li>
<li><strong>研究友好</strong>：易于实现复杂的研究想法</li>
<li><strong>生产就绪</strong>：TorchScript可以部署到生产环境</li>
</ol>
<p><strong>为什么需要 nn 模块：</strong></p>
<ol>
<li><strong>面向对象设计</strong>：将神经网络组件封装成类，便于管理</li>
<li><strong>参数自动管理</strong>：自动跟踪和更新可学习参数</li>
<li><strong>设备无关</strong>：一行代码即可在CPU/GPU间切换</li>
<li><strong>状态管理</strong>：自动处理训练/评估模式切换</li>
</ol>
<p><strong>为什么使用 functional：</strong></p>
<ol>
<li><strong>计算效率</strong>：某些操作的函数版本更高效</li>
<li><strong>代码简洁</strong>：不需要预先定义层对象</li>
<li><strong>灵活性</strong>：可以动态调整参数和行为</li>
</ol>
<h3 id="怎么做-在transformer中的具体应用"><strong>怎么做：</strong> 在Transformer中的具体应用</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 使用 nn 构建层</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linear </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">30000</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 全连接层：512维 -> 30000维（词汇表大小）</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 使用 functional 进行计算</span></span>
<span class="line"><span style="color:#E1E4E8">output_probs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> log_softmax(logits, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 计算对数概率</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 使用 pad 处理变长序列</span></span>
<span class="line"><span style="color:#E1E4E8">padded_sequences </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pad(sequences, (</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, max_len </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> seq_len))  </span><span style="color:#6A737D"># 右侧填充到最大长度</span></span></code></pre>
<p><strong>为什么这样设计：</strong>
Transformer 模型需要处理<strong>变长文本序列</strong>、进行<strong>大规模矩阵运算</strong>、支持<strong>并行计算</strong>，PyTorch 的这些模块完美满足了这些需求。</p>
<hr>
<h3 id="是什么-python标准库工具集"><strong>是什么：</strong> Python标准库工具集</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> math</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> copy</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> time</span></span></code></pre>
<p><strong>数学与工具模块详解：</strong></p>
<p><strong><code>math</code> - 数学运算库：</strong></p>
<ul>
<li>Python内置的数学函数库</li>
<li>提供基础数学函数：<code>sqrt()</code>、<code>log()</code>、<code>sin()</code>、<code>cos()</code>、<code>exp()</code> 等</li>
<li>常数：<code>math.pi</code>、<code>math.e</code> 等</li>
</ul>
<p><strong><code>copy</code> - 对象复制工具：</strong></p>
<ul>
<li>处理Python对象的复制操作</li>
<li><code>copy.copy()</code>：浅复制（shallow copy）</li>
<li><code>copy.deepcopy()</code>：深复制（deep copy），创建完全独立的副本</li>
</ul>
<p><strong><code>time</code> - 时间处理模块：</strong></p>
<ul>
<li>时间相关的功能：获取当前时间、计算时间差、休眠等</li>
<li>主要函数：<code>time.time()</code>、<code>time.sleep()</code></li>
</ul>
<h3 id="为什么需要这些-在transformer中的关键作用"><strong>为什么需要这些：</strong> 在Transformer中的关键作用</h3>
<p><strong>为什么需要 math：</strong></p>
<ol>
<li><strong>注意力缩放</strong>：使用 <code>math.sqrt(d_k)</code> 缩放注意力分数</li>
<li><strong>位置编码</strong>：使用三角函数 <code>sin</code>、<code>cos</code> 生成位置信息</li>
<li><strong>初始化</strong>：某些权重初始化方法需要数学计算</li>
</ol>
<p><strong>为什么需要 copy：</strong></p>
<ol>
<li><strong>层复制</strong>：Transformer有多层相同结构，需要创建独立的副本</li>
<li><strong>参数隔离</strong>：确保不同层的参数不会互相影响</li>
<li><strong>模型构建</strong>：避免多个层共享同一组参数</li>
</ol>
<p><strong>为什么需要 time：</strong></p>
<ol>
<li><strong>性能监控</strong>：测量训练和推理的时间消耗</li>
<li><strong>进度跟踪</strong>：计算剩余训练时间</li>
<li><strong>性能优化</strong>：识别计算瓶颈</li>
</ol>
<h3 id="怎么做-实际应用示例"><strong>怎么做：</strong> 实际应用示例</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># math 的使用</span></span>
<span class="line"><span style="color:#E1E4E8">attention_scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.matmul(Q, K.transpose(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> math.sqrt(d_k)</span></span>
<span class="line"><span style="color:#6A737D"># 为什么除以 sqrt(d_k)？防止 softmax 饱和，保持梯度流动</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># copy 的使用</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> clones</span><span style="color:#E1E4E8">(module, N):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> nn.ModuleList([copy.deepcopy(module) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(N)])</span></span>
<span class="line"><span style="color:#6A737D"># 创建N个相同但独立的层</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># time 的使用</span></span>
<span class="line"><span style="color:#E1E4E8">start_time </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time()</span></span>
<span class="line"><span style="color:#6A737D"># 训练代码...</span></span>
<span class="line"><span style="color:#E1E4E8">training_time </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time() </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> start_time</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"训练耗时: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">training_time</span><span style="color:#F97583">:.2f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> 秒"</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>设计原理：</strong>
这些工具看似简单，但在大规模神经网络中起到<strong>精确控制</strong>的作用：数学函数确保数值稳定性，深复制保证模型结构的正确性，时间监控帮助优化性能。</p>
<hr>
<h3 id="是什么-学习率优化工具"><strong>是什么：</strong> 学习率优化工具</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torch.optim.lr_scheduler </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> LambdaLR</span></span></code></pre>
<p><strong>学习率调度器本质：</strong></p>
<ul>
<li>学习率调度器是控制模型训练过程中学习率动态变化的工具</li>
<li><code>LambdaLR</code> 允许使用自定义函数来调整学习率</li>
<li>属于 PyTorch 优化模块的一部分</li>
</ul>
<h3 id="为什么至关重要-训练稳定性的核心"><strong>为什么至关重要：</strong> 训练稳定性的核心</h3>
<p><strong>学习率的重要性：</strong></p>
<ol>
<li>
<p><strong>训练稳定性</strong>：</p>
<ul>
<li>太大：梯度爆炸，训练发散，loss震荡</li>
<li>太小：收敛极慢，容易陷入局部最优</li>
<li>需要找到”甜蜜点”</li>
</ul>
</li>
<li>
<p><strong>Transformer特殊需求：</strong></p>
<ul>
<li><strong>大模型敏感性</strong>：参数量巨大，对学习率极其敏感</li>
<li><strong>注意力机制</strong>：需要特殊的预热策略</li>
<li><strong>梯度特性</strong>：初期梯度不稳定，需要小心处理</li>
</ul>
</li>
</ol>
<p><strong>为什么选择 LambdaLR：</strong></p>
<ul>
<li><strong>高度自定义</strong>：可以实现任意复杂的调度策略</li>
<li><strong>数学精确</strong>：直接使用数学函数描述学习率变化</li>
<li><strong>研究重现</strong>：能精确复现论文中的学习率策略</li>
</ul>
<h3 id="怎么做-transformer的学习率策略"><strong>怎么做：</strong> Transformer的学习率策略</h3>
<p><strong>原论文的学习率公式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> rate</span><span style="color:#E1E4E8">(step, model_size, factor, warmup):</span></span>
<span class="line"><span style="color:#6A737D">    # lr = factor * (model_size^(-0.5) * min(step^(-0.5), step * warmup^(-1.5)))</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        step </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">        model_size </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> min</span><span style="color:#E1E4E8">(step </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">), step </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> warmup </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 实际使用</span></span>
<span class="line"><span style="color:#E1E4E8">scheduler </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LambdaLR(optimizer, </span><span style="color:#FFAB70">lr_lambda</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8"> step: rate(step, </span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4000</span><span style="color:#E1E4E8">))</span></span></code></pre>
<p><strong>策略分解：</strong></p>
<ol>
<li><strong>Warmup阶段</strong>（前4000步）：学习率线性增长
<ul>
<li>原因：让模型逐渐适应数据分布，避免初期的不稳定</li>
</ul>
</li>
<li><strong>衰减阶段</strong>：学习率按步数的平方根衰减
<ul>
<li>原因：后期需要更精细的调整，避免过冲</li>
</ul>
</li>
</ol>
<p><strong>为什么这样设计：</strong>
这种策略结合了<strong>稳定性</strong>（warmup）和<strong>收敛性</strong>（衰减），被证明对大型Transformer模型特别有效。</p>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> pandas </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> pd</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> altair </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> alt</span></span></code></pre>
<p><strong>数据分析和可视化工具：</strong></p>
<p><strong><code>pandas</code></strong>：</p>
<ul>
<li>强大的数据分析库</li>
<li>提供 DataFrame 数据结构，类似于 Excel 表格</li>
<li>可以方便地读取、处理、分析数据</li>
</ul>
<p><strong><code>altair</code></strong>：</p>
<ul>
<li>数据可视化库</li>
<li>可以创建漂亮的交互式图表</li>
<li>在这个项目中用于可视化注意力权重、训练进度等</li>
</ul>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torchtext.data.functional </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> to_map_style_dataset</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torch.utils.data </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DataLoader</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torchtext.vocab </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> build_vocab_from_iterator</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torchtext.datasets </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> datasets</span></span></code></pre>
<p><strong>文本处理和数据加载工具：</strong></p>
<p><strong><code>torchtext</code></strong>：</p>
<ul>
<li>PyTorch 的官方文本处理库</li>
<li>专门用于处理自然语言数据</li>
</ul>
<p><strong><code>to_map_style_dataset</code></strong>：</p>
<ul>
<li>将数据转换为 PyTorch 能够理解的数据集格式</li>
</ul>
<p><strong><code>DataLoader</code></strong>：</p>
<ul>
<li>非常重要的工具，用于：
<ul>
<li>将数据分成小批次（batches）</li>
<li>打乱数据顺序</li>
<li>并行加载数据，提高效率</li>
</ul>
</li>
</ul>
<p><strong><code>build_vocab_from_iterator</code></strong>：</p>
<ul>
<li>从文本数据中构建词汇表</li>
<li>词汇表是单词到数字的映射，因为计算机只能处理数字</li>
</ul>
<p><strong><code>datasets</code></strong>：</p>
<ul>
<li>包含了一些标准的数据集（如翻译数据集）</li>
</ul>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> spacy</span></span></code></pre>
<p><strong>自然语言处理工具：</strong></p>
<ul>
<li>spaCy 是另一个强大的 NLP 库</li>
<li>主要用于分词（将句子拆分成单词或子词）</li>
<li>支持多种语言</li>
<li>在这个项目中用于预处理文本数据</li>
</ul>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> GPUtil</span></span></code></pre>
<p><strong>GPU 监控工具：</strong></p>
<ul>
<li>用于监控 GPU 的使用情况</li>
<li>可以查看 GPU 内存使用量、温度等</li>
<li>在训练大型模型时，监控 GPU 状态很重要</li>
</ul>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> warnings</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torch.utils.data.distributed </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DistributedSampler</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch.distributed </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> dist</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch.multiprocessing </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> mp</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> torch.nn.parallel </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DistributedDataParallel </span><span style="color:#F97583">as</span><span style="color:#79B8FF"> DDP</span></span></code></pre>
<p><strong>高级功能模块：</strong></p>
<p><strong><code>warnings</code></strong>：</p>
<ul>
<li>控制 Python 警告信息的显示</li>
<li>可以忽略不重要的警告，让输出更清洁</li>
</ul>
<p><strong>分布式训练模块</strong>：</p>
<ul>
<li><code>DistributedSampler</code>：在多GPU或多机器训练时分配数据</li>
<li><code>torch.distributed</code>：分布式训练的核心模块</li>
<li><code>torch.multiprocessing</code>：多进程处理</li>
<li><code>DistributedDataParallel</code>：将模型分布到多个GPU上并行训练</li>
</ul>
<p><strong>为什么需要分布式训练：</strong>
Transformer 模型通常很大，单个 GPU 可能无法承载。分布式训练可以将模型和数据分布到多个 GPU 或机器上，大大加快训练速度。</p>
<hr>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Set to False to skip notebook execution (e.g. for debugging)</span></span>
<span class="line"><span style="color:#E1E4E8">warnings.filterwarnings(</span><span style="color:#9ECBFF">"ignore"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">RUN_EXAMPLES</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> True</span></span></code></pre>
<p><strong>配置设置：</strong></p>
<p><strong><code>warnings.filterwarnings("ignore")</code></strong>：</p>
<ul>
<li>告诉 Python 忽略所有警告信息</li>
<li>这让输出更清洁，但在调试时可能需要注意</li>
</ul>
<p><strong><code>RUN_EXAMPLES = True</code></strong>：</p>
<ul>
<li>这是一个自定义的全局变量（布尔值，True/False）</li>
<li>用于控制是否运行示例代码</li>
<li>当设为 False 时，可以跳过耗时的示例，只查看代码结构</li>
</ul>
<hr>
<h2 id="单元格-7辅助函数和虚拟类">单元格 7：辅助函数和虚拟类</h2>
<h3 id="是什么-代码管理和测试工具"><strong>是什么：</strong> 代码管理和测试工具</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> is_interactive_notebook</span><span style="color:#E1E4E8">():</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> __name__</span><span style="color:#F97583"> ==</span><span style="color:#9ECBFF"> "__main__"</span></span></code></pre>
<p><strong>函数定义详解：</strong></p>
<ul>
<li><strong>函数语法</strong>：<code>def</code> 关键字定义函数，<code>()</code> 内为参数列表，<code>:</code> 后为函数体</li>
<li><strong>返回机制</strong>：<code>return</code> 指定函数的输出值</li>
<li><strong>特殊变量</strong>：<code>__name__</code> 是Python内置变量，存储模块名称</li>
</ul>
<p><strong><code>__name__ == "__main__"</code> 的深层含义：</strong></p>
<ul>
<li>当Python文件<strong>直接运行</strong>时，<code>__name__</code> 被设为 <code>"__main__"</code></li>
<li>当文件被<strong>导入</strong>时，<code>__name__</code> 是文件名</li>
<li>在Jupyter Notebook中，每个单元格都被视为”直接运行”</li>
</ul>
<h3 id="为什么需要环境检测-代码适应性"><strong>为什么需要环境检测：</strong> 代码适应性</h3>
<p><strong>不同运行环境的挑战：</strong></p>
<ol>
<li><strong>Jupyter Notebook</strong>：交互式环境，单元格独立运行</li>
<li><strong>Python脚本</strong>：整体运行，可能被其他模块导入</li>
<li><strong>模块导入</strong>：作为库使用，不应执行示例代码</li>
</ol>
<p><strong>环境检测的价值：</strong></p>
<ul>
<li><strong>防止意外执行</strong>：避免导入时运行示例代码</li>
<li><strong>条件控制</strong>：只在合适的环境下运行测试代码</li>
<li><strong>代码复用</strong>：同一份代码在不同环境下表现不同</li>
</ul>
<h3 id="怎么做-智能的示例管理"><strong>怎么做：</strong> 智能的示例管理</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> show_example</span><span style="color:#E1E4E8">(fn, args</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[]):</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#79B8FF"> __name__</span><span style="color:#F97583"> ==</span><span style="color:#9ECBFF"> "__main__"</span><span style="color:#F97583"> and</span><span style="color:#79B8FF"> RUN_EXAMPLES</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> fn(</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">args)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> execute_example</span><span style="color:#E1E4E8">(fn, args</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[]):</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#79B8FF"> __name__</span><span style="color:#F97583"> ==</span><span style="color:#9ECBFF"> "__main__"</span><span style="color:#F97583"> and</span><span style="color:#79B8FF"> RUN_EXAMPLES</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        fn(</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">args)</span></span></code></pre>
<p><strong>高阶函数设计：</strong></p>
<ul>
<li><strong>函数作为参数</strong>：<code>fn</code> 是一个函数对象，体现了Python的函数是”一等公民”</li>
<li><strong>参数解包</strong>：<code>*args</code> 将列表元素展开为独立参数</li>
<li><strong>默认参数</strong>：<code>args=[]</code> 提供空列表作为默认值</li>
</ul>
<p><strong>双重条件检查：</strong></p>
<ol>
<li><strong>环境检查</strong>：<code>__name__ == "__main__"</code> 确保在正确环境</li>
<li><strong>开关控制</strong>：<code>RUN_EXAMPLES</code> 提供手动控制</li>
</ol>
<p><strong>两个函数的差异：</strong></p>
<ul>
<li><code>show_example</code>：<strong>有返回值</strong>，适用于需要显示结果的演示</li>
<li><code>execute_example</code>：<strong>无返回值</strong>，适用于执行操作的演示</li>
</ul>
<p><strong>使用示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 使用 show_example 显示结果</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> show_example(my_calculation_function, [arg1, arg2])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 使用 execute_example 执行操作</span></span>
<span class="line"><span style="color:#E1E4E8">execute_example(my_visualization_function, [data])</span></span></code></pre>
<hr>
<h3 id="是什么-测试用虚拟对象"><strong>是什么：</strong> 测试用虚拟对象</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> DummyOptimizer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">torch</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">optim</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Optimizer</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.param_groups </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [{</span><span style="color:#9ECBFF">"lr"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">}]</span></span>
<span class="line"><span style="color:#79B8FF">        None</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> step</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#79B8FF">        None</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> zero_grad</span><span style="color:#E1E4E8">(self, set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        None</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> DummyScheduler</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> step</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#79B8FF">        None</span></span></code></pre>
<p><strong>面向对象编程基础：</strong></p>
<ul>
<li><strong>类定义</strong>：<code>class</code> 关键字创建新的对象类型</li>
<li><strong>继承机制</strong>：<code>(torch.optim.Optimizer)</code> 表示继承父类功能</li>
<li><strong>方法重写</strong>：子类重新定义父类的方法</li>
<li><strong>构造函数</strong>：<code>__init__</code> 方法在对象创建时自动调用</li>
</ul>
<p><strong>虚拟对象模式（Stub Pattern）：</strong></p>
<ul>
<li><strong>接口保持</strong>：提供与真实对象相同的方法接口</li>
<li><strong>功能禁用</strong>：方法体为空或返回None，不执行实际操作</li>
<li><strong>占位作用</strong>：在测试或演示中代替真实对象</li>
</ul>
<h3 id="为什么需要虚拟对象-开发和测试的需要"><strong>为什么需要虚拟对象：</strong> 开发和测试的需要</h3>
<p><strong>软件开发中的常见需求：</strong></p>
<ol>
<li><strong>单元测试</strong>：测试代码逻辑而不执行实际训练</li>
<li><strong>快速原型</strong>：验证架构而不消耗计算资源</li>
<li><strong>代码演示</strong>：展示代码结构而不等待训练完成</li>
<li><strong>调试分析</strong>：隔离问题而不受训练过程干扰</li>
</ol>
<p><strong>优化器的复杂性：</strong></p>
<ul>
<li>真实优化器需要<strong>计算梯度</strong>、<strong>更新参数</strong>、<strong>管理状态</strong></li>
<li>虚拟优化器避免了这些<strong>重计算操作</strong></li>
<li>保持了代码的<strong>接口一致性</strong></li>
</ul>
<p><strong>为什么继承 torch.optim.Optimizer：</strong></p>
<ul>
<li><strong>类型检查</strong>：确保对象类型正确</li>
<li><strong>接口兼容</strong>：其他代码期望optimizer对象有特定方法</li>
<li><strong>属性继承</strong>：自动获得必需的属性结构</li>
</ul>
<h3 id="怎么做-实际应用场景"><strong>怎么做：</strong> 实际应用场景</h3>
<p><strong>必需属性的最小实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.param_groups </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [{</span><span style="color:#9ECBFF">"lr"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">}]  </span><span style="color:#6A737D"># PyTorch优化器必须有的属性</span></span></code></pre>
<ul>
<li><code>param_groups</code>：存储参数组和学习率信息</li>
<li>即使是虚拟对象，也必须提供这个属性来保持兼容性</li>
</ul>
<p><strong>方法的空实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> step</span><span style="color:#E1E4E8">(self):        </span><span style="color:#6A737D"># 通常用于参数更新</span></span>
<span class="line"><span style="color:#79B8FF">    None</span><span style="color:#6A737D">              # 什么都不做</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> zero_grad</span><span style="color:#E1E4E8">(self, set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># 通常用于梯度清零</span></span>
<span class="line"><span style="color:#79B8FF">    None</span><span style="color:#6A737D">              # 什么都不做</span></span></code></pre>
<p><strong>使用场景示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 在快速测试中使用</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#79B8FF"> TESTING_MODE</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DummyOptimizer()</span></span>
<span class="line"><span style="color:#E1E4E8">    scheduler </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DummyScheduler()</span></span>
<span class="line"><span style="color:#F97583">else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.optim.Adam(model.parameters())</span></span>
<span class="line"><span style="color:#E1E4E8">    scheduler </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.optim.lr_scheduler.StepLR(optimizer)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 代码的其余部分保持不变</span></span>
<span class="line"><span style="color:#E1E4E8">optimizer.step()        </span><span style="color:#6A737D"># 测试时不执行，训练时正常执行</span></span>
<span class="line"><span style="color:#E1E4E8">scheduler.step()        </span><span style="color:#6A737D"># 同样的逻辑</span></span></code></pre>
<p><strong>设计哲学：</strong>
这种设计体现了<strong>依赖注入</strong>和<strong>接口隔离</strong>的原则，让代码在不同场景下具有<strong>高度的灵活性</strong>和<strong>可测试性</strong>。</p>
<hr>
<h2 id="总结第一部分">总结第一部分</h2>
<p>到目前为止，我们已经看到了：</p>
<ol>
<li><strong>环境准备</strong>：安装依赖包的命令</li>
<li><strong>工具导入</strong>：导入了构建 Transformer 模型所需的所有工具</li>
<li><strong>辅助函数</strong>：定义了一些帮助管理和测试代码的工具</li>
</ol>
<p>这些准备工作为后面的核心内容奠定了基础。在下一部分，我们将开始看到 Transformer 模型的实际实现。</p>
<p>每个导入的库都有其特定用途：</p>
<ul>
<li><strong>torch 系列</strong>：构建和训练神经网络</li>
<li><strong>数据处理</strong>：处理文本数据、创建词汇表</li>
<li><strong>可视化</strong>：创建图表、监控训练进度</li>
<li><strong>系统工具</strong>：文件操作、时间计算、GPU 监控</li>
</ul>
<p>这种模块化的设计让复杂的深度学习项目变得可管理和可维护。</p>
<hr>
<h2 id="单元格-8-13背景介绍和目录">单元格 8-13：背景介绍和目录</h2>
<p>这些单元格都是 Markdown 格式，包含了关于 Transformer 模型的背景介绍。它们解释了为什么需要 Transformer，以及它相比之前的模型有什么优势。这些都是文字说明，不是代码，所以我们直接跳到核心的模型实现部分。</p>
<hr>
<h2 id="单元格-14encoderdecoder-类---transformer-的整体架构">单元格 14：EncoderDecoder 类 - Transformer 的整体架构</h2>
<h3 id="是什么-transformer的顶层控制器"><strong>是什么：</strong> Transformer的顶层控制器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> EncoderDecoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"><span style="color:#9ECBFF">    A standard Encoder-Decoder architecture. Base for this and many</span></span>
<span class="line"><span style="color:#9ECBFF">    other models.</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, encoder, decoder, src_embed, tgt_embed, generator):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(EncoderDecoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.encoder </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> encoder</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.decoder </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> decoder</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.src_embed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> src_embed</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.tgt_embed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt_embed</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.generator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, src, tgt, src_mask, tgt_mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Take in and process masked src and target sequences."</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decode(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.encode(src, src_mask), src_mask, tgt, tgt_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> encode</span><span style="color:#E1E4E8">(self, src, src_mask):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.encoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_embed(src), src_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> decode</span><span style="color:#E1E4E8">(self, memory, src_mask, tgt, tgt_mask):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt_embed(tgt), memory, src_mask, tgt_mask)</span></span></code></pre>
<p><strong>面向对象编程的核心概念：</strong></p>
<ul>
<li><strong>类定义</strong>：<code>class</code> 创建新的数据类型，定义对象的结构和行为</li>
<li><strong>继承机制</strong>：<code>(nn.Module)</code> 从PyTorch基类继承功能</li>
<li><strong>组合模式</strong>：将多个组件组合成一个完整的系统</li>
</ul>
<p><strong>继承 nn.Module 的深层原因：</strong></p>
<ol>
<li><strong>参数自动注册</strong>：所有神经网络参数被自动跟踪</li>
<li><strong>设备管理</strong>：一键在CPU/GPU间移动模型</li>
<li><strong>状态管理</strong>：train()/eval()模式自动切换</li>
<li><strong>序列化支持</strong>：模型保存/加载功能</li>
<li><strong>钩子系统</strong>：支持前向/后向传播的钩子函数</li>
</ol>
<h3 id="为什么这样设计-编码器-解码器的架构优势"><strong>为什么这样设计：</strong> 编码器-解码器的架构优势</h3>
<p><strong>序列到序列任务的挑战：</strong></p>
<ol>
<li><strong>变长输入输出</strong>：输入和输出序列长度可能不同</li>
<li><strong>语义理解</strong>：需要理解输入的完整含义</li>
<li><strong>生成控制</strong>：输出需要逐步生成，保持连贯性</li>
<li><strong>注意力机制</strong>：需要在生成时关注输入的不同部分</li>
</ol>
<p><strong>编码器-解码器分离的好处：</strong></p>
<ol>
<li><strong>职责明确</strong>：编码器专注理解，解码器专注生成</li>
<li><strong>可扩展性</strong>：两部分可以独立改进</li>
<li><strong>通用性</strong>：这种架构适用于多种seq2seq任务</li>
<li><strong>并行化</strong>：编码器可以并行处理整个输入序列</li>
</ol>
<p><strong>组件设计哲学：</strong></p>
<ul>
<li><code>encoder</code>：<strong>理解模块</strong> - 将输入序列编码为语义表示</li>
<li><code>decoder</code>：<strong>生成模块</strong> - 基于编码表示生成输出序列</li>
<li><code>src_embed</code>：<strong>输入映射</strong> - 将源语言词汇转为向量空间</li>
<li><code>tgt_embed</code>：<strong>输出映射</strong> - 将目标语言词汇转为向量空间</li>
<li><code>generator</code>：<strong>概率输出</strong> - 将隐藏状态转为词汇概率</li>
</ul>
<h3 id="怎么做-数据流和处理流程"><strong>怎么做：</strong> 数据流和处理流程</h3>
<p><strong>前向传播的完整流程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, src, tgt, src_mask, tgt_mask):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decode(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.encode(src, src_mask), src_mask, tgt, tgt_mask)</span></span></code></pre>
<p><strong>步骤分解：</strong></p>
<ol>
<li>
<p><strong>编码阶段</strong>：<code>self.encode(src, src_mask)</code></p>
<ul>
<li>源序列经过嵌入层转为向量</li>
<li>编码器处理整个序列，产生语义表示</li>
</ul>
</li>
<li>
<p><strong>解码阶段</strong>：<code>self.decode(memory, src_mask, tgt, tgt_mask)</code></p>
<ul>
<li>目标序列经过嵌入层</li>
<li>解码器基于编码结果生成输出</li>
</ul>
</li>
</ol>
<p><strong>掩码机制的重要性：</strong></p>
<ul>
<li><code>src_mask</code>：隐藏源序列的填充部分</li>
<li><code>tgt_mask</code>：隐藏目标序列的填充部分和未来信息（因果掩码）</li>
</ul>
<p><strong>方法设计的解耦原则：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> encode</span><span style="color:#E1E4E8">(self, src, src_mask):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.encoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_embed(src), src_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> decode</span><span style="color:#E1E4E8">(self, memory, src_mask, tgt, tgt_mask):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.decoder(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt_embed(tgt), memory, src_mask, tgt_mask)</span></span></code></pre>
<ul>
<li><strong>单一职责</strong>：每个方法只负责一个明确的功能</li>
<li><strong>可测试性</strong>：可以单独测试编码或解码过程</li>
<li><strong>可复用性</strong>：推理时只需要调用encode一次，decode多次</li>
</ul>
<p><strong>实际使用场景：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 训练时：teacher forcing，已知完整目标序列</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model(src_tokens, tgt_tokens, src_mask, tgt_mask)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 推理时：逐步生成</span></span>
<span class="line"><span style="color:#E1E4E8">memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.encode(src_tokens, src_mask)</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(max_length):</span></span>
<span class="line"><span style="color:#E1E4E8">    output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.decode(memory, src_mask, generated_so_far, causal_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">    next_token </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> select_next_token(output)</span></span>
<span class="line"><span style="color:#E1E4E8">    generated_so_far </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> append(generated_so_far, next_token)</span></span></code></pre>
<hr>
<h2 id="单元格-15generator-类---将隐藏状态转换为词汇概率">单元格 15：Generator 类 - 将隐藏状态转换为词汇概率</h2>
<h3 id="是什么-概率分布生成器"><strong>是什么：</strong> 概率分布生成器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Generator</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Define standard linear + softmax generation step."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, d_model, vocab):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Generator, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.proj </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_model, vocab)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> log_softmax(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.proj(x), </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>神经网络输出层的本质：</strong></p>
<ul>
<li><strong>线性投影</strong>：将高维隐藏状态映射到词汇空间</li>
<li><strong>概率归一化</strong>：将实数分数转换为概率分布</li>
<li><strong>对数空间</strong>：在对数域进行计算，提高数值稳定性</li>
</ul>
<h3 id="为什么需要generator-从向量到词汇的转换"><strong>为什么需要Generator：</strong> 从向量到词汇的转换</h3>
<p><strong>高维向量的语义问题：</strong></p>
<ol>
<li><strong>解码器输出</strong>：每个位置产生d_model维向量（如512维）</li>
<li><strong>词汇表映射</strong>：需要判断这个向量对应哪个词汇</li>
<li><strong>概率解释</strong>：需要为每个可能的词汇分配概率</li>
<li><strong>可微分性</strong>：整个过程必须支持梯度传播</li>
</ol>
<p><strong>线性投影的数学原理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.proj </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_model, vocab)</span></span>
<span class="line"><span style="color:#6A737D"># 数学表示：output = input × W + b</span></span>
<span class="line"><span style="color:#6A737D"># W: (d_model, vocab) 权重矩阵</span></span>
<span class="line"><span style="color:#6A737D"># b: (vocab,) 偏置向量</span></span></code></pre>
<p><strong>为什么使用线性层：</strong></p>
<ol>
<li><strong>计算效率</strong>：矩阵乘法可以高度并行化</li>
<li><strong>表达能力</strong>：线性变换足以学习向量到词汇的映射</li>
<li><strong>梯度友好</strong>：线性函数的梯度计算简单稳定</li>
<li><strong>内存效率</strong>：相比非线性层，参数量相对可控</li>
</ol>
<h3 id="怎么做-概率计算的技术细节"><strong>怎么做：</strong> 概率计算的技术细节</h3>
<p><strong>Softmax函数的数学含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>softmax(x_i) = exp(x_i) / Σ(exp(x_j))</span></span></code></pre>
<ul>
<li>将任意实数转换为概率（0-1之间，和为1）</li>
<li>较大的输入值得到较高的概率</li>
</ul>
<p><strong>Log-Softmax的优势：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> log_softmax(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.proj(x), </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>为什么使用对数概率：</strong></p>
<ol>
<li><strong>数值稳定性</strong>：避免exp()函数的上溢/下溢</li>
<li><strong>计算精度</strong>：在对数域计算更精确</li>
<li><strong>损失函数</strong>：交叉熵损失天然使用对数概率</li>
<li><strong>梯度计算</strong>：对数函数的梯度更稳定</li>
</ol>
<p><strong>维度处理的详细说明：</strong></p>
<ul>
<li><code>dim=-1</code>：在最后一个维度应用softmax</li>
<li>输入形状：<code>(batch_size, seq_length, d_model)</code></li>
<li>输出形状：<code>(batch_size, seq_length, vocab_size)</code></li>
<li>每个位置都有一个完整的词汇概率分布</li>
</ul>
<p><strong>实际计算流程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设 d_model=512, vocab_size=30000</span></span>
<span class="line"><span style="color:#E1E4E8">hidden_state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randn(</span><span style="color:#79B8FF">32</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">20</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># (batch, length, hidden)</span></span>
<span class="line"><span style="color:#E1E4E8">generator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Generator(</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">30000</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 1. 线性投影</span></span>
<span class="line"><span style="color:#E1E4E8">logits </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator.proj(hidden_state)     </span><span style="color:#6A737D"># (32, 20, 30000)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. 对数概率计算</span></span>
<span class="line"><span style="color:#E1E4E8">log_probs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> log_softmax(logits, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)   </span><span style="color:#6A737D"># (32, 20, 30000)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. 每个位置的概率分布</span></span>
<span class="line"><span style="color:#6A737D"># log_probs[0, 0, :] 是第一个样本第一个位置的词汇对数概率</span></span></code></pre>
<p><strong>与训练损失的连接：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Generator输出对数概率</span></span>
<span class="line"><span style="color:#E1E4E8">log_probs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator(decoder_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 交叉熵损失直接使用对数概率</span></span>
<span class="line"><span style="color:#E1E4E8">loss </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> F.nll_loss(log_probs.view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, vocab_size), target_tokens.view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">))</span></span></code></pre>
<p>这种设计确保了从<strong>语义向量</strong>到<strong>词汇选择</strong>的平滑过渡，是整个生成过程的关键最后一步。</p>
<hr>
<h2 id="单元格-16-18图片和说明">单元格 16-18：图片和说明</h2>
<p>这些单元格包含了 Transformer 架构的图片说明，帮助理解模型的整体结构。</p>
<hr>
<h2 id="单元格-19clones-函数---创建多个相同的层">单元格 19：clones 函数 - 创建多个相同的层</h2>
<h3 id="是什么-神经网络层的复制工厂"><strong>是什么：</strong> 神经网络层的复制工厂</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> clones</span><span style="color:#E1E4E8">(module, N):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Produce N identical layers."</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> nn.ModuleList([copy.deepcopy(module) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(N)])</span></span></code></pre>
<p><strong>函数的核心功能：</strong></p>
<ul>
<li><strong>输入</strong>：一个神经网络模块和要复制的次数</li>
<li><strong>输出</strong>：N个结构相同但参数独立的模块列表</li>
<li><strong>目的</strong>：批量创建具有相同架构的神经网络层</li>
</ul>
<h3 id="为什么需要这个函数-transformer的结构重复性"><strong>为什么需要这个函数：</strong> Transformer的结构重复性</h3>
<p><strong>Transformer的架构特点：</strong></p>
<ol>
<li><strong>编码器堆叠</strong>：通常包含6个相同的编码器层</li>
<li><strong>解码器堆叠</strong>：通常包含6个相同的解码器层</li>
<li><strong>多头注意力</strong>：每层包含多个相同结构的注意力头</li>
<li><strong>层的一致性</strong>：保持每层架构相同，但参数独立</li>
</ol>
<p><strong>为什么不能简单复制引用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 错误的做法</span></span>
<span class="line"><span style="color:#E1E4E8">layer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SomeLayer()</span></span>
<span class="line"><span style="color:#E1E4E8">layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [layer] </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> 6</span><span style="color:#6A737D">  # 所有元素指向同一个对象！</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 正确的做法</span></span>
<span class="line"><span style="color:#E1E4E8">layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, </span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 每个都是独立的对象</span></span></code></pre>
<p><strong>参数独立性的重要性：</strong></p>
<ul>
<li><strong>学习差异化</strong>：每层需要学习不同的特征表示</li>
<li><strong>梯度更新</strong>：参数必须能够独立更新</li>
<li><strong>功能分工</strong>：不同层可能专注于不同的语言现象</li>
</ul>
<h3 id="怎么做-深度复制与容器管理"><strong>怎么做：</strong> 深度复制与容器管理</h3>
<p><strong>深度复制的技术细节：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">copy.deepcopy(module)</span></span></code></pre>
<p><strong>深拷贝 vs 浅拷贝的区别：</strong></p>
<ol>
<li>
<p><strong>浅拷贝</strong>：只复制对象的第一层引用</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">shallow_copy </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> copy.copy(module)  </span><span style="color:#6A737D"># 参数仍然共享！</span></span></code></pre>
</li>
<li>
<p><strong>深拷贝</strong>：递归复制所有层级的对象</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">deep_copy </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> copy.deepcopy(module)  </span><span style="color:#6A737D"># 完全独立的副本</span></span></code></pre>
</li>
</ol>
<p><strong>列表推导式的解析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">[copy.deepcopy(module) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(N)]</span></span></code></pre>
<ul>
<li><code>range(N)</code>：生成0到N-1的序列</li>
<li><code>for _ in range(N)</code>：下划线表示不使用循环变量</li>
<li><code>copy.deepcopy(module)</code>：每次循环创建一个独立副本</li>
</ul>
<p><strong>nn.ModuleList的特殊作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> nn.ModuleList([</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">])</span></span></code></pre>
<p><strong>为什么不用普通的Python列表：</strong></p>
<ol>
<li><strong>参数注册</strong>：PyTorch自动识别和注册其中的所有参数</li>
<li><strong>设备管理</strong>：<code>.to(device)</code> 会自动移动所有子模块</li>
<li><strong>状态管理</strong>：<code>.train()</code> 和 <code>.eval()</code> 会递归应用到所有子模块</li>
<li><strong>序列化支持</strong>：模型保存时会包含所有子模块</li>
</ol>
<p><strong>实际使用示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 创建一个注意力层模板</span></span>
<span class="line"><span style="color:#E1E4E8">attention_template </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> MultiHeadedAttention(</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 创建6个独立的编码器层</span></span>
<span class="line"><span style="color:#E1E4E8">encoder_layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(EncoderLayer(</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, attention_template, ff_layer, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">), </span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 验证参数独立性</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">id</span><span style="color:#E1E4E8">(encoder_layers[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">].self_attn))  </span><span style="color:#6A737D"># 不同的内存地址</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">id</span><span style="color:#E1E4E8">(encoder_layers[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">].self_attn))  </span><span style="color:#6A737D"># 证明是独立对象</span></span></code></pre>
<p><strong>内存和计算考虑：</strong></p>
<ul>
<li><strong>内存开销</strong>：每个副本都占用独立的内存空间</li>
<li><strong>初始化一致性</strong>：所有副本从相同的初始状态开始</li>
<li><strong>训练动态</strong>：通过梯度下降，各层参数会逐渐分化</li>
</ul>
<hr>
<h2 id="单元格-20encoder-类---编码器的实现">单元格 20：Encoder 类 - 编码器的实现</h2>
<h3 id="是什么-多层编码器的堆叠容器"><strong>是什么：</strong> 多层编码器的堆叠容器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Encoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Core encoder is a stack of N layers"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, layer, N):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Encoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, N)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(layer.size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Pass the input (and mask) through each layer in turn."</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">            x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, mask)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p><strong>编码器的架构设计：</strong></p>
<ul>
<li><strong>层次结构</strong>：由N个相同结构的编码器层堆叠而成</li>
<li><strong>顺序处理</strong>：输入依次通过每一层进行处理</li>
<li><strong>最终规范化</strong>：在输出前进行层归一化</li>
</ul>
<h3 id="为什么采用堆叠设计-深度学习的表示能力"><strong>为什么采用堆叠设计：</strong> 深度学习的表示能力</h3>
<p><strong>深度网络的优势：</strong></p>
<ol>
<li>
<p><strong>层次特征学习</strong>：</p>
<ul>
<li>浅层：学习局部特征（如词汇、短语）</li>
<li>深层：学习全局特征（如语法、语义）</li>
</ul>
</li>
<li>
<p><strong>非线性表达能力</strong>：</p>
<ul>
<li>每层增加模型的非线性变换能力</li>
<li>更深的网络可以表示更复杂的函数</li>
</ul>
</li>
<li>
<p><strong>渐进式抽象</strong>：</p>
<ul>
<li>第1层：词汇级别的表示</li>
<li>第2-3层：短语级别的表示</li>
<li>第4-6层：句子级别的语义表示</li>
</ul>
</li>
</ol>
<p><strong>为什么是6层：</strong></p>
<ul>
<li><strong>经验最优</strong>：原论文实验发现6层在性能和计算成本间的平衡点</li>
<li><strong>梯度传播</strong>：足够深以学习复杂特征，又不至于梯度消失</li>
<li><strong>计算效率</strong>：训练时间和性能的权衡</li>
</ul>
<h3 id="怎么做-前向传播的具体实现"><strong>怎么做：</strong> 前向传播的具体实现</h3>
<p><strong>顺序处理的实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, mask)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p><strong>数据流分析：</strong></p>
<ol>
<li><strong>输入</strong>：<code>x</code> 是嵌入后的序列，<code>mask</code> 是注意力掩码</li>
<li><strong>逐层处理</strong>：每层接收前一层的输出作为输入</li>
<li><strong>掩码传递</strong>：所有层共享相同的掩码信息</li>
<li><strong>最终归一化</strong>：输出前进行层归一化处理</li>
</ol>
<p><strong>为什么在最后进行层归一化：</strong>
确保编码器的最终输出具有稳定的数值分布，为后续的解码器或其他组件提供良好的输入。
经过6层的残差连接后，数值可能会发生累积偏移，最后的归一化起到校正作用。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<hr>
<h2 id="单元格-21-22layernorm-类---层归一化">单元格 21-22：LayerNorm 类 - 层归一化</h2>
<h3 id="是什么-神经网络中的数据标准化技术"><strong>是什么：</strong> 神经网络中的数据标准化技术</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> LayerNorm</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Construct a layernorm module (See citation for details)."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, features, eps</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1e-6</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(LayerNorm, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.ones(features))</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.b_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.zeros(features))</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.eps </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> eps</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">        mean </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        std </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.std(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (x </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> mean) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (std </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.eps) </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.b_2</span></span></code></pre>
<p><strong>层归一化的数学本质：</strong></p>
<ul>
<li><strong>标准化操作</strong>：将数据转换为均值为0、标准差为1的分布</li>
<li><strong>仿射变换</strong>：通过可学习参数进行缩放和平移</li>
<li><strong>数值稳定性</strong>：防止数值计算中的溢出和下溢问题</li>
</ul>
<h3 id="为什么需要层归一化-深度网络训练的挑战"><strong>为什么需要层归一化：</strong> 深度网络训练的挑战</h3>
<p><strong>深度网络的训练问题：</strong></p>
<ol>
<li><strong>内部协变量偏移</strong>：随着网络层数增加，每层的输入分布不断变化</li>
<li><strong>梯度爆炸/消失</strong>：深层网络中梯度可能变得极大或极小</li>
<li><strong>学习率敏感性</strong>：不同的初始化和学习率对性能影响巨大</li>
<li><strong>收敛缓慢</strong>：训练过程可能非常缓慢或不稳定</li>
</ol>
<p><strong>层归一化的解决方案：</strong></p>
<ol>
<li><strong>稳定分布</strong>：每层的输入保持相似的数值范围</li>
<li><strong>加速收敛</strong>：标准化的数据更容易优化</li>
<li><strong>减少依赖</strong>：对权重初始化和学习率不那么敏感</li>
<li><strong>提升性能</strong>：通常能获得更好的最终效果</li>
</ol>
<p><strong>层归一化 vs 批归一化的区别：</strong></p>
<ul>
<li><strong>批归一化</strong>：在批次维度上计算统计量，适用于CNN</li>
<li><strong>层归一化</strong>：在特征维度上计算统计量，适用于RNN和Transformer</li>
<li><strong>独立性</strong>：层归一化不依赖批次大小，更适合变长序列</li>
</ul>
<h3 id="怎么做-数学公式和实现细节"><strong>怎么做：</strong> 数学公式和实现细节</h3>
<p><strong>数学公式详解：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>μ = mean(x)                    # 计算均值</span></span>
<span class="line"><span>σ = std(x)                     # 计算标准差</span></span>
<span class="line"><span>x_norm = (x - μ) / (σ + ε)     # 标准化</span></span>
<span class="line"><span>y = γ * x_norm + β             # 仿射变换</span></span></code></pre>
<p><strong>参数的含义和作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.ones(features))   </span><span style="color:#6A737D"># γ (gamma) 缩放参数</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.b_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.zeros(features))  </span><span style="color:#6A737D"># β (beta) 偏移参数</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.eps </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> eps                                   </span><span style="color:#6A737D"># ε (epsilon) 数值稳定项</span></span></code></pre>
<p><strong>nn.Parameter的特殊性：</strong></p>
<ul>
<li><strong>自动注册</strong>：PyTorch自动将其识别为模型参数</li>
<li><strong>梯度计算</strong>：参与反向传播，可以被优化器更新</li>
<li><strong>设备管理</strong>：跟随模型在GPU/CPU间移动</li>
<li><strong>序列化</strong>：模型保存时包含这些参数的值</li>
</ul>
<p><strong>前向传播的具体实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">    mean </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)     </span><span style="color:#6A737D"># 沿最后一维计算均值</span></span>
<span class="line"><span style="color:#E1E4E8">    std </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.std(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)       </span><span style="color:#6A737D"># 沿最后一维计算标准差</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (x </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> mean) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (std </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.eps) </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.b_2</span></span></code></pre>
<p><strong>维度处理的技术细节：</strong></p>
<ul>
<li><code>dim=-1</code>：在最后一个维度（特征维度）上计算统计量</li>
<li><code>keepdim=True</code>：保持维度，便于广播运算</li>
<li>输入shape：<code>[batch_size, seq_length, d_model]</code></li>
<li>统计量shape：<code>[batch_size, seq_length, 1]</code></li>
</ul>
<p><strong>为什么在特征维度归一化：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设输入形状为 [2, 3, 4] (batch=2, seq=3, features=4)</span></span>
<span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randn(</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">mean </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># shape: [2, 3, 1]</span></span>
<span class="line"><span style="color:#6A737D"># 每个位置的4个特征被独立归一化</span></span></code></pre>
<ol>
<li><strong>语义一致性</strong>：每个位置的特征向量代表语义信息</li>
<li><strong>尺度统一</strong>：不同特征维度的数值范围得到统一</li>
<li><strong>位置独立</strong>：每个序列位置的归一化是独立的</li>
</ol>
<p><strong>初始化策略的原理：</strong></p>
<ul>
<li><code>torch.ones(features)</code>：γ初始化为1，保持标准化后的方差</li>
<li><code>torch.zeros(features)</code>：β初始化为0，保持标准化后的均值</li>
<li>这样初始状态下，层归一化相当于标准的z-score标准化</li>
</ul>
<p><strong>数值稳定性考虑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">std </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.eps  </span><span style="color:#6A737D"># 防止除零错误</span></span></code></pre>
<ul>
<li>当标准差接近0时，加上小常数防止数值不稳定</li>
<li><code>eps=1e-6</code>是经验值，平衡精度和稳定性</li>
</ul>
<p><strong>实际效果演示：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 未归一化的数据可能范围很大</span></span>
<span class="line"><span style="color:#E1E4E8">before </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.tensor([[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">100.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">50.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">200.0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.3</span><span style="color:#E1E4E8">]])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">layer_norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(</span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">after </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer_norm(before)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 归一化后每行的均值接近0，标准差接近1</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(after.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">))  </span><span style="color:#6A737D"># 接近 [0, 0]</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(after.std(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">))   </span><span style="color:#6A737D"># 接近 [1, 1]</span></span></code></pre>
<p>这种标准化确保了神经网络中每一层都能接收到<strong>数值稳定、分布一致</strong>的输入，是Transformer稳定训练的重要基础。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Encoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Core encoder is a stack of N layers"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, layer, N):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Encoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, N)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(layer.size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Pass the input (and mask) through each layer in turn."</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">            x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, mask)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p><strong>编码器的作用：</strong>
编码器的任务是理解输入序列（比如一个德语句子），并将其转换为一系列向量表示，这些向量包含了句子的语义信息。</p>
<p><strong>构造函数分析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, layer, N):</span></span>
<span class="line"><span style="color:#79B8FF">    super</span><span style="color:#E1E4E8">(Encoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, N)</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(layer.size)</span></span></code></pre>
<ul>
<li><code>layer</code>：单个编码器层的模板</li>
<li><code>N</code>：编码器层的数量（论文中是 6）</li>
<li><code>self.layers = clones(layer, N)</code>：创建 N 个相同的编码器层</li>
<li><code>self.norm = LayerNorm(layer.size)</code>：最后的层归一化</li>
</ul>
<p><strong>forward 方法分析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Pass the input (and mask) through each layer in turn."</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, mask)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p><strong>数据流动过程：</strong></p>
<ol>
<li>输入 <code>x</code> 是词汇的向量表示</li>
<li><code>mask</code> 告诉模型哪些位置是真实的词汇，哪些是填充</li>
<li>依次通过每个编码器层，每一层都会更新 <code>x</code></li>
<li>最后应用层归一化，得到最终的编码表示</li>
</ol>
<p><strong>为什么要逐层处理：</strong></p>
<ul>
<li>每一层都会从不同角度分析输入</li>
<li>浅层关注局部特征（如词汇和短语）</li>
<li>深层关注全局特征（如句子的整体含义）</li>
<li>堆叠多层可以获得更丰富的表示</li>
</ul>
<p><strong>mask 的重要性：</strong>
在批处理时，不同的句子长度不同，短句子会用特殊符号填充。mask 告诉模型忽略这些填充位置，只关注真实的内容。</p>
<hr>
<h2 id="单元格-21-22layernorm-类---层归一化-1">单元格 21-22：LayerNorm 类 - 层归一化</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> LayerNorm</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Construct a layernorm module (See citation for details)."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, features, eps</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1e-6</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(LayerNorm, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.ones(features))</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.b_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Parameter(torch.zeros(features))</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.eps </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> eps</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">        mean </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        std </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.std(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (x </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> mean) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (std </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.eps) </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.b_2</span></span></code></pre>
<p><strong>层归一化的作用：</strong>
层归一化是深度学习中的重要技术，它可以：</p>
<ol>
<li>稳定训练过程</li>
<li>加快收敛速度</li>
<li>减少对初始化的依赖</li>
<li>提高模型性能</li>
</ol>
<p><strong>参数解释：</strong></p>
<ul>
<li><code>features</code>：输入特征的维度</li>
<li><code>eps</code>：很小的数值（1e-6），防止除零错误</li>
<li><code>self.a_2</code>：可学习的缩放参数，初始化为 1</li>
<li><code>self.b_2</code>：可学习的偏移参数，初始化为 0</li>
</ul>
<p><strong>nn.Parameter 的含义：</strong>
<code>nn.Parameter</code> 将普通张量转换为模型参数，这意味着：</p>
<ul>
<li>优化器会自动更新这些参数</li>
<li>模型保存时会包含这些参数</li>
<li>可以通过 <code>model.parameters()</code> 访问</li>
</ul>
<p><strong>归一化过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">mean </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.mean(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 计算均值</span></span>
<span class="line"><span style="color:#E1E4E8">std </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.std(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">keepdim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)    </span><span style="color:#6A737D"># 计算标准差</span></span>
<span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.a_2 </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (x </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> mean) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (std </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.eps) </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.b_2</span></span></code></pre>
<ol>
<li>计算输入的均值和标准差</li>
<li>将输入标准化：<code>(x - mean) / (std + eps)</code></li>
<li>应用可学习的缩放和偏移：<code>a_2 * 标准化值 + b_2</code></li>
</ol>
<p><strong>为什么在最后一个维度归一化：</strong></p>
<ul>
<li><code>-1</code> 表示最后一个维度</li>
<li>对于形状为 <code>[batch_size, sequence_length, features]</code> 的张量</li>
<li>我们在 <code>features</code> 维度上进行归一化</li>
<li>这意味着每个位置的特征向量都被独立归一化</li>
</ul>
<hr>
<h2 id="单元格-23-24sublayerconnection-类---残差连接和层归一化">单元格 23-24：SublayerConnection 类 - 残差连接和层归一化</h2>
<h3 id="是什么-深度网络的连接模式"><strong>是什么：</strong> 深度网络的连接模式</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> SublayerConnection</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"><span style="color:#9ECBFF">    A residual connection followed by a layer norm.</span></span>
<span class="line"><span style="color:#9ECBFF">    Note for code simplicity the norm is first as opposed to last.</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, dropout):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(SublayerConnection, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(size)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, sublayer):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Apply residual connection to any sublayer with the same size."</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(sublayer(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.norm(x)))</span></span></code></pre>
<p><strong>残差连接的核心概念：</strong></p>
<ul>
<li><strong>跳跃连接</strong>：直接将输入加到输出上，创建”高速公路”</li>
<li><strong>恒等映射</strong>：当子层学习为零函数时，输出等于输入</li>
<li><strong>梯度流动</strong>：为梯度提供直接的反向传播路径</li>
</ul>
<h3 id="为什么需要残差连接-解决深度网络的根本问题"><strong>为什么需要残差连接：</strong> 解决深度网络的根本问题</h3>
<p><strong>深度网络的退化问题：</strong></p>
<ol>
<li><strong>梯度消失</strong>：在深层网络中，梯度在反向传播时逐层衰减</li>
<li><strong>训练困难</strong>：单纯堆叠层数并不能保证性能提升</li>
<li><strong>表示学习</strong>：网络难以学习恒等映射，即”什么都不做”</li>
</ol>
<p><strong>残差连接的革命性解决方案：</strong></p>
<ol>
<li><strong>梯度高速公路</strong>：梯度可以直接从输出传播到输入</li>
<li><strong>学习简化</strong>：网络只需学习”残差”（调整量）而非完整映射</li>
<li><strong>深度可扩展</strong>：理论上可以训练非常深的网络</li>
</ol>
<p><strong>数学表达的深层含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>传统网络：H(x) = F(x)</span></span>
<span class="line"><span>残差网络：H(x) = F(x) + x</span></span>
<span class="line"><span>等价于：  F(x) = H(x) - x  (学习残差)</span></span></code></pre>
<p><strong>为什么学习残差更容易：</strong></p>
<ul>
<li>如果最优映射就是恒等映射，F(x)只需学习为0</li>
<li>如果需要微调，F(x)只需学习小的调整量</li>
<li>比从头学习完整映射H(x)要简单得多</li>
</ul>
<h3 id="怎么做-技术实现和设计选择"><strong>怎么做：</strong> 技术实现和设计选择</h3>
<p><strong>前向传播的执行顺序：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(sublayer(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.norm(x)))</span></span></code></pre>
<p><strong>步骤分解：</strong></p>
<ol>
<li><code>self.norm(x)</code>：输入先经过层归一化</li>
<li><code>sublayer(...)</code>：标准化后的输入通过子层（如注意力层）</li>
<li><code>self.dropout(...)</code>：对子层输出应用dropout正则化</li>
<li><code>x + ...</code>：原始输入与处理后输出相加</li>
</ol>
<p><strong>层归一化前置的设计选择：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Pre-LN (本实现)：先归一化再处理</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(sublayer(norm(x)))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Post-LN (原论文)：先处理再归一化</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> norm(x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(sublayer(x)))</span></span></code></pre>
<p><strong>Pre-LN的优势：</strong></p>
<ol>
<li><strong>训练稳定性</strong>：归一化的输入让子层更容易训练</li>
<li><strong>梯度流动</strong>：更好的梯度传播特性</li>
<li><strong>收敛速度</strong>：通常收敛更快，需要更少的warmup</li>
</ol>
<p><strong>Dropout正则化的作用机制：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(dropout)</span></span></code></pre>
<p><strong>Dropout的工作原理：</strong></p>
<ol>
<li><strong>训练时</strong>：随机将部分神经元输出设为0</li>
<li><strong>推理时</strong>：使用所有神经元，但按比例缩放</li>
<li><strong>正则化</strong>：防止过拟合，提高泛化能力</li>
</ol>
<p><strong>为什么对残差应用Dropout：</strong></p>
<ul>
<li><strong>噪声注入</strong>：为学习过程增加随机性</li>
<li><strong>鲁棒性</strong>：让模型不过度依赖特定的连接</li>
<li><strong>泛化能力</strong>：提高在新数据上的表现</li>
</ul>
<p><strong>子层接口的通用性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, sublayer):</span></span></code></pre>
<p><strong>设计哲学：</strong></p>
<ul>
<li><code>sublayer</code>可以是任何接受相同输入输出维度的层</li>
<li>注意力层、前馈网络等都可以作为子层</li>
<li>实现了高度的模块化和可复用性</li>
</ul>
<p><strong>实际使用示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 在编码器层中使用</span></span>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> EncoderLayer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, self_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(EncoderLayer, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.self_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attn</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.feed_forward </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.sublayer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(SublayerConnection(size, dropout), </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#6A737D">        # 第一个残差连接：自注意力</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span>
<span class="line"><span style="color:#6A737D">        # 第二个残差连接：前馈网络</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>lambda函数的巧妙使用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask)</span></span></code></pre>
<ul>
<li>将多参数函数包装成单参数函数</li>
<li>满足SublayerConnection的接口要求</li>
<li>闭包捕获了mask变量</li>
</ul>
<p><strong>残差连接的数值效果：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设输入和子层输出</span></span>
<span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randn(</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">)           </span><span style="color:#6A737D"># 原始输入</span></span>
<span class="line"><span style="color:#E1E4E8">sublayer_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randn(</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 子层输出</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 没有残差连接：只有子层输出</span></span>
<span class="line"><span style="color:#E1E4E8">without_residual </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sublayer_output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 有残差连接：原始输入+子层输出</span></span>
<span class="line"><span style="color:#E1E4E8">with_residual </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> sublayer_output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 梯度流动：</span></span>
<span class="line"><span style="color:#6A737D"># without_residual对x的梯度 = 0</span></span>
<span class="line"><span style="color:#6A737D"># with_residual对x的梯度 = 1 + 子层梯度 (至少保证梯度为1)</span></span></code></pre>
<p>这种设计确保了即使在非常深的网络中，梯度也能有效地传播到早期层，是Transformer能够训练深层网络的关键技术。</p>
<hr>
<h2 id="单元格-25-26encoderlayer-类---编码器的单个层">单元格 25-26：EncoderLayer 类 - 编码器的单个层</h2>
<h3 id="是什么-编码器的基本构建块"><strong>是什么：</strong> 编码器的基本构建块</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> EncoderLayer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Encoder is made up of self-attn and feed forward (defined below)"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, self_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(EncoderLayer, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.self_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attn</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.feed_forward </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.sublayer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(SublayerConnection(size, dropout), </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Follow Figure 1 (left) for connections."</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>编码器层的双重结构：</strong></p>
<ul>
<li><strong>自注意力机制</strong>：让序列中的每个位置关注其他位置</li>
<li><strong>前馈网络</strong>：对每个位置独立进行非线性变换</li>
<li><strong>残差连接</strong>：确保信息流动和梯度传播</li>
</ul>
<h3 id="为什么这样设计-transformer的核心创新"><strong>为什么这样设计：</strong> Transformer的核心创新</h3>
<p><strong>自注意力的革命性意义：</strong></p>
<ol>
<li><strong>并行计算</strong>：不像RNN需要顺序处理，可以并行处理所有位置</li>
<li><strong>长距离依赖</strong>：直接建立任意两个位置之间的连接</li>
<li><strong>动态权重</strong>：注意力权重根据输入内容动态调整</li>
<li><strong>位置无关</strong>：不受固定窗口大小限制</li>
</ol>
<p><strong>前馈网络的补充作用：</strong></p>
<ol>
<li><strong>非线性变换</strong>：注意力机制本身是线性的，需要非线性增强表达能力</li>
<li><strong>位置独立处理</strong>：对每个位置进行相同的变换</li>
<li><strong>特征混合</strong>：在更高维空间中混合特征</li>
<li><strong>模式识别</strong>：学习复杂的语言模式</li>
</ol>
<p><strong>两阶段处理的协同效应：</strong></p>
<ul>
<li><strong>注意力阶段</strong>：关注什么（Where to look）</li>
<li><strong>前馈阶段</strong>：处理什么（What to do）</li>
</ul>
<h3 id="怎么做-具体实现和技术细节"><strong>怎么做：</strong> 具体实现和技术细节</h3>
<p><strong>初始化中的组件管理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, self_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.self_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attn</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.feed_forward </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.sublayer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(SublayerConnection(size, dropout), </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size</span></span></code></pre>
<p><strong>组件的职责分工：</strong></p>
<ul>
<li><code>self_attn</code>：自注意力层，处理序列间的关系</li>
<li><code>feed_forward</code>：前馈网络，进行位置级的特征变换</li>
<li><code>sublayer</code>：两个残差连接层，包装上述两个组件</li>
<li><code>size</code>：特征维度，确保所有组件的维度一致性</li>
</ul>
<p><strong>前向传播的两阶段处理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#E1E4E8">    x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>第一阶段：自注意力处理</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span></code></pre>
<p><strong>自注意力的三个输入：</strong></p>
<ul>
<li>第一个 <code>x</code>：Query（查询），“我想要什么信息”</li>
<li>第二个 <code>x</code>：Key（键），“我有什么信息”</li>
<li>第三个 <code>x</code>：Value（值），“具体的信息内容”</li>
<li><code>mask</code>：掩码，控制注意力的范围</li>
</ul>
<p><strong>为什么自注意力需要Q、K、V都是同一个输入：</strong></p>
<ul>
<li><strong>自注意力</strong>：序列与自身的注意力</li>
<li><strong>位置关系</strong>：每个位置都可以作为查询者和被查询者</li>
<li><strong>信息整合</strong>：允许每个位置收集来自所有位置的信息</li>
</ul>
<p><strong>第二阶段：前馈网络处理</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>前馈网络的作用：</strong></p>
<ul>
<li>接收注意力处理后的表示</li>
<li>进行独立的非线性变换</li>
<li>输出增强的特征表示</li>
</ul>
<p><strong>lambda函数的封装技巧：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask)</span></span></code></pre>
<p><strong>为什么需要lambda封装：</strong></p>
<ol>
<li><strong>接口适配</strong>：SublayerConnection期望单参数函数</li>
<li><strong>参数绑定</strong>：将4参数的注意力函数转为1参数函数</li>
<li><strong>闭包捕获</strong>：自动捕获mask变量</li>
<li><strong>代码简洁</strong>：避免定义额外的辅助函数</li>
</ol>
<p><strong>数据流动的完整路径：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 输入：[batch_size, seq_length, d_model]</span></span>
<span class="line"><span style="color:#E1E4E8">input_x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 第一个残差连接：self-attention</span></span>
<span class="line"><span style="color:#6A737D"># 1. 层归一化</span></span>
<span class="line"><span style="color:#E1E4E8">norm_x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer_norm(input_x)</span></span>
<span class="line"><span style="color:#6A737D"># 2. 自注意力</span></span>
<span class="line"><span style="color:#E1E4E8">attn_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attention(norm_x, norm_x, norm_x, mask)</span></span>
<span class="line"><span style="color:#6A737D"># 3. dropout + 残差连接</span></span>
<span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> input_x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(attn_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 第二个残差连接：feed-forward</span></span>
<span class="line"><span style="color:#6A737D"># 1. 层归一化</span></span>
<span class="line"><span style="color:#E1E4E8">norm_x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer_norm(x)</span></span>
<span class="line"><span style="color:#6A737D"># 2. 前馈网络</span></span>
<span class="line"><span style="color:#E1E4E8">ff_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward(norm_x)</span></span>
<span class="line"><span style="color:#6A737D"># 3. dropout + 残差连接</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(ff_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 输出：[batch_size, seq_length, d_model]</span></span></code></pre>
<p><strong>性能考虑：</strong></p>
<ul>
<li><strong>内存效率</strong>：残差连接避免了额外的内存分配</li>
<li><strong>计算并行</strong>：自注意力可以高度并行化</li>
<li><strong>梯度稳定</strong>：残差连接保证梯度流动</li>
</ul>
<p>这种双阶段设计使得编码器层既能捕获<strong>序列内的关系</strong>（通过自注意力），又能进行<strong>深度的特征变换</strong>（通过前馈网络），是Transformer强大表示能力的核心。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Follow Figure 1 (left) for connections."</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>编码器层的组成：</strong>
每个编码器层包含两个主要组件：</p>
<ol>
<li><strong>自注意力机制</strong> (self-attention)</li>
<li><strong>前馈神经网络</strong> (feed-forward network)</li>
</ol>
<p><strong>构造函数分析：</strong></p>
<ul>
<li><code>size</code>：层的维度大小</li>
<li><code>self_attn</code>：自注意力模块</li>
<li><code>feed_forward</code>：前馈网络模块</li>
<li><code>dropout</code>：dropout 概率</li>
<li><code>self.sublayer = clones(SublayerConnection(size, dropout), 2)</code>：创建两个残差连接</li>
</ul>
<p><strong>forward 方法详解：</strong></p>
<p><strong>第一个子层（自注意力）：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask))</span></span></code></pre>
<ul>
<li><code>lambda x: self.self_attn(x, x, x, mask)</code> 是一个匿名函数</li>
<li>将相同的输入 <code>x</code> 作为 query、key、value 传给自注意力</li>
<li>这就是”自”注意力的含义：序列关注自己</li>
</ul>
<p><strong>第二个子层（前馈网络）：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<ul>
<li>将自注意力的输出传给前馈网络</li>
<li>前馈网络对每个位置独立地进行非线性变换</li>
</ul>
<p><strong>lambda 函数解释：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask)</span></span></code></pre>
<ul>
<li><code>lambda</code> 是 Python 中创建匿名函数的关键字</li>
<li>等价于：</li>
</ul>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> temp_function</span><span style="color:#E1E4E8">(x):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.self_attn(x, x, x, mask)</span></span></code></pre>
<ul>
<li>用 lambda 更简洁，适合简单的一行函数</li>
</ul>
<p><strong>数据流动过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>输入 x </span></span>
<span class="line"><span>→ 层归一化 </span></span>
<span class="line"><span>→ 自注意力 </span></span>
<span class="line"><span>→ dropout </span></span>
<span class="line"><span>→ 残差连接 </span></span>
<span class="line"><span>→ 层归一化 </span></span>
<span class="line"><span>→ 前馈网络 </span></span>
<span class="line"><span>→ dropout </span></span>
<span class="line"><span>→ 残差连接 </span></span>
<span class="line"><span>→ 输出</span></span></code></pre>
<hr>
<h2 id="单元格-27-29decoder-相关类---解码器实现">单元格 27-29：Decoder 相关类 - 解码器实现</h2>
<h3 id="是什么-序列生成的核心引擎"><strong>是什么：</strong> 序列生成的核心引擎</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Decoder</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Generic N layer decoder with masking."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, layer, N):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Decoder, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.layers </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(layer, N)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.norm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LayerNorm(layer.size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">            x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, memory, src_mask, tgt_mask)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.norm(x)</span></span></code></pre>
<p><strong>解码器的根本使命：</strong></p>
<ul>
<li><strong>序列生成</strong>：逐步生成目标序列的每个位置</li>
<li><strong>条件生成</strong>：基于源序列（编码器输出）生成目标序列</li>
<li><strong>自回归特性</strong>：每个位置的生成依赖于之前已生成的位置</li>
</ul>
<h3 id="为什么解码器更复杂-生成任务的挑战"><strong>为什么解码器更复杂：</strong> 生成任务的挑战</h3>
<p><strong>编码器 vs 解码器的根本差异：</strong></p>
<ol>
<li><strong>编码器</strong>：理解任务，可以看到完整输入，并行处理</li>
<li><strong>解码器</strong>：生成任务，只能看到已生成部分，顺序依赖</li>
</ol>
<p><strong>解码器面临的三重挑战：</strong></p>
<ol>
<li><strong>自回归约束</strong>：不能”偷看”未来的词汇</li>
<li><strong>条件依赖</strong>：必须基于源序列信息生成</li>
<li><strong>长序列生成</strong>：保持生成过程的一致性和连贯性</li>
</ol>
<p><strong>为什么需要memory参数：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, memory, src_mask, tgt_mask):</span></span></code></pre>
<ul>
<li><code>memory</code>：编码器的输出，包含源序列的完整语义信息</li>
<li>解码器必须”记住”源序列的内容才能正确翻译</li>
</ul>
<h3 id="怎么做-解码器的具体实现机制"><strong>怎么做：</strong> 解码器的具体实现机制</h3>
<p><strong>多层堆叠的生成策略：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.layers:</span></span>
<span class="line"><span style="color:#E1E4E8">    x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> layer(x, memory, src_mask, tgt_mask)</span></span></code></pre>
<p><strong>逐层抽象的生成过程：</strong></p>
<ul>
<li><strong>浅层</strong>：局部语法结构生成</li>
<li><strong>中层</strong>：短语级别的语义转换</li>
<li><strong>深层</strong>：句子级别的语义一致性</li>
</ul>
<p><strong>四种掩码的协同作用：</strong></p>
<ul>
<li><code>src_mask</code>：隐藏源序列的填充部分</li>
<li><code>tgt_mask</code>：防止解码器看到未来位置</li>
<li>确保生成过程的正确性和有效性</li>
</ul>
<hr>
<h3 id="是什么-解码器的基本构建单元"><strong>是什么：</strong> 解码器的基本构建单元</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> DecoderLayer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, self_attn, src_attn, feed_forward, dropout):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(DecoderLayer, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.self_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attn</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.src_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> src_attn</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.feed_forward </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.sublayer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(SublayerConnection(size, dropout), </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, memory, src_mask, tgt_mask):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Follow Figure 1 (right) for connections."</span></span>
<span class="line"><span style="color:#E1E4E8">        m </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> memory</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, tgt_mask))</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_attn(x, m, m, src_mask))</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<p><strong>解码器层的三阶段处理架构：</strong></p>
<ul>
<li><strong>阶段1</strong>：自注意力 - 整合已生成的目标序列信息</li>
<li><strong>阶段2</strong>：交叉注意力 - 结合源序列的语义信息</li>
<li><strong>阶段3</strong>：前馈网络 - 进行最终的特征变换</li>
</ul>
<h3 id="为什么需要三个组件-生成任务的复杂性"><strong>为什么需要三个组件：</strong> 生成任务的复杂性</h3>
<p><strong>自注意力的必要性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, tgt_mask))</span></span></code></pre>
<p><strong>自注意力解决的问题：</strong></p>
<ol>
<li><strong>内部一致性</strong>：确保已生成部分的语法和语义一致</li>
<li><strong>长距离依赖</strong>：让当前位置关注远程的已生成内容</li>
<li><strong>上下文整合</strong>：整合目标序列的局部上下文</li>
</ol>
<p><strong>交叉注意力的关键作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x, </span><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_attn(x, m, m, src_mask))</span></span></code></pre>
<p><strong>Query-Key-Value的分工：</strong></p>
<ul>
<li><strong>Query (x)</strong>：当前目标位置的”问题” - “我需要什么信息？”</li>
<li><strong>Key (m)</strong>：源序列的”索引” - “我有什么信息？”</li>
<li><strong>Value (m)</strong>：源序列的”内容” - “具体信息是什么？”</li>
</ul>
<p><strong>交叉注意力的核心作用：</strong></p>
<ol>
<li><strong>信息检索</strong>：从源序列中检索相关信息</li>
<li><strong>对齐学习</strong>：学习源语言和目标语言的对应关系</li>
<li><strong>条件生成</strong>：基于源序列内容生成合适的目标词汇</li>
</ol>
<p><strong>前馈网络的补强作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.sublayer[</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">](x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.feed_forward)</span></span></code></pre>
<ul>
<li>在整合了目标和源序列信息后，进行最终的非线性变换</li>
<li>将复杂的语义表示转换为可用于下一层的格式</li>
</ul>
<h3 id="怎么做-三阶段协同的技术实现"><strong>怎么做：</strong> 三阶段协同的技术实现</h3>
<p><strong>注意力机制参数模式的深层含义：</strong></p>
<p><strong>自注意力模式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.self_attn(x, x, x, tgt_mask)</span></span>
<span class="line"><span style="color:#6A737D"># Q=目标序列, K=目标序列, V=目标序列</span></span>
<span class="line"><span style="color:#6A737D"># 目标序列关注自身，但不能看到未来</span></span></code></pre>
<p><strong>交叉注意力模式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_attn(x, m, m, src_mask)</span></span>
<span class="line"><span style="color:#6A737D"># Q=目标序列, K=源序列, V=源序列</span></span>
<span class="line"><span style="color:#6A737D"># 目标序列查询源序列信息</span></span></code></pre>
<p><strong>残差连接的三重保护：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.sublayer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(SublayerConnection(size, dropout), </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li>每个阶段都有独立的残差连接</li>
<li>确保梯度能够顺利传播到所有层</li>
<li>防止信息在多阶段处理中丢失</li>
</ul>
<p><strong>lambda函数的参数绑定技巧：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">lambda</span><span style="color:#E1E4E8"> x: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_attn(x, m, m, src_mask)</span></span></code></pre>
<ul>
<li>将4参数函数适配为SublayerConnection要求的1参数接口</li>
<li><code>m</code>和<code>src_mask</code>通过闭包机制被捕获</li>
<li>体现了函数式编程的优雅</li>
</ul>
<p><strong>数据流的完整路径：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 输入：部分生成的目标序列 + 完整源序列编码</span></span>
<span class="line"><span style="color:#E1E4E8">target_partial </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x      </span><span style="color:#6A737D"># [batch, target_len, d_model]</span></span>
<span class="line"><span style="color:#E1E4E8">source_memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> memory  </span><span style="color:#6A737D"># [batch, source_len, d_model]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 阶段1：目标序列自注意力</span></span>
<span class="line"><span style="color:#E1E4E8">self_attended </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> self_attention(target_partial, target_partial, target_partial, tgt_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(layernorm(self_attended))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 阶段2：交叉注意力（目标查询源）</span></span>
<span class="line"><span style="color:#E1E4E8">cross_attended </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cross_attention(x, source_memory, source_memory, src_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(layernorm(cross_attended))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 阶段3：前馈网络</span></span>
<span class="line"><span style="color:#E1E4E8">ff_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> feed_forward(x)</span></span>
<span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> dropout(layernorm(ff_output))</span></span></code></pre>
<p><strong>性能优化的设计考虑：</strong></p>
<ul>
<li><strong>并行化</strong>：自注意力和交叉注意力都可以并行计算</li>
<li><strong>缓存重用</strong>：推理时可以缓存已计算的key和value</li>
<li><strong>内存效率</strong>：残差连接避免额外的内存分配</li>
</ul>
<p>这种三阶段设计使得解码器既能<strong>维护生成序列的内部一致性</strong>，又能<strong>有效利用源序列信息</strong>，还能<strong>进行复杂的语义变换</strong>，是序列到序列生成任务的核心架构。</p>
<hr>
<h2 id="单元格-30-32subsequent_mask-函数---防止看到未来">单元格 30-32：subsequent_mask 函数 - 防止看到未来</h2>
<h3 id="是什么-自回归生成的守护机制"><strong>是什么：</strong> 自回归生成的守护机制</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> subsequent_mask</span><span style="color:#E1E4E8">(size):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Mask out subsequent positions."</span></span>
<span class="line"><span style="color:#E1E4E8">    attn_shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, size, size)</span></span>
<span class="line"><span style="color:#E1E4E8">    subsequent_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.triu(torch.ones(attn_shape), </span><span style="color:#FFAB70">diagonal</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">).type(</span></span>
<span class="line"><span style="color:#E1E4E8">        torch.uint8</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> subsequent_mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span></code></pre>
<p><strong>掩码的数学本质：</strong></p>
<ul>
<li><strong>上三角矩阵</strong>：只保留下三角和对角线的信息</li>
<li><strong>因果关系</strong>：确保时间步t只能看到时间步≤t的信息</li>
<li><strong>自回归约束</strong>：维护生成过程的单向性</li>
</ul>
<h3 id="为什么需要这个掩码-自回归生成的根本要求"><strong>为什么需要这个掩码：</strong> 自回归生成的根本要求</h3>
<p><strong>自回归模型的基本原理：</strong></p>
<ul>
<li>在生成序列时，每个位置只能依赖于前面已经生成的位置</li>
<li>这模拟了真实的生成过程：写文章时，下一句依赖于前面已写的内容</li>
</ul>
<p><strong>“作弊”问题的严重性：</strong></p>
<ol>
<li><strong>训练与推理不一致</strong>：训练时如果能看到未来，推理时却不能</li>
<li><strong>模型能力虚高</strong>：模型可能学会”抄答案”而非真正理解</li>
<li><strong>泛化能力差</strong>：在真实场景中性能会大幅下降</li>
</ol>
<p><strong>因果掩码的哲学意义：</strong></p>
<ul>
<li><strong>时间的不可逆性</strong>：模拟现实世界中时间的单向流动</li>
<li><strong>信息的渐进性</strong>：知识和理解是逐步积累的过程</li>
<li><strong>决策的顺序性</strong>：每个决策基于当前可用的信息</li>
</ul>
<h3 id="怎么做-掩码的技术实现细节"><strong>怎么做：</strong> 掩码的技术实现细节</h3>
<p><strong>逐步构建掩码的过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> subsequent_mask</span><span style="color:#E1E4E8">(size):</span></span>
<span class="line"><span style="color:#E1E4E8">    attn_shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, size, size)          </span><span style="color:#6A737D"># 步骤1：定义形状</span></span>
<span class="line"><span style="color:#E1E4E8">    subsequent_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.triu(          </span><span style="color:#6A737D"># 步骤2：创建上三角</span></span>
<span class="line"><span style="color:#E1E4E8">        torch.ones(attn_shape), </span></span>
<span class="line"><span style="color:#FFAB70">        diagonal</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span></span>
<span class="line"><span style="color:#E1E4E8">    ).type(torch.uint8)                   </span><span style="color:#6A737D"># 步骤3：类型转换</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> subsequent_mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">           # 步骤4：逻辑反转</span></span></code></pre>
<p><strong>关键函数详解：</strong></p>
<p><strong>torch.triu的工作原理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">torch.triu(torch.ones(</span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">), </span><span style="color:#FFAB70">diagonal</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># 输出：</span></span>
<span class="line"><span style="color:#6A737D"># [[0, 1, 1],</span></span>
<span class="line"><span style="color:#6A737D">#  [0, 0, 1], </span></span>
<span class="line"><span style="color:#6A737D">#  [0, 0, 0]]</span></span></code></pre>
<ul>
<li><code>triu</code>：“triangular upper”的缩写</li>
<li><code>diagonal=1</code>：从主对角线上方一位开始</li>
<li>保留上三角部分，其余置为0</li>
</ul>
<p><strong>逻辑反转的必要性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">subsequent_mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span></code></pre>
<ul>
<li>PyTorch的注意力机制中：True表示可以关注，False表示屏蔽</li>
<li>我们要屏蔽上三角（未来位置），所以需要反转逻辑</li>
</ul>
<p><strong>形状设计的考虑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">attn_shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, size, size)  </span><span style="color:#6A737D"># (batch_dim, seq_len, seq_len)</span></span></code></pre>
<ul>
<li>第一维为1，便于广播到任意batch_size</li>
<li>后两维构成注意力矩阵的形状</li>
</ul>
<p><strong>实际掩码效果演示：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 对于长度为4的序列</span></span>
<span class="line"><span style="color:#E1E4E8">mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> subsequent_mask(</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(mask.squeeze())</span></span>
<span class="line"><span style="color:#6A737D"># 输出：</span></span>
<span class="line"><span style="color:#6A737D"># [[True,  False, False, False],</span></span>
<span class="line"><span style="color:#6A737D">#  [True,  True,  False, False],</span></span>
<span class="line"><span style="color:#6A737D">#  [True,  True,  True,  False],</span></span>
<span class="line"><span style="color:#6A737D">#  [True,  True,  True,  True ]]</span></span></code></pre>
<p><strong>掩码的语义解释：</strong></p>
<ul>
<li><strong>位置0</strong>：只能看到自己（冷启动状态）</li>
<li><strong>位置1</strong>：可以看到位置0和1（基于前一个词生成）</li>
<li><strong>位置2</strong>：可以看到位置0,1,2（基于前两个词生成）</li>
<li><strong>位置3</strong>：可以看到所有位置（基于完整上下文生成）</li>
</ul>
<p><strong>在注意力计算中的应用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 注意力分数计算</span></span>
<span class="line"><span style="color:#E1E4E8">scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.matmul(Q, K.transpose(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> math.sqrt(d_k)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 应用掩码</span></span>
<span class="line"><span style="color:#E1E4E8">scores.masked_fill_(mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1e9</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 将False位置设为很大的负数</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Softmax后，-1e9变为接近0的概率</span></span>
<span class="line"><span style="color:#E1E4E8">attention_weights </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.softmax(scores, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>数值技巧的重要性：</strong></p>
<ul>
<li>使用<code>-1e9</code>而不是<code>-inf</code>：避免数值不稳定</li>
<li>经过softmax后，大负数变为接近0的正数</li>
<li>实现了”软掩码”而非”硬截断”</li>
</ul>
<p><strong>掩码与训练效率：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Teacher Forcing + 掩码的组合</span></span>
<span class="line"><span style="color:#6A737D"># 训练时：已知完整目标序列，但用掩码防止看到未来</span></span>
<span class="line"><span style="color:#E1E4E8">target_input </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> target_sequence[:, :</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># 去掉最后一个</span></span>
<span class="line"><span style="color:#E1E4E8">target_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> target_sequence[:, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">:]  </span><span style="color:#6A737D"># 去掉第一个</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 一次性计算所有位置，但每个位置只能看到合法的历史</span></span>
<span class="line"><span style="color:#E1E4E8">decoder_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> decoder(target_input, encoder_memory, src_mask, tgt_mask)</span></span></code></pre>
<p><strong>推理时的逐步生成：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 推理时的自回归生成</span></span>
<span class="line"><span style="color:#E1E4E8">generated </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span><span style="color:#79B8FF">START_TOKEN</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(max_length):</span></span>
<span class="line"><span style="color:#6A737D">    # 当前掩码只需要考虑已生成的部分</span></span>
<span class="line"><span style="color:#E1E4E8">    current_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> subsequent_mask(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(generated))</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#6A737D">    # 生成下一个token</span></span>
<span class="line"><span style="color:#E1E4E8">    output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> decoder(generated, encoder_memory, src_mask, current_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">    next_token </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> output[:, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, :].argmax(</span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    generated.append(next_token)</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> next_token </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> END_TOKEN</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">        break</span></span></code></pre>
<p>这种掩码机制确保了模型在<strong>训练时学会正确的依赖关系</strong>，在<strong>推理时能够逐步生成合理的序列</strong>，是自回归语言模型的核心技术基础。</p>
<hr>
<h2 id="单元格-33-34example_mask-函数---可视化掩码">单元格 33-34：example_mask 函数 - 可视化掩码</h2>
<p>这个函数创建了一个可视化图表，展示掩码的效果。它使用 Altair 库创建热力图，帮助理解哪些位置可以相互关注。</p>
<hr>
<h2 id="单元格-35-37注意力机制---transformer-的核心">单元格 35-37：注意力机制 - Transformer 的核心</h2>
<h3 id="是什么-序列建模的革命性机制"><strong>是什么：</strong> 序列建模的革命性机制</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> attention</span><span style="color:#E1E4E8">(query, key, value, mask</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Compute 'Scaled Dot Product Attention'"</span></span>
<span class="line"><span style="color:#E1E4E8">    d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> query.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.matmul(query, key.transpose(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> math.sqrt(d_k)</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores.masked_fill(mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1e9</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    p_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores.softmax(</span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> dropout </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        p_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> dropout(p_attn)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> torch.matmul(p_attn, value), p_attn</span></span></code></pre>
<p><strong>注意力机制的本质：</strong></p>
<ul>
<li><strong>信息检索系统</strong>：从大量信息中找到与当前任务最相关的部分</li>
<li><strong>动态权重分配</strong>：根据内容自适应地分配注意力权重</li>
<li><strong>关系建模工具</strong>：建立序列中任意两个位置之间的联系</li>
</ul>
<h3 id="为什么注意力机制如此重要-解决序列建模的根本问题"><strong>为什么注意力机制如此重要：</strong> 解决序列建模的根本问题</h3>
<p><strong>传统RNN的局限性：</strong></p>
<ol>
<li><strong>顺序依赖</strong>：必须逐步处理，无法并行计算</li>
<li><strong>长距离问题</strong>：远距离信息容易丢失（梯度消失）</li>
<li><strong>固定容量</strong>：隐藏状态容量有限，长序列信息压缩困难</li>
<li><strong>位置偏见</strong>：更多关注近期信息，忽略远程依赖</li>
</ol>
<p><strong>注意力机制的革命性解决：</strong></p>
<ol>
<li><strong>直接连接</strong>：任意两个位置可以直接交互，路径长度为1</li>
<li><strong>并行计算</strong>：所有位置可以同时计算注意力</li>
<li><strong>动态容量</strong>：根据需要分配注意力，没有固定瓶颈</li>
<li><strong>全局视野</strong>：平等对待所有位置，无位置偏见</li>
</ol>
<p><strong>Query-Key-Value的哲学意义：</strong></p>
<ul>
<li><strong>Query</strong>：“我想要什么？” - 表达信息需求</li>
<li><strong>Key</strong>：“我是什么？” - 表达信息标识</li>
<li><strong>Value</strong>：“我有什么？” - 表达信息内容</li>
</ul>
<h3 id="怎么做-注意力的计算细节和技术实现"><strong>怎么做：</strong> 注意力的计算细节和技术实现</h3>
<p><strong>第一步：相似度计算</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> query.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.matmul(query, key.transpose(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> math.sqrt(d_k)</span></span></code></pre>
<p><strong>点积注意力的数学原理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Attention(Q,K,V) = softmax(QK^T/√d_k)V</span></span></code></pre>
<p><strong>为什么使用点积：</strong></p>
<ol>
<li><strong>计算效率</strong>：矩阵乘法高度优化，GPU友好</li>
<li><strong>语义直觉</strong>：点积衡量向量相似度的自然选择</li>
<li><strong>可微分性</strong>：完全可微，支持端到端训练</li>
<li><strong>缩放性</strong>：通过批次矩阵操作实现高效并行</li>
</ol>
<p><strong>缩放因子的深层作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">/</span><span style="color:#E1E4E8"> math.sqrt(d_k)</span></span></code></pre>
<p><strong>为什么需要缩放：</strong></p>
<ol>
<li><strong>数值稳定性</strong>：防止点积过大导致softmax饱和</li>
<li><strong>梯度保持</strong>：保持梯度在合理范围，避免梯度消失</li>
<li><strong>维度无关</strong>：使注意力机制对特征维度不敏感</li>
<li><strong>理论基础</strong>：保持点积的方差为1（假设输入是标准正态分布）</li>
</ol>
<p><strong>数学推导：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>假设 q_i, k_j 是独立的标准正态分布</span></span>
<span class="line"><span>E[q_i * k_j] = 0</span></span>
<span class="line"><span>Var(q_i * k_j) = 1</span></span>
<span class="line"><span>对于 d_k 维向量：Var(q * k) = d_k</span></span>
<span class="line"><span>因此需要除以 sqrt(d_k) 来保持方差为1</span></span></code></pre>
<p><strong>第二步：掩码应用</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    scores </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores.masked_fill(mask </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1e9</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>掩码的精确机制：</strong></p>
<ul>
<li><code>mask == 0</code>：找到需要屏蔽的位置</li>
<li><code>-1e9</code>：使用大负数而非负无穷，避免数值问题</li>
<li><code>masked_fill</code>：就地替换，内存效率高</li>
</ul>
<p><strong>为什么使用-1e9：</strong></p>
<ul>
<li>softmax(-1e9) ≈ 0，但仍是有限数值</li>
<li>避免NaN和Inf的传播</li>
<li>保持数值计算的稳定性</li>
</ul>
<p><strong>第三步：概率归一化</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">p_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scores.softmax(</span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>Softmax的核心作用：</strong></p>
<ol>
<li><strong>概率解释</strong>：将任意实数转为概率分布</li>
<li><strong>竞争机制</strong>：突出重要信息，抑制不重要信息</li>
<li><strong>可微分性</strong>：平滑函数，梯度传播友好</li>
<li><strong>归一化</strong>：确保所有注意力权重和为1</li>
</ol>
<p><strong>第四步：正则化</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> dropout </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    p_attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> dropout(p_attn)</span></span></code></pre>
<p><strong>注意力Dropout的特殊意义：</strong></p>
<ul>
<li><strong>连接稀疏化</strong>：随机断开一些注意力连接</li>
<li><strong>鲁棒性增强</strong>：防止过度依赖特定位置</li>
<li><strong>泛化能力</strong>：提高模型的泛化性能</li>
</ul>
<p><strong>第五步：信息聚合</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> torch.matmul(p_attn, value), p_attn</span></span></code></pre>
<p><strong>加权聚合的数学含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>output_i = Σ(attention_weights_ij * value_j)</span></span></code></pre>
<ul>
<li>每个输出位置是所有Value的加权平均</li>
<li>权重由注意力分数决定</li>
<li>实现了<strong>内容基础的信息选择</strong></li>
</ul>
<p><strong>形状变换的追踪：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设：batch_size=32, seq_len=20, d_model=512</span></span>
<span class="line"><span style="color:#E1E4E8">query.shape    </span><span style="color:#6A737D"># [32, 20, 512]</span></span>
<span class="line"><span style="color:#E1E4E8">key.shape      </span><span style="color:#6A737D"># [32, 20, 512]  </span></span>
<span class="line"><span style="color:#E1E4E8">value.shape    </span><span style="color:#6A737D"># [32, 20, 512]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 计算注意力分数</span></span>
<span class="line"><span style="color:#E1E4E8">scores.shape   </span><span style="color:#6A737D"># [32, 20, 20]  # (batch, seq_len, seq_len)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 注意力权重</span></span>
<span class="line"><span style="color:#E1E4E8">p_attn.shape   </span><span style="color:#6A737D"># [32, 20, 20]  # 每行是一个概率分布</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 最终输出</span></span>
<span class="line"><span style="color:#E1E4E8">output.shape   </span><span style="color:#6A737D"># [32, 20, 512] # 与输入相同形状</span></span></code></pre>
<p><strong>注意力矩阵的解释：</strong></p>
<ul>
<li><code>p_attn[b, i, j]</code>：在批次b中，位置i对位置j的注意力权重</li>
<li>每行代表一个query对所有key的注意力分布</li>
<li>每列代表所有query对某个key的关注程度</li>
</ul>
<p><strong>实际应用示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 机器翻译场景</span></span>
<span class="line"><span style="color:#6A737D"># 德语："Das ist ein Buch"</span></span>
<span class="line"><span style="color:#6A737D"># 英语："This is a book"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 生成"book"时的注意力权重可能是：</span></span>
<span class="line"><span style="color:#6A737D"># Das: 0.1, ist: 0.1, ein: 0.2, Buch: 0.6</span></span>
<span class="line"><span style="color:#6A737D"># 模型学会了"Buch"对应"book"</span></span></code></pre>
<p>这种设计让模型能够<strong>自动发现重要信息</strong>，<strong>建立长距离依赖</strong>，<strong>实现并行计算</strong>，是Transformer架构成功的核心基础。</p>
<hr>
<h2 id="单元格-38-40多头注意力机制---让模型从多个角度看问题">单元格 38-40：多头注意力机制 - 让模型从多个角度看问题</h2>
<h3 id="是什么-并行的多视角信息处理系统"><strong>是什么：</strong> 并行的多视角信息处理系统</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> MultiHeadedAttention</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, h, d_model, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Take in model size and number of heads."</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(MultiHeadedAttention, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#F97583">        assert</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> h </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#6A737D">        # We assume d_v always equals d_k</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> None</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(</span><span style="color:#FFAB70">p</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, query, key, value, mask</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Implements Figure 2"</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#6A737D">            # Same mask applied to all h heads.</span></span>
<span class="line"><span style="color:#E1E4E8">            mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mask.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        nbatches </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> query.size(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 1) Do all the linear projections in batch from d_model => h x d_k</span></span>
<span class="line"><span style="color:#E1E4E8">        query, key, value </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">            lin(x).view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_k).transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> lin, x </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> zip</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears, (query, key, value))</span></span>
<span class="line"><span style="color:#E1E4E8">        ]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 2) Apply attention on all the projected vectors in batch.</span></span>
<span class="line"><span style="color:#E1E4E8">        x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> attention(</span></span>
<span class="line"><span style="color:#E1E4E8">            query, key, value, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">mask, </span><span style="color:#FFAB70">dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 3) "Concat" using a view and apply a final linear.</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">            x.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">            .contiguous()</span></span>
<span class="line"><span style="color:#E1E4E8">            .view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.d_k)</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> query</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> key</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> value</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.linears[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x)</span></span></code></pre>
<p><strong>多头注意力的核心理念：</strong></p>
<ul>
<li><strong>认知多样性</strong>：不同的”注意力头”关注不同的语言现象</li>
<li><strong>并行处理</strong>：多个头同时工作，提高计算效率</li>
<li><strong>表示丰富性</strong>：组合多个视角得到更全面的理解</li>
</ul>
<h3 id="为什么需要多头-单头注意力的局限性"><strong>为什么需要多头：</strong> 单头注意力的局限性</h3>
<p><strong>单头注意力的问题：</strong></p>
<ol>
<li><strong>视角单一</strong>：只能从一个角度理解信息</li>
<li><strong>容量限制</strong>：单一表示空间可能无法捕获所有模式</li>
<li><strong>注意力冲突</strong>：不同类型的信息竞争同一个注意力通道</li>
<li><strong>表达瓶颈</strong>：复杂的语言现象需要多维度表示</li>
</ol>
<p><strong>多头的优势解析：</strong></p>
<ol>
<li>
<p><strong>专业化分工</strong>：</p>
<ul>
<li>头1：关注句法结构（主谓宾关系）</li>
<li>头2：关注语义相似性</li>
<li>头3：关注词汇对应关系</li>
<li>头4：关注上下文信息</li>
</ul>
</li>
<li>
<p><strong>并行效率</strong>：</p>
<ul>
<li>所有头同时计算，没有顺序依赖</li>
<li>充分利用现代GPU的并行计算能力</li>
</ul>
</li>
<li>
<p><strong>鲁棒性增强</strong>：</p>
<ul>
<li>即使部分头失效，其他头仍能维持功能</li>
<li>不同头之间形成互补关系</li>
</ul>
</li>
</ol>
<h3 id="怎么做-多头机制的精密实现"><strong>怎么做：</strong> 多头机制的精密实现</h3>
<p><strong>初始化的设计思考：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, h, d_model, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    assert</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> h </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>维度分配的数学逻辑：</strong></p>
<ul>
<li><strong>约束条件</strong>：<code>d_model % h == 0</code> 确保均匀分割</li>
<li><strong>单头维度</strong>：<code>d_k = d_model // h</code> 每头处理的维度</li>
<li><strong>总维度保持</strong>：<code>h * d_k = d_model</code> 确保信息不丢失</li>
</ul>
<p><strong>线性层的四重作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ol>
<li><strong>W_Q</strong>：Query投影矩阵</li>
<li><strong>W_K</strong>：Key投影矩阵</li>
<li><strong>W_V</strong>：Value投影矩阵</li>
<li><strong>W_O</strong>：输出投影矩阵</li>
</ol>
<p><strong>前向传播的三阶段处理：</strong></p>
<p><strong>阶段1：并行投影和重塑</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">query, key, value </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    lin(x).view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_k).transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> lin, x </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> zip</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears, (query, key, value))</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span></code></pre>
<p><strong>张量变换的详细追踪：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 输入形状</span></span>
<span class="line"><span style="color:#E1E4E8">input_shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [batch_size, seq_len, d_model]  </span><span style="color:#6A737D"># [32, 20, 512]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 线性投影</span></span>
<span class="line"><span style="color:#E1E4E8">projected </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> linear(</span><span style="color:#79B8FF">input</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [32, 20, 512] → [32, 20, 512]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 重塑为多头</span></span>
<span class="line"><span style="color:#E1E4E8">reshaped </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> projected.view(</span><span style="color:#79B8FF">32</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">20</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">64</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [batch, seq, heads, head_dim]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 转置以便并行处理</span></span>
<span class="line"><span style="color:#E1E4E8">transposed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reshaped.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [32, 8, 20, 64] [batch, heads, seq, head_dim]</span></span></code></pre>
<p><strong>为什么这样重塑：</strong></p>
<ol>
<li><strong>并行计算</strong>：将不同头放在不同维度，便于并行处理</li>
<li><strong>内存布局</strong>：优化内存访问模式，提高计算效率</li>
<li><strong>批次处理</strong>：所有头可以用一次矩阵乘法完成</li>
</ol>
<p><strong>阶段2：并行注意力计算</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> attention(query, key, value, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">mask, </span><span style="color:#FFAB70">dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout)</span></span></code></pre>
<p><strong>并行处理的效率分析：</strong></p>
<ul>
<li><strong>输入</strong>：每个头获得独立的Q、K、V投影</li>
<li><strong>计算</strong>：8个头的注意力同时计算</li>
<li><strong>输出</strong>：每个头产生独立的表示</li>
</ul>
<p><strong>阶段3：头融合和输出投影</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">    x.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    .contiguous()</span></span>
<span class="line"><span style="color:#E1E4E8">    .view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.d_k)</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.linears[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x)</span></span></code></pre>
<p><strong>融合过程的技术细节：</strong></p>
<p><strong>形状恢复：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [32, 8, 20, 64] → [32, 20, 8, 64]</span></span></code></pre>
<p><strong>内存连续性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">.contiguous()</span></span></code></pre>
<ul>
<li>PyTorch中transpose可能导致内存不连续</li>
<li>contiguous()确保内存布局适合后续操作</li>
<li>这是view()操作的前提条件</li>
</ul>
<p><strong>拼接操作：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">.view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.d_k)  </span><span style="color:#6A737D"># [32, 20, 8, 64] → [32, 20, 512]</span></span></code></pre>
<ul>
<li>将多个头的输出拼接成一个向量</li>
<li>相当于concat操作，但更高效</li>
</ul>
<p><strong>最终投影：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x)  </span><span style="color:#6A737D"># 通过W_O矩阵进行最终变换</span></span></code></pre>
<p><strong>掩码处理的技巧：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mask.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 为头维度添加广播维度</span></span></code></pre>
<ul>
<li>原始掩码形状：[batch, seq_len, seq_len]</li>
<li>扩展后形状：[batch, 1, seq_len, seq_len]</li>
<li>广播到所有头：[batch, heads, seq_len, seq_len]</li>
</ul>
<p><strong>内存优化的考虑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> query</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> key  </span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> value</span></span></code></pre>
<ul>
<li>显式删除中间变量释放GPU内存</li>
<li>在大模型训练中，内存管理至关重要</li>
</ul>
<p><strong>多头注意力的数学公式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O</span></span>
<span class="line"><span>where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</span></span></code></pre>
<p><strong>实际效果分析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 单头注意力容量：512维表示空间</span></span>
<span class="line"><span style="color:#6A737D"># 多头注意力容量：8个64维子空间 = 8倍的表示多样性</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 每个头专注不同方面：</span></span>
<span class="line"><span style="color:#6A737D"># head_1: 句法依赖关系</span></span>
<span class="line"><span style="color:#6A737D"># head_2: 语义相似性  </span></span>
<span class="line"><span style="color:#6A737D"># head_3: 共指消解</span></span>
<span class="line"><span style="color:#6A737D"># head_4: 词汇对应</span></span>
<span class="line"><span style="color:#6A737D"># ...</span></span></code></pre>
<p><strong>计算复杂度分析：</strong></p>
<ul>
<li><strong>时间复杂度</strong>：O(n²d) （n=序列长度，d=模型维度）</li>
<li><strong>空间复杂度</strong>：O(n²h) （h=头数）</li>
<li><strong>并行度</strong>：头数量和序列长度都可以并行</li>
</ul>
<p>这种设计让Transformer能够<strong>同时处理多种语言现象</strong>，<strong>高效利用计算资源</strong>，<strong>提供丰富的表示能力</strong>，是其强大性能的关键技术基础。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> MultiHeadedAttention</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, h, d_model, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Take in model size and number of heads."</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(MultiHeadedAttention, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#F97583">        assert</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> h </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#6A737D">        # We assume d_v always equals d_k</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> None</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(</span><span style="color:#FFAB70">p</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, query, key, value, mask</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Implements Figure 2"</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#6A737D">            # Same mask applied to all h heads.</span></span>
<span class="line"><span style="color:#E1E4E8">            mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mask.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        nbatches </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> query.size(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 1) Do all the linear projections in batch from d_model => h x d_k</span></span>
<span class="line"><span style="color:#E1E4E8">        query, key, value </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">            lin(x).view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_k).transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> lin, x </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> zip</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears, (query, key, value))</span></span>
<span class="line"><span style="color:#E1E4E8">        ]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 2) Apply attention on all the projected vectors in batch.</span></span>
<span class="line"><span style="color:#E1E4E8">        x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> attention(</span></span>
<span class="line"><span style="color:#E1E4E8">            query, key, value, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">mask, </span><span style="color:#FFAB70">dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # 3) "Concat" using a view and apply a final linear.</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">            x.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">            .contiguous()</span></span>
<span class="line"><span style="color:#E1E4E8">            .view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.d_k)</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> query</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> key</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> value</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.linears[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x)</span></span></code></pre>
<p><strong>多头注意力的核心思想：</strong>
多头注意力就像是让模型从多个不同的角度来观察同一件事情。想象一下：</p>
<ul>
<li>如果你只用一只眼睛看东西，你只能得到一个视角</li>
<li>用两只眼睛看，你可以感知深度</li>
<li>多头注意力让模型用”多只眼睛”来理解语言</li>
</ul>
<p><strong>构造函数详解：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, h, d_model, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">    super</span><span style="color:#E1E4E8">(MultiHeadedAttention, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#F97583">    assert</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> h </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> h</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> None</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(</span><span style="color:#FFAB70">p</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">dropout)</span></span></code></pre>
<p><strong>参数解释：</strong></p>
<ul>
<li><code>h</code>：注意力头的数量（论文中是 8）</li>
<li><code>d_model</code>：模型的维度（论文中是 512）</li>
<li><code>dropout</code>：dropout 概率</li>
</ul>
<p><strong>assert 语句：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">assert</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> h </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span></span></code></pre>
<ul>
<li><code>assert</code> 是 Python 的断言语句，用于检查条件是否为真</li>
<li>这里检查 <code>d_model</code> 是否能被 <code>h</code> 整除</li>
<li>如果不能整除，程序会报错并停止</li>
<li>这确保每个头都有相同的维度</li>
</ul>
<p><strong>关键计算：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> h</span></span></code></pre>
<ul>
<li><code>//</code> 是整数除法运算符</li>
<li>如果 <code>d_model=512</code>, <code>h=8</code>，那么 <code>d_k=64</code></li>
<li>这意味着每个注意力头处理 64 维的特征</li>
</ul>
<p><strong>线性变换层：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> clones(nn.Linear(d_model, d_model), </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li>创建 4 个线性变换层</li>
<li>前 3 个用于 Query、Key、Value 的变换</li>
<li>第 4 个用于最后的输出变换</li>
</ul>
<p><strong>forward 方法详解：</strong></p>
<p><strong>步骤 1：处理掩码</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mask </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mask.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li><code>unsqueeze(1)</code> 在第 1 个维度插入一个新维度</li>
<li>这是为了让掩码适配多头的结构</li>
</ul>
<p><strong>步骤 2：线性变换和重塑</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">query, key, value </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    lin(x).view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_k).transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> lin, x </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> zip</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.linears, (query, key, value))</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span></code></pre>
<p>这是一个复杂的列表推导式，让我们分解它：</p>
<ol>
<li>
<p><code>zip(self.linears, (query, key, value))</code>：</p>
<ul>
<li>将线性层和输入配对</li>
<li>第一个线性层配对 query</li>
<li>第二个线性层配对 key</li>
<li>第三个线性层配对 value</li>
</ul>
</li>
<li>
<p><code>lin(x)</code>：</p>
<ul>
<li>对输入应用线性变换</li>
<li>输入形状：<code>[batch_size, seq_len, d_model]</code></li>
<li>输出形状：<code>[batch_size, seq_len, d_model]</code></li>
</ul>
</li>
<li>
<p><code>.view(nbatches, -1, self.h, self.d_k)</code>：</p>
<ul>
<li>重塑张量形状</li>
<li><code>-1</code> 表示自动计算这个维度的大小</li>
<li>新形状：<code>[batch_size, seq_len, h, d_k]</code></li>
<li>这将 <code>d_model</code> 维度分割成 <code>h</code> 个 <code>d_k</code> 维的头</li>
</ul>
</li>
<li>
<p><code>.transpose(1, 2)</code>：</p>
<ul>
<li>交换第 1 和第 2 个维度</li>
<li>最终形状：<code>[batch_size, h, seq_len, d_k]</code></li>
<li>这样每个头就可以独立处理了</li>
</ul>
</li>
</ol>
<p><strong>view() 和 transpose() 详解：</strong></p>
<p><code>view()</code> 方法：</p>
<ul>
<li>用于改变张量的形状，但不改变数据</li>
<li>类似于 numpy 的 <code>reshape()</code></li>
<li>新形状的元素总数必须与原形状相同</li>
</ul>
<p><code>transpose()</code> 方法：</p>
<ul>
<li>交换张量的两个维度</li>
<li><code>.transpose(1, 2)</code> 交换第 1 和第 2 个维度</li>
</ul>
<p><strong>为什么需要这些操作：</strong>
原始输入是 <code>[batch_size, seq_len, d_model]</code>，我们需要将其转换为 <code>[batch_size, h, seq_len, d_k]</code>，这样就可以对每个头独立地计算注意力。</p>
<p><strong>步骤 3：计算注意力</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> attention(</span></span>
<span class="line"><span style="color:#E1E4E8">    query, key, value, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">mask, </span><span style="color:#FFAB70">dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li>使用之前定义的 <code>attention</code> 函数</li>
<li>对所有头同时计算注意力</li>
<li>由于张量形状的设计，每个头都会独立计算</li>
</ul>
<p><strong>步骤 4：合并头并输出</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">    x.transpose(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    .contiguous()</span></span>
<span class="line"><span style="color:#E1E4E8">    .view(nbatches, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.h </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.d_k)</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> query</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> key</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> value</span></span>
<span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.linears[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">](x)</span></span></code></pre>
<p><strong>合并过程：</strong></p>
<ol>
<li><code>.transpose(1, 2)</code>：将形状从 <code>[batch, h, seq_len, d_k]</code> 变为 <code>[batch, seq_len, h, d_k]</code></li>
<li><code>.contiguous()</code>：确保内存布局是连续的，这对 <code>view()</code> 操作是必需的</li>
<li><code>.view(nbatches, -1, self.h * self.d_k)</code>：将形状变为 <code>[batch, seq_len, d_model]</code></li>
<li><code>self.linears[-1](x)</code>：通过最后一个线性层进行最终变换</li>
</ol>
<p><strong>内存管理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> query</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> key</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> value</span></span></code></pre>
<ul>
<li>显式删除不再需要的变量</li>
<li>释放 GPU 内存，在处理大型模型时很重要</li>
</ul>
<p><strong>多头注意力的优势：</strong></p>
<ol>
<li><strong>多样性</strong>：每个头可以关注不同类型的模式</li>
<li><strong>并行性</strong>：所有头可以同时计算</li>
<li><strong>表达能力</strong>：多个头的组合比单个头更强大</li>
<li><strong>稳定性</strong>：即使某些头失效，其他头仍能工作</li>
</ol>
<p><strong>实际例子：</strong>
在翻译任务中，不同的头可能关注：</p>
<ul>
<li>头1：语法结构</li>
<li>头2：语义关系</li>
<li>头3：词汇对应</li>
<li>头4：上下文信息</li>
</ul>
<hr>
<h2 id="单元格-41前馈神经网络---非线性变换">单元格 41：前馈神经网络 - 非线性变换</h2>
<h3 id="是什么-位置级的深度特征变换器"><strong>是什么：</strong> 位置级的深度特征变换器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> PositionwiseFeedForward</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Implements FFN equation."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, d_model, d_ff, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(PositionwiseFeedForward, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.w_1 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_model, d_ff)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.w_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_ff, d_model)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.w_2(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.w_1(x).relu()))</span></span></code></pre>
<p><strong>前馈网络的本质定位：</strong></p>
<ul>
<li><strong>非线性引入器</strong>：为线性的注意力机制添加非线性变换能力</li>
<li><strong>特征混合器</strong>：在高维空间中重新组合和变换特征</li>
<li><strong>位置处理器</strong>：对每个序列位置独立进行深度变换</li>
</ul>
<h3 id="为什么需要前馈网络-弥补注意力机制的不足"><strong>为什么需要前馈网络：</strong> 弥补注意力机制的不足</h3>
<p><strong>注意力机制的局限性：</strong></p>
<ol>
<li><strong>线性约束</strong>：注意力本质上是加权平均，缺乏非线性变换</li>
<li><strong>交互单一</strong>：主要处理位置间关系，缺乏位置内的深度处理</li>
<li><strong>表达瓶颈</strong>：单纯的注意力无法学习复杂的函数映射</li>
<li><strong>模式识别</strong>：难以捕获需要非线性组合的复杂语言模式</li>
</ol>
<p><strong>前馈网络的补强作用：</strong></p>
<ol>
<li><strong>非线性注入</strong>：ReLU激活函数提供强大的非线性建模能力</li>
<li><strong>容量扩张</strong>：中间层维度扩大4倍，提供充足的表示空间</li>
<li><strong>独立处理</strong>：每个位置获得深度的、个性化的特征变换</li>
<li><strong>模式学习</strong>：能够学习复杂的语言模式和规则</li>
</ol>
<p><strong>与注意力的互补关系：</strong></p>
<ul>
<li><strong>注意力</strong>：负责”关注什么”（What to attend）</li>
<li><strong>前馈网络</strong>：负责”如何处理”（How to process）</li>
</ul>
<h3 id="怎么做-前馈网络的精妙设计"><strong>怎么做：</strong> 前馈网络的精妙设计</h3>
<p><strong>网络架构的数学表达：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</span></span></code></pre>
<p><strong>两层结构的设计逻辑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.w_1 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_model, d_ff)    </span><span style="color:#6A737D"># 512 → 2048 (扩张)</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.w_2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Linear(d_ff, d_model)    </span><span style="color:#6A737D"># 2048 → 512 (压缩)</span></span></code></pre>
<p><strong>维度变化的战略意图：</strong></p>
<ol>
<li>
<p><strong>扩张阶段</strong>：<code>d_model → d_ff</code> (512 → 2048)</p>
<ul>
<li>提供4倍的表示容量</li>
<li>允许学习更复杂的特征组合</li>
<li>为非线性变换提供充足空间</li>
</ul>
</li>
<li>
<p><strong>压缩阶段</strong>：<code>d_ff → d_model</code> (2048 → 512)</p>
<ul>
<li>将丰富信息压缩回原始维度</li>
<li>强制网络学习最重要的特征</li>
<li>保持与其他组件的维度兼容性</li>
</ul>
</li>
</ol>
<p><strong>前向传播的四阶段处理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.w_2(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.dropout(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.w_1(x).relu()))</span></span></code></pre>
<p><strong>阶段1：线性扩张</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">expanded </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.w_1(x)  </span><span style="color:#6A737D"># [batch, seq_len, 512] → [batch, seq_len, 2048]</span></span></code></pre>
<ul>
<li>通过权重矩阵W₁进行线性变换</li>
<li>将每个位置的特征向量扩展到高维空间</li>
</ul>
<p><strong>阶段2：非线性激活</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">activated </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> expanded.relu()  </span><span style="color:#6A737D"># 应用ReLU激活函数</span></span></code></pre>
<p><strong>ReLU函数的关键作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>ReLU(x) = max(0, x) = {x if x > 0; 0 if x ≤ 0}</span></span></code></pre>
<ul>
<li><strong>稀疏性</strong>：约50%的神经元被激活，产生稀疏表示</li>
<li><strong>非饱和</strong>：正值区域梯度为1，避免梯度消失</li>
<li><strong>计算效率</strong>：实现简单，计算速度快</li>
<li><strong>生物启发</strong>：模拟神经元的激活模式</li>
</ul>
<p><strong>阶段3：正则化</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">regularized </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(activated)</span></span></code></pre>
<ul>
<li>训练时随机将部分神经元置零</li>
<li>防止过拟合，提高泛化能力</li>
<li>增强模型鲁棒性</li>
</ul>
<p><strong>阶段4：线性压缩</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">output </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.w_2(regularized)  </span><span style="color:#6A737D"># [batch, seq_len, 2048] → [batch, seq_len, 512]</span></span></code></pre>
<ul>
<li>将高维表示压缩回原始维度</li>
<li>提取最重要的特征信息</li>
</ul>
<p><strong>位置独立性的重要含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 前馈网络对每个位置独立处理</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(seq_len):</span></span>
<span class="line"><span style="color:#E1E4E8">    output[i] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> FFN(</span><span style="color:#79B8FF">input</span><span style="color:#E1E4E8">[i])  </span><span style="color:#6A737D"># 位置i的处理不依赖其他位置</span></span></code></pre>
<p><strong>为什么设计为位置独立：</strong></p>
<ol>
<li><strong>并行效率</strong>：所有位置可以同时处理</li>
<li><strong>职责分离</strong>：注意力负责位置交互，FFN负责位置内变换</li>
<li><strong>计算优化</strong>：可以高效地批量处理</li>
<li><strong>模块化</strong>：保持架构的清晰和可理解性</li>
</ol>
<p><strong>容量分析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 参数量计算</span></span>
<span class="line"><span style="color:#E1E4E8">w1_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_ff </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 512</span><span style="color:#F97583"> *</span><span style="color:#79B8FF"> 2048</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">0</span><span style="color:#FDAEB7;font-style:italic">48</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">576</span></span>
<span class="line"><span style="color:#E1E4E8">w2_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_ff </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 2048</span><span style="color:#F97583"> *</span><span style="color:#79B8FF"> 512</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">0</span><span style="color:#FDAEB7;font-style:italic">48</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">576</span></span>
<span class="line"><span style="color:#E1E4E8">total_params </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">0</span><span style="color:#FDAEB7;font-style:italic">97</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">152</span><span style="color:#6A737D">  # 约200万参数</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 这占了Transformer单层参数的很大比例</span></span></code></pre>
<p><strong>为什么选择4倍扩张：</strong></p>
<ul>
<li><strong>经验最优</strong>：实验证明4倍是性能和计算的最佳平衡</li>
<li><strong>足够容量</strong>：提供充分的非线性变换能力</li>
<li><strong>计算可行</strong>：在现有硬件上可以高效训练</li>
</ul>
<p><strong>与传统MLP的区别：</strong></p>
<ul>
<li><strong>应用方式</strong>：每个位置独立应用，而非整个序列</li>
<li><strong>维度设计</strong>：扩张-压缩模式，而非单纯增加层数</li>
<li><strong>集成方式</strong>：与注意力机制紧密结合，形成完整架构</li>
</ul>
<p>这种设计使得Transformer在<strong>保持注意力机制优势</strong>的同时，<strong>获得强大的非线性建模能力</strong>，是架构成功的关键组成部分。</p>
<hr>
<h2 id="单元格-42-43嵌入层---将词汇转换为向量">单元格 42-43：嵌入层 - 将词汇转换为向量</h2>
<h3 id="是什么-离散符号到连续向量的桥梁"><strong>是什么：</strong> 离散符号到连续向量的桥梁</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Embeddings</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, d_model, vocab):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(Embeddings, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.lut </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Embedding(vocab, d_model)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.d_model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> d_model</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.lut(x) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> math.sqrt(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_model)</span></span></code></pre>
<p><strong>嵌入层的根本使命：</strong></p>
<ul>
<li><strong>符号化解器</strong>：将离散的词汇ID转换为连续的向量表示</li>
<li><strong>语义编码器</strong>：学习词汇的分布式语义表示</li>
<li><strong>维度统一器</strong>：为后续处理提供统一的向量维度</li>
</ul>
<h3 id="为什么需要嵌入层-计算机理解语言的基础"><strong>为什么需要嵌入层：</strong> 计算机理解语言的基础</h3>
<p><strong>离散vs连续的根本差异：</strong></p>
<ol>
<li><strong>计算机的限制</strong>：只能处理数字，无法直接理解文字符号</li>
<li><strong>语义距离</strong>：离散符号无法表达语义相似性</li>
<li><strong>梯度传播</strong>：离散表示无法进行梯度优化</li>
<li><strong>维度一致</strong>：神经网络需要固定维度的输入</li>
</ol>
<p><strong>词汇嵌入的革命性意义：</strong></p>
<ol>
<li><strong>语义空间</strong>：在高维空间中建立词汇的语义关系</li>
<li><strong>相似性度量</strong>：语义相近的词在向量空间中距离更近</li>
<li><strong>可学习性</strong>：嵌入向量可以通过训练不断优化</li>
<li><strong>泛化能力</strong>：学到的语义表示可以迁移到新任务</li>
</ol>
<h3 id="怎么做-嵌入机制的技术实现"><strong>怎么做：</strong> 嵌入机制的技术实现</h3>
<p><strong>nn.Embedding的工作原理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.lut </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Embedding(vocab, d_model)</span></span></code></pre>
<p><strong>查找表(Look-Up Table)的本质：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 本质上是一个大的权重矩阵</span></span>
<span class="line"><span style="color:#E1E4E8">embedding_matrix.shape </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_size, d_model]  </span><span style="color:#6A737D"># [30000, 512]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 对于输入的词汇ID</span></span>
<span class="line"><span style="color:#E1E4E8">word_id </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1234</span></span>
<span class="line"><span style="color:#E1E4E8">word_vector </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> embedding_matrix[word_id]  </span><span style="color:#6A737D"># 取出对应行作为词向量</span></span></code></pre>
<p><strong>为什么叫”查找表”：</strong></p>
<ul>
<li>给定词汇ID，直接查找对应的向量</li>
<li>比传统的one-hot编码+矩阵乘法更高效</li>
<li>避免了稀疏矩阵的计算开销</li>
</ul>
<p><strong>缩放因子的深层含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.lut(x) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> math.sqrt(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_model)</span></span></code></pre>
<p><strong>为什么乘以√d_model：</strong></p>
<ol>
<li><strong>数值平衡</strong>：与位置编码的量级保持一致</li>
<li><strong>理论基础</strong>：保持方差的数学期望</li>
<li><strong>训练稳定</strong>：避免不同组件贡献的不平衡</li>
<li><strong>论文一致</strong>：与原始Transformer论文保持一致</li>
</ol>
<p><strong>数学推导：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>假设嵌入向量的每个维度都是独立的标准正态分布 N(0,1)</span></span>
<span class="line"><span>则 d_model 维向量的L2范数期望为 √d_model</span></span>
<span class="line"><span>通过乘以 √d_model，使嵌入向量与位置编码具有相似的量级</span></span></code></pre>
<p><strong>嵌入层的学习过程：</strong></p>
<p><strong>初始化阶段：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 随机初始化</span></span>
<span class="line"><span style="color:#E1E4E8">embedding_matrix </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randn(vocab_size, d_model) </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> 0.1</span></span></code></pre>
<p><strong>训练过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 前向传播：根据词汇ID获取向量</span></span>
<span class="line"><span style="color:#E1E4E8">word_vectors </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> embedding(input_ids)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 反向传播：根据损失更新对应的嵌入向量</span></span>
<span class="line"><span style="color:#E1E4E8">loss.backward()  </span><span style="color:#6A737D"># 只有被使用的词汇嵌入会被更新</span></span></code></pre>
<p><strong>共享权重的考虑：</strong>
在某些实现中，输入嵌入和输出投影共享权重：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 共享权重可以减少参数量</span></span>
<span class="line"><span style="color:#E1E4E8">generator.proj.weight </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> embeddings.lut.weight.transpose()</span></span></code></pre>
<p><strong>嵌入维度的选择：</strong></p>
<ul>
<li><strong>d_model=512</strong>：在表达能力和计算效率间的平衡</li>
<li><strong>更大维度</strong>：更强的表达能力，但计算成本更高</li>
<li><strong>更小维度</strong>：计算效率高，但可能损失表达能力</li>
</ul>
<p><strong>词汇表大小的影响：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 参数量计算</span></span>
<span class="line"><span style="color:#E1E4E8">embedding_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> vocab_size </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span></span>
<span class="line"><span style="color:#6A737D"># 例如：30000 * 512 = 15,360,000 参数</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 这通常是模型参数的很大一部分</span></span></code></pre>
<p><strong>实际应用中的技巧：</strong></p>
<ol>
<li><strong>预训练嵌入</strong>：使用Word2Vec、GloVe等预训练向量初始化</li>
<li><strong>子词嵌入</strong>：使用BPE、SentencePiece等处理未登录词</li>
<li><strong>层次嵌入</strong>：对于超大词汇表，使用层次化的嵌入策略</li>
</ol>
<p><strong>嵌入质量的评估：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 语义相似性测试</span></span>
<span class="line"><span style="color:#E1E4E8">similar_words </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> find_similar(embedding_matrix, </span><span style="color:#9ECBFF">"king"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># 期望结果：["queen", "prince", "royal", ...]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 类比关系测试  </span></span>
<span class="line"><span style="color:#6A737D"># king - man + woman ≈ queen</span></span></code></pre>
<p>这种设计将<strong>离散的语言符号</strong>转换为<strong>连续的数值表示</strong>，为神经网络处理自然语言奠定了基础，是连接符号智能和连接主义的关键桥梁。</p>
<p><strong>嵌入层的作用：</strong>
嵌入层是深度学习处理离散符号（如单词）的关键技术：</p>
<ol>
<li><strong>符号到向量</strong>：将单词 ID 转换为稠密向量</li>
<li><strong>语义表示</strong>：相似的单词在向量空间中距离较近</li>
<li><strong>可训练</strong>：嵌入向量在训练过程中会不断优化</li>
</ol>
<p><strong>nn.Embedding 详解：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.lut </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Embedding(vocab, d_model)</span></span></code></pre>
<ul>
<li><code>lut</code> 代表 “Look-Up Table”（查找表）</li>
<li><code>vocab</code>：词汇表大小（比如 30000 个单词）</li>
<li><code>d_model</code>：每个单词的向量维度（512）</li>
<li>本质上是一个大小为 <code>[vocab, d_model]</code> 的矩阵</li>
</ul>
<p><strong>嵌入过程：</strong></p>
<ol>
<li>输入：单词的 ID（整数），比如 [1, 5, 3, 8]</li>
<li>查找：从嵌入矩阵中取出对应行</li>
<li>输出：每个 ID 对应一个 d_model 维的向量</li>
</ol>
<p><strong>缩放因子：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.lut(x) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> math.sqrt(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.d_model)</span></span></code></pre>
<p><strong>为什么要乘以 sqrt(d_model)：</strong></p>
<ol>
<li><strong>数值稳定性</strong>：确保嵌入向量的数值范围合理</li>
<li><strong>与位置编码匹配</strong>：位置编码的数值范围也是这个量级</li>
<li><strong>论文建议</strong>：原始 Transformer 论文中的做法</li>
</ol>
<p><strong>嵌入学习的直觉：</strong></p>
<ul>
<li>初始时：嵌入向量是随机的</li>
<li>训练过程中：相似含义的词会逐渐靠近</li>
<li>最终：语义相近的词在向量空间中聚集</li>
</ul>
<hr>
<h2 id="单元格-44-46位置编码---为序列注入位置信息">单元格 44-46：位置编码 - 为序列注入位置信息</h2>
<h3 id="是什么-序列顺序信息的编码器"><strong>是什么：</strong> 序列顺序信息的编码器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> PositionalEncoding</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Implement the PE function."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, d_model, dropout, max_len</span><span style="color:#F97583">=</span><span style="color:#79B8FF">5000</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(PositionalEncoding, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.dropout </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.Dropout(</span><span style="color:#FFAB70">p</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # Compute the positional encodings once in log space.</span></span>
<span class="line"><span style="color:#E1E4E8">        pe </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(max_len, d_model)</span></span>
<span class="line"><span style="color:#E1E4E8">        position </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, max_len).unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        div_term </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.exp(</span></span>
<span class="line"><span style="color:#E1E4E8">            torch.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, d_model, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#F97583"> -</span><span style="color:#E1E4E8">(math.log(</span><span style="color:#79B8FF">10000.0</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> d_model)</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#E1E4E8">        pe[:, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">::</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.sin(position </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> div_term)</span></span>
<span class="line"><span style="color:#E1E4E8">        pe[:, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">::</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cos(position </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> div_term)</span></span>
<span class="line"><span style="color:#E1E4E8">        pe </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pe.unsqueeze(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.register_buffer(</span><span style="color:#9ECBFF">"pe"</span><span style="color:#E1E4E8">, pe)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.pe[:, : x.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)].requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(x)</span></span></code></pre>
<p><strong>位置编码的核心使命：</strong></p>
<ul>
<li><strong>位置感知器</strong>：为无序的注意力机制注入序列顺序信息</li>
<li><strong>几何编码器</strong>：使用数学函数将位置信息映射到向量空间</li>
<li><strong>永恒记忆</strong>：提供不依赖参数学习的固定位置表示</li>
</ul>
<h3 id="为什么需要位置编码-解决注意力机制的位置盲区"><strong>为什么需要位置编码：</strong> 解决注意力机制的位置盲区</h3>
<p><strong>Transformer架构的根本问题：</strong></p>
<ol>
<li><strong>置换不变性</strong>：注意力机制对输入顺序不敏感</li>
<li><strong>位置无关</strong>：相同的词在不同位置得到相同的处理</li>
<li><strong>语义缺失</strong>：丢失了语言中至关重要的位置信息</li>
<li><strong>结构失效</strong>：无法理解句法结构和语序规则</li>
</ol>
<p><strong>举例说明位置的重要性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>"猫咬了狗" vs "狗咬了猫"  # 相同的词，不同的语义</span></span>
<span class="line"><span>"我很高兴" vs "很我高兴"  # 语序决定合法性</span></span></code></pre>
<p><strong>为什么不用位置嵌入：</strong></p>
<ol>
<li><strong>固定长度限制</strong>：学习的位置嵌入受训练序列长度限制</li>
<li><strong>泛化能力差</strong>：对超出训练长度的序列效果差</li>
<li><strong>参数开销</strong>：需要额外的参数存储</li>
<li><strong>数学优雅性</strong>：函数式编码更简洁和理论化</li>
</ol>
<h3 id="怎么做-正弦位置编码的数学艺术"><strong>怎么做：</strong> 正弦位置编码的数学艺术</h3>
<p><strong>核心数学公式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</span></span>
<span class="line"><span>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</span></span></code></pre>
<p><strong>其中：</strong></p>
<ul>
<li><code>pos</code>：序列中的位置 (0, 1, 2, …)</li>
<li><code>i</code>：维度索引 (0, 1, 2, …, d_model/2-1)</li>
<li><code>2i</code> 和 <code>2i+1</code>：偶数和奇数维度</li>
</ul>
<p><strong>初始化过程的逐步分析：</strong></p>
<p><strong>步骤1：创建位置张量</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">pe </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(max_len, d_model)  </span><span style="color:#6A737D"># [5000, 512]</span></span>
<span class="line"><span style="color:#E1E4E8">position </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, max_len).unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [5000, 1]</span></span></code></pre>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># position 的内容：</span></span>
<span class="line"><span style="color:#6A737D"># [[0], [1], [2], [3], ..., [4999]]</span></span></code></pre>
<p><strong>步骤2：计算频率项</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">div_term </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.exp(</span></span>
<span class="line"><span style="color:#E1E4E8">    torch.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, d_model, </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#F97583"> -</span><span style="color:#E1E4E8">(math.log(</span><span style="color:#79B8FF">10000.0</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> d_model)</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>频率项的数学含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 对于维度 i (i = 0, 1, 2, ...)</span></span>
<span class="line"><span style="color:#E1E4E8">div_term[i] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> exp(</span><span style="color:#F97583">-</span><span style="color:#E1E4E8">log(</span><span style="color:#79B8FF">10000</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#FDAEB7;font-style:italic"> 2i</span><span style="color:#F97583"> /</span><span style="color:#E1E4E8"> d_model)</span></span>
<span class="line"><span style="color:#F97583">            =</span><span style="color:#E1E4E8"> exp(log(</span><span style="color:#79B8FF">10000</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#FDAEB7;font-style:italic">2i</span><span style="color:#F97583">/</span><span style="color:#E1E4E8">d_model)))</span></span>
<span class="line"><span style="color:#F97583">            =</span><span style="color:#79B8FF"> 10000</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#FDAEB7;font-style:italic">2i</span><span style="color:#F97583">/</span><span style="color:#E1E4E8">d_model)</span></span>
<span class="line"><span style="color:#F97583">            =</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> /</span><span style="color:#79B8FF"> 10000</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#FDAEB7;font-style:italic">2i</span><span style="color:#F97583">/</span><span style="color:#E1E4E8">d_model)</span></span></code></pre>
<p><strong>为什么使用这种频率设计：</strong></p>
<ol>
<li><strong>多尺度表示</strong>：不同维度具有不同的频率</li>
<li><strong>几何级数</strong>：频率按几何级数递减，覆盖多个时间尺度</li>
<li><strong>唯一性保证</strong>：每个位置都有唯一的编码向量</li>
<li><strong>相对位置</strong>：能够表达相对位置关系</li>
</ol>
<p><strong>步骤3：应用三角函数</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">pe[:, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">::</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.sin(position </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> div_term)  </span><span style="color:#6A737D"># 偶数维度使用sin</span></span>
<span class="line"><span style="color:#E1E4E8">pe[:, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">::</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cos(position </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> div_term)  </span><span style="color:#6A737D"># 奇数维度使用cos</span></span></code></pre>
<p><strong>三角函数编码的优势：</strong></p>
<p><strong>1. 周期性特征：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># sin和cos函数具有周期性，能够处理长序列</span></span>
<span class="line"><span style="color:#6A737D"># 即使超出训练长度，也能产生有意义的位置编码</span></span></code></pre>
<p><strong>2. 相对位置信息：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 利用三角恒等式：</span></span>
<span class="line"><span style="color:#6A737D"># sin(a + b) = sin(a)cos(b) + cos(a)sin(b)</span></span>
<span class="line"><span style="color:#6A737D"># cos(a + b) = cos(a)cos(b) - sin(a)sin(b)</span></span>
<span class="line"><span style="color:#6A737D"># 位置 pos+k 的编码可以表示为位置 pos 编码的线性组合</span></span></code></pre>
<p><strong>3. 有界性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># sin和cos的值域为[-1, 1]，编码值稳定</span></span>
<span class="line"><span style="color:#6A737D"># 避免了位置编码随位置增长而发散</span></span></code></pre>
<p><strong>步骤4：添加批次维度</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">pe </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pe.unsqueeze(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># [1, 5000, 512]</span></span></code></pre>
<p><strong>步骤5：注册为缓冲区</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.register_buffer(</span><span style="color:#9ECBFF">"pe"</span><span style="color:#E1E4E8">, pe)</span></span></code></pre>
<p><strong>register_buffer的作用：</strong></p>
<ul>
<li><strong>非参数存储</strong>：不作为模型参数，不参与梯度更新</li>
<li><strong>设备同步</strong>：自动跟随模型移动到GPU/CPU</li>
<li><strong>状态保存</strong>：在模型保存/加载时自动处理</li>
</ul>
<p><strong>前向传播的应用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">    x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.pe[:, : x.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)].requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(x)</span></span></code></pre>
<p><strong>关键操作解析：</strong></p>
<p><strong>1. 长度匹配：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.pe[:, : x.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)]  </span><span style="color:#6A737D"># 截取与输入序列长度相同的位置编码</span></span></code></pre>
<p><strong>2. 梯度控制：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 确保位置编码不参与梯度计算</span></span></code></pre>
<p><strong>3. 相加融合：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.pe[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># 词嵌入 + 位置编码</span></span></code></pre>
<p><strong>为什么是相加而不是拼接：</strong></p>
<ol>
<li><strong>维度保持</strong>：不改变特征维度，保持架构简洁</li>
<li><strong>线性叠加</strong>：允许注意力机制同时关注内容和位置</li>
<li><strong>计算效率</strong>：避免维度增长带来的计算开销</li>
<li><strong>理论支持</strong>：线性叠加能够被后续的线性变换有效处理</li>
</ol>
<p><strong>可视化理解：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 不同位置的编码模式：</span></span>
<span class="line"><span style="color:#6A737D"># 位置0: [sin(0/1), cos(0/1), sin(0/100), cos(0/100), ...]</span></span>
<span class="line"><span style="color:#6A737D"># 位置1: [sin(1/1), cos(1/1), sin(1/100), cos(1/100), ...]</span></span>
<span class="line"><span style="color:#6A737D"># 位置2: [sin(2/1), cos(2/1), sin(2/100), cos(2/100), ...]</span></span></code></pre>
<p><strong>频率谱分析：</strong></p>
<ul>
<li><strong>低频成分</strong>：编码长期位置模式</li>
<li><strong>高频成分</strong>：编码短期位置变化</li>
<li><strong>多频融合</strong>：提供丰富的位置表示</li>
</ul>
<p><strong>实际效果验证：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 相邻位置编码的相似性较高</span></span>
<span class="line"><span style="color:#E1E4E8">similarity </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cosine_similarity(pe[pos], pe[pos</span><span style="color:#F97583">+</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 远距离位置编码的相似性较低</span></span>
<span class="line"><span style="color:#E1E4E8">distance_similarity </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cosine_similarity(pe[pos], pe[pos</span><span style="color:#F97583">+</span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">])</span></span></code></pre>
<p>这种设计巧妙地将<strong>数学的优雅性</strong>、<strong>计算的高效性</strong>和<strong>表示的丰富性</strong>完美结合，为Transformer提供了强大而灵活的位置感知能力。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">pe </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pe.unsqueeze(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.register_buffer(</span><span style="color:#9ECBFF">"pe"</span><span style="color:#E1E4E8">, pe)</span></span></code></pre>
<ul>
<li><code>unsqueeze(0)</code> 添加批次维度</li>
<li><code>register_buffer</code> 将 <code>pe</code> 注册为模型的一部分，但不是参数</li>
<li>缓冲区会随模型移动（CPU/GPU），但不会被优化器更新</li>
</ul>
<p><strong>forward 方法：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x):</span></span>
<span class="line"><span style="color:#E1E4E8">    x </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.pe[:, : x.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)].requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.dropout(x)</span></span></code></pre>
<ul>
<li>将位置编码加到嵌入向量上</li>
<li><code>: x.size(1)</code> 只取序列长度对应的位置编码</li>
<li><code>requires_grad_(False)</code> 确保位置编码不参与梯度计算</li>
</ul>
<p><strong>正弦函数的优势：</strong></p>
<ol>
<li><strong>周期性</strong>：可以表示相对位置关系</li>
<li><strong>外推性</strong>：可以处理比训练时更长的序列</li>
<li><strong>数值稳定</strong>：值域在 [-1, 1] 之间</li>
<li><strong>几何性质</strong>：可以通过线性变换表示相对位置</li>
</ol>
<hr>
<h2 id="单元格-47可视化位置编码">单元格 47：可视化位置编码</h2>
<p>这个单元格创建了一个图表，展示不同维度的位置编码是如何随位置变化的。你可以看到正弦和余弦波的周期性模式。</p>
<hr>
<h2 id="单元格-48-49完整模型构建---组装所有组件">单元格 48-49：完整模型构建 - 组装所有组件</h2>
<h3 id="是什么-完整transformer模型的工厂函数"><strong>是什么：</strong> 完整Transformer模型的工厂函数</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> make_model</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#E1E4E8">    src_vocab, tgt_vocab, N</span><span style="color:#F97583">=</span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">, d_model</span><span style="color:#F97583">=</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, d_ff</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2048</span><span style="color:#E1E4E8">, h</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span></span>
<span class="line"><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Helper: Construct a model from hyperparameters."</span></span>
<span class="line"><span style="color:#E1E4E8">    c </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> copy.deepcopy</span></span>
<span class="line"><span style="color:#E1E4E8">    attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> MultiHeadedAttention(h, d_model)</span></span>
<span class="line"><span style="color:#E1E4E8">    ff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionwiseFeedForward(d_model, d_ff, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8">    position </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionalEncoding(d_model, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8">    model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> EncoderDecoder(</span></span>
<span class="line"><span style="color:#E1E4E8">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        Generator(d_model, tgt_vocab),</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # This was important from their code.</span></span>
<span class="line"><span style="color:#6A737D">    # Initialize parameters with Glorot / fan_avg.</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> p </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> model.parameters():</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> p.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            nn.init.xavier_uniform_(p)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> model</span></span></code></pre>
<p><strong>工厂函数的核心职责：</strong></p>
<ul>
<li><strong>组件制造器</strong>：批量创建所有必需的神经网络模块</li>
<li><strong>架构装配器</strong>：按照正确的拓扑结构组装完整模型</li>
<li><strong>参数初始化器</strong>：使用最佳实践初始化所有可训练参数</li>
</ul>
<h3 id="为什么需要工厂函数-模块化设计的智慧"><strong>为什么需要工厂函数：</strong> 模块化设计的智慧</h3>
<p><strong>复杂系统的组装挑战：</strong></p>
<ol>
<li><strong>组件繁多</strong>：编码器、解码器、注意力、前馈网络等众多模块</li>
<li><strong>依赖关系</strong>：各组件间有复杂的参数和结构依赖</li>
<li><strong>配置管理</strong>：需要统一管理超参数和配置</li>
<li><strong>重复使用</strong>：需要创建相同结构的多个实例</li>
</ol>
<p><strong>工厂模式的优势：</strong></p>
<ol>
<li><strong>封装复杂性</strong>：隐藏组装细节，提供简洁接口</li>
<li><strong>保证一致性</strong>：确保所有组件使用相同的超参数</li>
<li><strong>便于实验</strong>：通过修改参数快速创建不同配置的模型</li>
<li><strong>减少错误</strong>：避免手动组装过程中的配置错误</li>
</ol>
<h3 id="怎么做-精密的模型装配流程"><strong>怎么做：</strong> 精密的模型装配流程</h3>
<p><strong>超参数配置解析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> make_model</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#E1E4E8">    src_vocab, tgt_vocab, N</span><span style="color:#F97583">=</span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">, d_model</span><span style="color:#F97583">=</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, d_ff</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2048</span><span style="color:#E1E4E8">, h</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span></span>
<span class="line"><span style="color:#E1E4E8">):</span></span></code></pre>
<p><strong>参数含义与典型值：</strong></p>
<ul>
<li><code>src_vocab</code>：源语言词汇表大小（如英语：30,000）</li>
<li><code>tgt_vocab</code>：目标语言词汇表大小（如中文：50,000）</li>
<li><code>N=6</code>：编码器和解码器层数（原论文标准）</li>
<li><code>d_model=512</code>：模型隐藏维度（平衡性能与计算）</li>
<li><code>d_ff=2048</code>：前馈网络维度（4倍扩张）</li>
<li><code>h=8</code>：多头注意力头数（并行处理）</li>
<li><code>dropout=0.1</code>：正则化强度（防止过拟合）</li>
</ul>
<p><strong>深拷贝策略：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">c </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> copy.deepcopy</span></span></code></pre>
<p><strong>为什么使用深拷贝：</strong></p>
<ol>
<li><strong>独立实例</strong>：每个层都需要独立的参数，不能共享</li>
<li><strong>避免别名</strong>：防止多个引用指向同一个对象</li>
<li><strong>梯度隔离</strong>：确保各层的梯度更新相互独立</li>
<li><strong>内存安全</strong>：避免意外的参数共享导致的训练问题</li>
</ol>
<p><strong>组件模板创建：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> MultiHeadedAttention(h, d_model)          </span><span style="color:#6A737D"># 注意力模板</span></span>
<span class="line"><span style="color:#E1E4E8">ff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionwiseFeedForward(d_model, d_ff, dropout)  </span><span style="color:#6A737D"># 前馈网络模板</span></span>
<span class="line"><span style="color:#E1E4E8">position </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionalEncoding(d_model, dropout)  </span><span style="color:#6A737D"># 位置编码模板</span></span></code></pre>
<p><strong>模板的作用：</strong></p>
<ul>
<li>作为”蓝图”创建多个独立实例</li>
<li>保证所有实例使用相同的架构配置</li>
<li>通过深拷贝确保参数独立性</li>
</ul>
<p><strong>模型主体组装：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> EncoderDecoder(</span></span>
<span class="line"><span style="color:#E1E4E8">    Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">    Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">    nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">    nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">    Generator(d_model, tgt_vocab),</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>组装逻辑深度分析：</strong></p>
<p><strong>编码器构建：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)</span></span></code></pre>
<ul>
<li>创建编码器层模板：<code>EncoderLayer(...)</code></li>
<li>复制N层：每层都是独立的深拷贝</li>
<li>堆叠形成完整编码器</li>
</ul>
<p><strong>解码器构建：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)</span></span></code></pre>
<ul>
<li><strong>注意双重注意力</strong>：<code>c(attn), c(attn)</code>
<ul>
<li>第一个：自注意力（self-attention）</li>
<li>第二个：编码器-解码器注意力（cross-attention）</li>
</ul>
</li>
<li>每个解码器层需要两个独立的注意力模块</li>
</ul>
<p><strong>嵌入层构建：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">nn.Sequential(Embeddings(d_model, src_vocab), c(position))  </span><span style="color:#6A737D"># 源语言</span></span>
<span class="line"><span style="color:#E1E4E8">nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))  </span><span style="color:#6A737D"># 目标语言</span></span></code></pre>
<p><strong>Sequential的设计意图：</strong></p>
<ol>
<li><strong>流水线处理</strong>：词嵌入 → 位置编码</li>
<li><strong>模块化封装</strong>：将两个步骤打包为一个模块</li>
<li><strong>复用性</strong>：编码器和解码器都需要相同的流程</li>
<li><strong>扩展性</strong>：便于添加其他预处理步骤</li>
</ol>
<p><strong>输出层配置：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">Generator(d_model, tgt_vocab)</span></span></code></pre>
<ul>
<li>将隐藏表示映射到目标词汇表</li>
<li>产生最终的词汇概率分布</li>
</ul>
<p><strong>Xavier权重初始化：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> p </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> model.parameters():</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> p.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.init.xavier_uniform_(p)</span></span></code></pre>
<p><strong>Xavier初始化的深层原理：</strong></p>
<p><strong>为什么需要合适的初始化：</strong></p>
<ol>
<li><strong>梯度流动</strong>：不当初始化导致梯度消失或爆炸</li>
<li><strong>激活分布</strong>：保持各层激活值的合理分布</li>
<li><strong>收敛速度</strong>：好的初始化加速训练收敛</li>
<li><strong>性能上限</strong>：影响最终模型的性能表现</li>
</ol>
<p><strong>Xavier公式：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 对于权重矩阵 W ∈ R^(m×n)</span></span>
<span class="line"><span style="color:#E1E4E8">std </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sqrt(</span><span style="color:#79B8FF">2.0</span><span style="color:#F97583"> /</span><span style="color:#E1E4E8"> (fan_in </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> fan_out))</span></span>
<span class="line"><span style="color:#E1E4E8">W </span><span style="color:#F97583">~</span><span style="color:#E1E4E8"> Uniform(</span><span style="color:#F97583">-</span><span style="color:#E1E4E8">std </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> sqrt(</span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">), std </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> sqrt(</span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">))</span></span></code></pre>
<p><strong>其中：</strong></p>
<ul>
<li><code>fan_in</code>：输入维度</li>
<li><code>fan_out</code>：输出维度</li>
<li>保持前向和反向传播的方差稳定</li>
</ul>
<p><strong>维度判断逻辑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> p.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">:</span></span></code></pre>
<ul>
<li><strong>多维参数</strong>：权重矩阵（需要Xavier初始化）</li>
<li><strong>一维参数</strong>：偏置项（通常初始化为0）</li>
</ul>
<p><strong>模型规模分析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 参数量估算（N=6, d_model=512, h=8, d_ff=2048）</span></span>
<span class="line"><span style="color:#E1E4E8">encoder_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> N </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#79B8FF">    3</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">+</span><span style="color:#6A737D">  # 注意力的Q、K、V投影</span></span>
<span class="line"><span style="color:#E1E4E8">    d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">+</span><span style="color:#6A737D">      # 注意力输出投影</span></span>
<span class="line"><span style="color:#79B8FF">    2</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_ff </span><span style="color:#F97583">+</span><span style="color:#6A737D">     # 前馈网络</span></span>
<span class="line"><span style="color:#79B8FF">    4</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model              </span><span style="color:#6A737D"># 层归一化参数</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">decoder_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> N </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#79B8FF">    6</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">+</span><span style="color:#6A737D">  # 两个注意力模块</span></span>
<span class="line"><span style="color:#79B8FF">    2</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_ff </span><span style="color:#F97583">+</span><span style="color:#6A737D">     # 前馈网络</span></span>
<span class="line"><span style="color:#79B8FF">    6</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> d_model              </span><span style="color:#6A737D"># 层归一化参数</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">embedding_params </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (src_vocab </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> tgt_vocab) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span></span>
<span class="line"><span style="color:#E1E4E8">total_params ≈ </span><span style="color:#FDAEB7;font-style:italic">65M</span><span style="color:#6A737D">  # 约6500万参数</span></span></code></pre>
<p><strong>实际使用示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 创建标准Transformer模型</span></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> make_model(</span></span>
<span class="line"><span style="color:#FFAB70">    src_vocab</span><span style="color:#F97583">=</span><span style="color:#79B8FF">30000</span><span style="color:#E1E4E8">,    </span><span style="color:#6A737D"># 英语词汇表</span></span>
<span class="line"><span style="color:#FFAB70">    tgt_vocab</span><span style="color:#F97583">=</span><span style="color:#79B8FF">50000</span><span style="color:#E1E4E8">,    </span><span style="color:#6A737D"># 中文词汇表</span></span>
<span class="line"><span style="color:#FFAB70">    N</span><span style="color:#F97583">=</span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">,                </span><span style="color:#6A737D"># 6层编码器和解码器</span></span>
<span class="line"><span style="color:#FFAB70">    d_model</span><span style="color:#F97583">=</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">,        </span><span style="color:#6A737D"># 512维隐藏状态</span></span>
<span class="line"><span style="color:#FFAB70">    d_ff</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2048</span><span style="color:#E1E4E8">,          </span><span style="color:#6A737D"># 2048维前馈网络</span></span>
<span class="line"><span style="color:#FFAB70">    h</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">,                </span><span style="color:#6A737D"># 8个注意力头</span></span>
<span class="line"><span style="color:#FFAB70">    dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#6A737D">         # 10%的dropout</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"模型参数量: </span><span style="color:#79B8FF">{sum</span><span style="color:#E1E4E8">(p.numel() </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> p </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> model.parameters())</span><span style="color:#F97583">:,</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p>这个工厂函数优雅地封装了Transformer的复杂性，使得研究者和工程师能够专注于模型的使用和改进，而不必纠结于繁琐的组装细节。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">    model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> EncoderDecoder(</span></span>
<span class="line"><span style="color:#E1E4E8">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        Generator(d_model, tgt_vocab),</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span></code></pre>
<p><strong>组装过程分析：</strong></p>
<ol>
<li>
<p><strong>编码器</strong>：<code>Encoder(EncoderLayer(...), N)</code></p>
<ul>
<li>创建一个编码器层的模板</li>
<li>复制 N 次创建完整编码器</li>
</ul>
</li>
<li>
<p><strong>解码器</strong>：<code>Decoder(DecoderLayer(...), N)</code></p>
<ul>
<li>注意解码器层有两个注意力：<code>c(attn), c(attn)</code></li>
<li>第一个是自注意力，第二个是编码器-解码器注意力</li>
</ul>
</li>
<li>
<p><strong>源语言嵌入</strong>：<code>nn.Sequential(Embeddings(...), c(position))</code></p>
<ul>
<li>将嵌入层和位置编码串联</li>
<li>输入先经过嵌入，再加上位置编码</li>
</ul>
</li>
<li>
<p><strong>目标语言嵌入</strong>：同源语言嵌入</p>
</li>
<li>
<p><strong>生成器</strong>：<code>Generator(d_model, tgt_vocab)</code></p>
<ul>
<li>将解码器输出转换为词汇概率</li>
</ul>
</li>
</ol>
<p><strong>参数初始化：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> p </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> model.parameters():</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> p.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.init.xavier_uniform_(p)</span></span></code></pre>
<p><strong>Xavier 初始化：</strong></p>
<ul>
<li>也叫 Glorot 初始化</li>
<li>根据层的输入和输出维度来设置初始权重</li>
<li>目标是保持梯度在合理范围内</li>
<li><code>p.dim() > 1</code> 确保只初始化矩阵参数，不初始化偏置</li>
</ul>
<p><strong>为什么需要好的初始化：</strong></p>
<ol>
<li><strong>梯度流动</strong>：好的初始化帮助梯度正常传播</li>
<li><strong>收敛速度</strong>：影响训练的收敛速度</li>
<li><strong>避免饱和</strong>：防止激活函数进入饱和区域</li>
</ol>
<p>这样，我们就有了一个完整的 Transformer 模型！</p>
<h2 id="单元格-50-54推理测试和简单示例">单元格 50-54：推理测试和简单示例</h2>
<h3 id="是什么-模型推理能力的验证器"><strong>是什么：</strong> 模型推理能力的验证器</h3>
<p>这个函数展示了如何使用未训练的模型进行推理。虽然模型还没有训练，但可以看到推理的完整流程。</p>
<p><strong>推理过程的本质：</strong></p>
<ul>
<li><strong>前向传播验证器</strong>：确保模型架构正确无误</li>
<li><strong>数据流测试器</strong>：验证输入输出的形状和类型匹配</li>
<li><strong>接口展示器</strong>：演示模型的使用方法</li>
</ul>
<h3 id="为什么需要推理测试-验证架构完整性"><strong>为什么需要推理测试：</strong> 验证架构完整性</h3>
<p><strong>未训练模型的测试价值：</strong></p>
<ol>
<li><strong>架构验证</strong>：确保所有组件正确连接</li>
<li><strong>维度检查</strong>：验证数据在各层间的形状变换</li>
<li><strong>接口测试</strong>：确认模型的输入输出接口设计合理</li>
<li><strong>调试基础</strong>：为后续训练提供基础的错误检查</li>
</ol>
<p><strong>早期测试的重要性：</strong></p>
<ul>
<li><strong>快速发现问题</strong>：在投入大量训练资源前发现架构问题</li>
<li><strong>节省时间</strong>：避免长时间训练后才发现基础错误</li>
<li><strong>建立信心</strong>：确认模型基本功能正常</li>
</ul>
<h3 id="怎么做-推理测试的实现策略"><strong>怎么做：</strong> 推理测试的实现策略</h3>
<p>虽然模型未训练，但输出应该具有合理的形状和数值特征。这为后续的完整训练和评估奠定了基础。</p>
<hr>
<h2 id="单元格-55-58训练准备---批次和掩码">单元格 55-58：训练准备 - 批次和掩码</h2>
<h3 id="是什么-训练数据的智能包装器"><strong>是什么：</strong> 训练数据的智能包装器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> Batch</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#9ECBFF">    """Object for holding a batch of data with mask during training."""</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, src, tgt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, pad</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># 2 = &#x3C;blank></span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> src</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.src_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (src </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).unsqueeze(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> tgt </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.tgt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt[:, :</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.tgt_y </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt[:, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">:]</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.tgt_mask </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.make_std_mask(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt, pad)</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.ntokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt_y </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).data.sum()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">staticmethod</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> make_std_mask</span><span style="color:#E1E4E8">(tgt, pad):</span></span>
<span class="line"><span style="color:#9ECBFF">        "Create a mask to hide padding and future words."</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (tgt </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).unsqueeze(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt_mask </span><span style="color:#F97583">&#x26;</span><span style="color:#E1E4E8"> subsequent_mask(tgt.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)).type_as(</span></span>
<span class="line"><span style="color:#E1E4E8">            tgt_mask.data</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> tgt_mask</span></span></code></pre>
<p><strong>Batch类的核心使命：</strong></p>
<ul>
<li><strong>数据组织器</strong>：将原始序列数据转换为模型可处理的批次格式</li>
<li><strong>掩码生成器</strong>：为注意力机制创建必要的掩码矩阵</li>
<li><strong>训练适配器</strong>：为teacher forcing训练模式准备输入输出对</li>
</ul>
<h3 id="为什么需要batch类-解决训练中的实际问题"><strong>为什么需要Batch类：</strong> 解决训练中的实际问题</h3>
<p><strong>批次处理的必要性：</strong></p>
<ol>
<li><strong>计算效率</strong>：GPU并行处理多个样本比逐一处理快得多</li>
<li><strong>内存优化</strong>：批次处理充分利用GPU的并行计算能力</li>
<li><strong>梯度稳定</strong>：批次内梯度的平均化提高训练稳定性</li>
<li><strong>序列对齐</strong>：处理不等长序列需要填充和掩码机制</li>
</ol>
<p><strong>掩码的关键作用：</strong></p>
<ol>
<li><strong>填充掩码</strong>：区分真实内容和填充内容</li>
<li><strong>因果掩码</strong>：防止解码器看到未来信息</li>
<li><strong>注意力控制</strong>：精确控制模型的注意力分布</li>
</ol>
<h3 id="怎么做-精密的数据预处理流程"><strong>怎么做：</strong> 精密的数据预处理流程</h3>
<p><strong>源序列处理的细节：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> src  </span><span style="color:#6A737D"># 源序列，如英语句子</span></span>
<span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.src_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (src </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).unsqueeze(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 创建填充掩码</span></span></code></pre>
<p><strong>掩码创建的数学逻辑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设源序列：[1, 15, 234, 5, 2, 2, 2]  # 2是填充符</span></span>
<span class="line"><span style="color:#6A737D"># 掩码结果：  [T,  T,   T, T, F, F, F]  # T=True(真实), F=False(填充)</span></span></code></pre>
<p><strong>unsqueeze(-2)的作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 原始掩码形状：[batch_size, seq_len]</span></span>
<span class="line"><span style="color:#6A737D"># 处理后形状：  [batch_size, 1, seq_len]</span></span>
<span class="line"><span style="color:#6A737D"># 目的：为多头注意力的广播机制做准备</span></span></code></pre>
<p><strong>Teacher Forcing的巧妙实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> tgt </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.tgt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt[:, :</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]    </span><span style="color:#6A737D"># 解码器输入：去掉最后一个词</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.tgt_y </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt[:, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">:]   </span><span style="color:#6A737D"># 期望输出：去掉第一个词</span></span></code></pre>
<p><strong>具体示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 原始目标序列：[&#x3C;start>, I, am, happy, &#x3C;end>]</span></span>
<span class="line"><span style="color:#6A737D"># 解码器输入：  [&#x3C;start>, I, am, happy]        (self.tgt)</span></span>
<span class="line"><span style="color:#6A737D"># 期望输出：    [I, am, happy, &#x3C;end>]          (self.tgt_y)</span></span></code></pre>
<p><strong>这种设计的深层原理：</strong></p>
<ol>
<li><strong>自回归性质</strong>：每个位置的输出只依赖前面的输入</li>
<li><strong>并行训练</strong>：可以同时计算所有位置的损失</li>
<li><strong>稳定性</strong>：避免了推理时的错误累积</li>
</ol>
<p><strong>目标掩码的双重保护：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#B392F0">@</span><span style="color:#79B8FF">staticmethod</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> make_std_mask</span><span style="color:#E1E4E8">(tgt, pad):</span></span>
<span class="line"><span style="color:#E1E4E8">    tgt_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (tgt </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).unsqueeze(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 填充掩码</span></span>
<span class="line"><span style="color:#E1E4E8">    tgt_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tgt_mask </span><span style="color:#F97583">&#x26;</span><span style="color:#E1E4E8"> subsequent_mask(tgt.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)).type_as(tgt_mask.data)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> tgt_mask</span></span></code></pre>
<p><strong>双重掩码的逻辑运算：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 填充掩码：   [T, T, T, F, F]</span></span>
<span class="line"><span style="color:#6A737D"># 因果掩码：   下三角矩阵</span></span>
<span class="line"><span style="color:#6A737D"># 最终掩码：   填充掩码 AND 因果掩码</span></span></code></pre>
<p><strong>静态方法的设计考虑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#B392F0">@</span><span style="color:#79B8FF">staticmethod</span><span style="color:#6A737D">  # 不依赖实例状态的纯函数</span></span></code></pre>
<ul>
<li><strong>功能独立</strong>：可以独立测试和使用</li>
<li><strong>内存效率</strong>：不需要访问实例变量</li>
<li><strong>设计清晰</strong>：明确表示这是一个工具函数</li>
</ul>
<p><strong>词元统计的实用价值：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.ntokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.tgt_y </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad).data.sum()</span></span></code></pre>
<ul>
<li><strong>训练监控</strong>：跟踪处理的有效词元数量</li>
<li><strong>性能评估</strong>：计算每秒处理的词元数</li>
<li><strong>损失归一化</strong>：按有效词元数归一化损失</li>
</ul>
<p>这种精心设计的批次处理机制，确保了训练过程的高效性和正确性，是深度学习训练管道中的关键环节。</p>
<hr>
<h2 id="单元格-59-61训练循环核心---模型学习的引擎">单元格 59-61：训练循环核心 - 模型学习的引擎</h2>
<h3 id="是什么-深度学习训练的执行引擎"><strong>是什么：</strong> 深度学习训练的执行引擎</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> TrainState</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#9ECBFF">    """Track number of steps, examples, and tokens processed"""</span></span>
<span class="line"><span style="color:#E1E4E8">    step: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # Steps in the current epoch</span></span>
<span class="line"><span style="color:#E1E4E8">    accum_step: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # Number of gradient accumulation steps</span></span>
<span class="line"><span style="color:#E1E4E8">    samples: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # total # of examples used</span></span>
<span class="line"><span style="color:#E1E4E8">    tokens: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # total # of tokens processed</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> run_epoch</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#E1E4E8">    data_iter, model, loss_compute, optimizer, scheduler,</span></span>
<span class="line"><span style="color:#E1E4E8">    mode</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"train"</span><span style="color:#E1E4E8">, accum_iter</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, train_state</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">TrainState(),</span></span>
<span class="line"><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    """Train a single epoch"""</span></span>
<span class="line"><span style="color:#E1E4E8">    start </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time()</span></span>
<span class="line"><span style="color:#E1E4E8">    total_tokens </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#E1E4E8">    total_loss </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#E1E4E8">    tokens </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#E1E4E8">    n_accum </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> i, batch </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> enumerate</span><span style="color:#E1E4E8">(data_iter):</span></span>
<span class="line"><span style="color:#E1E4E8">        out </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">        loss, loss_node </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> loss_compute(out, batch.tgt_y, batch.ntokens)</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train"</span><span style="color:#F97583"> or</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train+log"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            loss_node.backward()</span></span>
<span class="line"><span style="color:#E1E4E8">            train_state.step </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#E1E4E8">            train_state.samples </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.src.shape[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">            train_state.tokens </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.ntokens</span></span>
<span class="line"><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> accum_iter </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">                optimizer.step()</span></span>
<span class="line"><span style="color:#E1E4E8">                optimizer.zero_grad(</span><span style="color:#FFAB70">set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">                n_accum </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#E1E4E8">                train_state.accum_step </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#E1E4E8">            scheduler.step()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">        total_loss </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> loss</span></span>
<span class="line"><span style="color:#E1E4E8">        total_tokens </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.ntokens</span></span>
<span class="line"><span style="color:#E1E4E8">        tokens </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.ntokens</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#79B8FF"> 40</span><span style="color:#F97583"> ==</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> and</span><span style="color:#E1E4E8"> (mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train"</span><span style="color:#F97583"> or</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train+log"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">            lr </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> optimizer.param_groups[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">"lr"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">            elapsed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time() </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> start</span></span>
<span class="line"><span style="color:#79B8FF">            print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Epoch Step: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">i</span><span style="color:#F97583">:6d</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | Accumulation Step: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">n_accum</span><span style="color:#F97583">:3d</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">                  f</span><span style="color:#9ECBFF">"Loss: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">loss </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> batch.ntokens</span><span style="color:#F97583">:6.2f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">                  f</span><span style="color:#9ECBFF">"Tokens / Sec: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">tokens </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> elapsed</span><span style="color:#F97583">:7.1f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">                  f</span><span style="color:#9ECBFF">"Learning Rate: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">lr</span><span style="color:#F97583">:6.1e</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">            start </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time()</span></span>
<span class="line"><span style="color:#E1E4E8">            tokens </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> loss</span></span>
<span class="line"><span style="color:#F97583">        del</span><span style="color:#E1E4E8"> loss_node</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> total_loss </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> total_tokens, train_state</span></span></code></pre>
<p><strong>训练引擎的核心职责：</strong></p>
<ul>
<li><strong>状态跟踪器</strong>：监控训练进度和统计信息</li>
<li><strong>梯度协调器</strong>：管理前向传播、反向传播和参数更新</li>
<li><strong>性能监视器</strong>：实时展示训练效率和学习进度</li>
</ul>
<h3 id="为什么需要训练循环-深度学习的核心机制"><strong>为什么需要训练循环：</strong> 深度学习的核心机制</h3>
<p><strong>训练循环的根本意义：</strong></p>
<ol>
<li><strong>参数优化</strong>：通过梯度下降不断调整模型参数</li>
<li><strong>知识积累</strong>：每个批次都为模型提供学习机会</li>
<li><strong>收敛控制</strong>：监控和引导模型向最优解收敛</li>
<li><strong>资源管理</strong>：高效利用计算资源和内存</li>
</ol>
<p><strong>TrainState的设计哲学：</strong></p>
<ol>
<li><strong>全局视角</strong>：跟踪整个训练过程的宏观统计</li>
<li><strong>细粒度监控</strong>：从步骤、样本到词元的多层次计数</li>
<li><strong>性能评估</strong>：为训练效率分析提供数据支撑</li>
<li><strong>状态恢复</strong>：为训练中断恢复提供状态快照</li>
</ol>
<h3 id="怎么做-训练循环的精密编排"><strong>怎么做：</strong> 训练循环的精密编排</h3>
<p><strong>TrainState的状态管理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">step: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">          # 当前epoch内的训练步数</span></span>
<span class="line"><span style="color:#E1E4E8">accum_step: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">    # 梯度累积步数（实际参数更新次数）</span></span>
<span class="line"><span style="color:#E1E4E8">samples: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">       # 处理的样本总数</span></span>
<span class="line"><span style="color:#E1E4E8">tokens: </span><span style="color:#79B8FF">int</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">        # 处理的词元总数</span></span></code></pre>
<p><strong>状态字段的具体含义：</strong></p>
<ul>
<li><strong>step vs accum_step</strong>：区分批次处理和参数更新频率</li>
<li><strong>samples</strong>：用于计算平均每个样本的处理时间</li>
<li><strong>tokens</strong>：更精确的处理量度量，因为序列长度不同</li>
</ul>
<p><strong>run_epoch的核心循环：</strong></p>
<p><strong>阶段1：前向传播</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">out </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)</span></span></code></pre>
<ul>
<li>输入源序列和目标序列（带掩码）</li>
<li>获得模型对每个位置的词汇概率预测</li>
</ul>
<p><strong>阶段2：损失计算</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">loss, loss_node </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> loss_compute(out, batch.tgt_y, batch.ntokens)</span></span></code></pre>
<ul>
<li><code>loss</code>：标量损失值，用于统计</li>
<li><code>loss_node</code>：保留计算图的张量，用于反向传播</li>
<li><code>batch.tgt_y</code>：真实的目标标签</li>
<li><code>batch.ntokens</code>：用于损失归一化</li>
</ul>
<p><strong>阶段3：反向传播与优化</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train"</span><span style="color:#F97583"> or</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train+log"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    loss_node.backward()                    </span><span style="color:#6A737D"># 计算梯度</span></span>
<span class="line"><span style="color:#E1E4E8">    train_state.step </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span><span style="color:#6A737D">                   # 更新步数</span></span>
<span class="line"><span style="color:#E1E4E8">    train_state.samples </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.src.shape[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># 累积样本数</span></span>
<span class="line"><span style="color:#E1E4E8">    train_state.tokens </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> batch.ntokens     </span><span style="color:#6A737D"># 累积词元数</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> accum_iter </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:                 </span><span style="color:#6A737D"># 梯度累积条件</span></span>
<span class="line"><span style="color:#E1E4E8">        optimizer.step()                    </span><span style="color:#6A737D"># 应用梯度更新</span></span>
<span class="line"><span style="color:#E1E4E8">        optimizer.zero_grad(</span><span style="color:#FFAB70">set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 清空梯度</span></span>
<span class="line"><span style="color:#E1E4E8">        n_accum </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span><span style="color:#6A737D">                        # 累积步数递增</span></span>
<span class="line"><span style="color:#E1E4E8">        train_state.accum_step </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    scheduler.step()                        </span><span style="color:#6A737D"># 学习率调度</span></span></code></pre>
<p><strong>梯度累积的巧妙机制：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> accum_iter </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer.step()</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer.zero_grad(</span><span style="color:#FFAB70">set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>梯度累积的作用原理：</strong></p>
<ol>
<li><strong>有效批次大小扩大</strong>：<code>effective_batch_size = batch_size * accum_iter</code></li>
<li><strong>内存限制应对</strong>：在GPU内存不足时模拟大批次训练</li>
<li><strong>梯度稳定性</strong>：更大的有效批次提供更稳定的梯度估计</li>
<li><strong>训练等价性</strong>：与真实大批次训练数学上等价</li>
</ol>
<p><strong>set_to_none=True的优化：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">optimizer.zero_grad(</span><span style="color:#FFAB70">set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li><strong>内存效率</strong>：释放梯度张量的内存，而不是置零</li>
<li><strong>性能提升</strong>：避免不必要的内存操作</li>
<li><strong>现代最佳实践</strong>：PyTorch 1.7+推荐的梯度清理方式</li>
</ul>
<p><strong>学习率调度的时机：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">scheduler.step()  </span><span style="color:#6A737D"># 每个批次后调用，而非每次参数更新后</span></span></code></pre>
<ul>
<li><strong>精细控制</strong>：提供更细粒度的学习率调整</li>
<li><strong>warmup支持</strong>：支持复杂的学习率预热策略</li>
<li><strong>收敛优化</strong>：根据训练进度动态调整学习率</li>
</ul>
<p><strong>阶段4：实时监控与报告</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#79B8FF"> 40</span><span style="color:#F97583"> ==</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> and</span><span style="color:#E1E4E8"> (mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train"</span><span style="color:#F97583"> or</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train+log"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">    lr </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> optimizer.param_groups[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">"lr"</span><span style="color:#E1E4E8">]    </span><span style="color:#6A737D"># 获取当前学习率</span></span>
<span class="line"><span style="color:#E1E4E8">    elapsed </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time() </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> start           </span><span style="color:#6A737D"># 计算时间间隔</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Epoch Step: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">i</span><span style="color:#F97583">:6d</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | Accumulation Step: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">n_accum</span><span style="color:#F97583">:3d</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">          f</span><span style="color:#9ECBFF">"Loss: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">loss </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> batch.ntokens</span><span style="color:#F97583">:6.2f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">          f</span><span style="color:#9ECBFF">"Tokens / Sec: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">tokens </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> elapsed</span><span style="color:#F97583">:7.1f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> | "</span></span>
<span class="line"><span style="color:#F97583">          f</span><span style="color:#9ECBFF">"Learning Rate: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">lr</span><span style="color:#F97583">:6.1e</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    start </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> time.time()                     </span><span style="color:#6A737D"># 重置计时器</span></span>
<span class="line"><span style="color:#E1E4E8">    tokens </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">                              # 重置词元计数</span></span></code></pre>
<p><strong>监控报告的设计考虑：</strong></p>
<ol>
<li><strong>适度频率</strong>：每40步报告一次，平衡信息量和性能</li>
<li><strong>关键指标</strong>：损失、吞吐量、学习率三大核心指标</li>
<li><strong>归一化损失</strong>：按词元数归一化，提供可比较的损失值</li>
<li><strong>吞吐量监控</strong>：Tokens/Sec是衡量训练效率的重要指标</li>
</ol>
<p><strong>内存管理的细节：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> loss</span></span>
<span class="line"><span style="color:#F97583">del</span><span style="color:#E1E4E8"> loss_node</span></span></code></pre>
<ul>
<li><strong>显式删除</strong>：及时释放不再需要的张量</li>
<li><strong>内存优化</strong>：防止循环中的内存累积</li>
<li><strong>PyTorch最佳实践</strong>：在长循环中及时清理计算图</li>
</ul>
<p><strong>返回值的设计：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> total_loss </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> total_tokens, train_state</span></span></code></pre>
<ul>
<li><strong>平均损失</strong>：按词元数归一化的epoch平均损失</li>
<li><strong>状态传递</strong>：更新后的训练状态，支持多epoch训练</li>
</ul>
<p><strong>实际使用场景：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 训练模式</span></span>
<span class="line"><span style="color:#E1E4E8">train_loss, train_state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> run_epoch(</span></span>
<span class="line"><span style="color:#E1E4E8">    train_dataloader, model, loss_compute, </span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer, scheduler, </span><span style="color:#FFAB70">mode</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"train"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">accum_iter</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 验证模式</span></span>
<span class="line"><span style="color:#E1E4E8">val_loss, _ </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> run_epoch(</span></span>
<span class="line"><span style="color:#E1E4E8">    val_dataloader, model, loss_compute,</span></span>
<span class="line"><span style="color:#79B8FF">    None</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">mode</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"eval"</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span></code></pre>
<p>这种精心设计的训练循环，提供了<strong>高效、稳定、可监控</strong>的模型训练框架，是深度学习项目成功的关键基础设施。</p>
<p><strong>训练循环详解：</strong></p>
<p><strong>1. 前向传播：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">out </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.forward(</span></span>
<span class="line"><span style="color:#E1E4E8">    batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li>将批次数据传入模型</li>
<li>获得模型的预测输出</li>
</ul>
<p><strong>2. 损失计算：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">loss, loss_node </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> loss_compute(out, batch.tgt_y, batch.ntokens)</span></span></code></pre>
<ul>
<li>比较模型预测和真实标签</li>
<li>返回损失值和可计算梯度的损失节点</li>
</ul>
<p><strong>3. 反向传播（仅在训练模式）：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train"</span><span style="color:#F97583"> or</span><span style="color:#E1E4E8"> mode </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "train+log"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    loss_node.backward()</span></span></code></pre>
<ul>
<li><code>backward()</code> 计算梯度</li>
<li>梯度存储在模型参数的 <code>.grad</code> 属性中</li>
</ul>
<p><strong>4. 梯度累积：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> accum_iter </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer.step()</span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer.zero_grad(</span><span style="color:#FFAB70">set_to_none</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span></code></pre>
<hr>
<h2 id="单元格-62-65学习率调度---训练的节奏掌控者">单元格 62-65：学习率调度 - 训练的节奏掌控者</h2>
<h3 id="是什么-动态学习率控制策略"><strong>是什么：</strong> 动态学习率控制策略</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> rate</span><span style="color:#E1E4E8">(step, model_size, factor, warmup):</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"><span style="color:#9ECBFF">    we have to default the step to 1 for LambdaLR function</span></span>
<span class="line"><span style="color:#9ECBFF">    to avoid zero raising to negative power.</span></span>
<span class="line"><span style="color:#9ECBFF">    """</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        step </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#E1E4E8">        model_size </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> min</span><span style="color:#E1E4E8">(step </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">), step </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> warmup </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span></code></pre>
<p><strong>学习率调度器的核心使命：</strong></p>
<ul>
<li><strong>训练节奏掌控者</strong>：在训练过程中动态调整学习步长</li>
<li><strong>收敛优化器</strong>：平衡探索能力和收敛稳定性</li>
<li><strong>性能提升器</strong>：通过精心设计的策略提升最终性能</li>
</ul>
<h3 id="为什么需要学习率调度-优化过程的智慧"><strong>为什么需要学习率调度：</strong> 优化过程的智慧</h3>
<p><strong>固定学习率的局限性：</strong></p>
<ol>
<li><strong>探索vs利用矛盾</strong>：高学习率利于探索，低学习率利于收敛</li>
<li><strong>训练阶段差异</strong>：早期需要大步探索，后期需要细致调优</li>
<li><strong>损失景观复杂性</strong>：不同阶段的损失函数特性不同</li>
<li><strong>泛化性能优化</strong>：动态调整有助于找到更好的泛化解</li>
</ol>
<p><strong>Transformer特有的挑战：</strong></p>
<ol>
<li><strong>参数规模庞大</strong>：需要更谨慎的优化策略</li>
<li><strong>梯度传播复杂</strong>：深层网络需要稳定的初始训练</li>
<li><strong>注意力机制敏感性</strong>：对学习率变化较为敏感</li>
<li><strong>收敛困难</strong>：大型模型容易陷入局部最优</li>
</ol>
<h3 id="怎么做-transformer的学习率艺术"><strong>怎么做：</strong> Transformer的学习率艺术</h3>
<p><strong>核心公式深度解析：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>lr = factor * d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))</span></span></code></pre>
<p><strong>公式的三个关键组成部分：</strong></p>
<p><strong>1. 基础缩放因子：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> model_size </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li><strong>factor</strong>：全局缩放参数，通常设为1.0或2.0</li>
<li><strong>d_model^(-0.5)</strong>：根据模型维度自适应缩放</li>
<li><strong>数学直觉</strong>：更大的模型需要更小的学习率</li>
</ul>
<p><strong>为什么使用d_model^(-0.5)：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 对于d_model=512: 1/√512 ≈ 0.044</span></span>
<span class="line"><span style="color:#6A737D"># 对于d_model=1024: 1/√1024 ≈ 0.031</span></span>
<span class="line"><span style="color:#6A737D"># 模型越大，基础学习率越小</span></span></code></pre>
<p><strong>2. Warmup阶段（step &#x3C; warmup）：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">step </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> warmup </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>Warmup的数学特性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 当step &#x3C; warmup时：</span></span>
<span class="line"><span style="color:#E1E4E8">lr </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> warmup</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">   =</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (step </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> warmup</span><span style="color:#F97583">^</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 这实际上是线性增长，直到warmup步数</span></span></code></pre>
<p><strong>Warmup阶段的渐进过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设warmup=4000</span></span>
<span class="line"><span style="color:#6A737D"># step=1000: lr ∝ 1000/4000^1.5 = 很小的值</span></span>
<span class="line"><span style="color:#6A737D"># step=2000: lr ∝ 2000/4000^1.5 = 稍大的值  </span></span>
<span class="line"><span style="color:#6A737D"># step=4000: lr ∝ 4000/4000^1.5 = 达到峰值</span></span></code></pre>
<p><strong>3. 衰减阶段（step >= warmup）：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">step </span><span style="color:#F97583">**</span><span style="color:#E1E4E8"> (</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>衰减的数学特性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 当step >= warmup时：</span></span>
<span class="line"><span style="color:#E1E4E8">lr </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> step</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">   =</span><span style="color:#E1E4E8"> factor </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> d_model</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> √step</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 这是平方根衰减，比指数衰减更温和</span></span></code></pre>
<p><strong>两阶段的平滑过渡：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 在warmup点，两个表达式相等：</span></span>
<span class="line"><span style="color:#E1E4E8">step</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.5</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> warmup</span><span style="color:#F97583">^</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.5</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># 解得：step = warmup（过渡点）</span></span></code></pre>
<p><strong>特殊处理的边界情况：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> step </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    step </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span></span></code></pre>
<ul>
<li><strong>数学稳定性</strong>：避免0的负数次幂导致的数值问题</li>
<li><strong>实现细节</strong>：LambdaLR要求学习率函数在step=0时有定义</li>
<li><strong>逻辑合理性</strong>：第一步应该有一个合理的小学习率</li>
</ul>
<p><strong>可视化理解学习率曲线：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Warmup阶段：线性上升</span></span>
<span class="line"><span style="color:#6A737D"># 峰值点：在warmup步数达到最大值</span></span>
<span class="line"><span style="color:#6A737D"># 衰减阶段：平方根衰减，缓慢下降</span></span></code></pre>
<p><strong>不同参数的影响：</strong></p>
<p><strong>factor参数的作用：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">factor </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1.0</span><span style="color:#6A737D">  # 标准设置</span></span>
<span class="line"><span style="color:#E1E4E8">factor </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 2.0</span><span style="color:#6A737D">  # 更激进的学习率</span></span>
<span class="line"><span style="color:#E1E4E8">factor </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.5</span><span style="color:#6A737D">  # 更保守的学习率</span></span></code></pre>
<p><strong>warmup参数的影响：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">warmup </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 4000</span><span style="color:#6A737D">   # 标准设置，适中的预热期</span></span>
<span class="line"><span style="color:#E1E4E8">warmup </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 8000</span><span style="color:#6A737D">   # 更长的预热期，更稳定</span></span>
<span class="line"><span style="color:#E1E4E8">warmup </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 2000</span><span style="color:#6A737D">   # 更短的预热期，更快进入衰减</span></span></code></pre>
<p><strong>model_size的自适应性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">d_model </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 512</span><span style="color:#6A737D">   # Base模型</span></span>
<span class="line"><span style="color:#E1E4E8">d_model </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1024</span><span style="color:#6A737D">  # Large模型，自动使用更小的学习率</span></span></code></pre>
<p><strong>实际应用中的优势：</strong></p>
<p><strong>1. 训练稳定性：</strong></p>
<ul>
<li>避免初期的梯度爆炸</li>
<li>为复杂的注意力机制提供平滑的优化路径</li>
</ul>
<p><strong>2. 收敛质量：</strong></p>
<ul>
<li>Warmup确保良好的初始化</li>
<li>平方根衰减保持持续的学习能力</li>
</ul>
<p><strong>3. 实验可重复性：</strong></p>
<ul>
<li>数学公式明确，减少超参数调优的主观性</li>
<li>在不同规模的模型间具有良好的迁移性</li>
</ul>
<p><strong>与其他调度策略的比较：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 指数衰减：lr *= gamma^step （衰减太快）</span></span>
<span class="line"><span style="color:#6A737D"># 线性衰减：lr -= alpha*step （缺乏理论基础）</span></span>
<span class="line"><span style="color:#6A737D"># 余弦衰减：lr = base * cos(π*step/max_steps) （缺乏warmup）</span></span>
<span class="line"><span style="color:#6A737D"># Transformer调度：兼具warmup和适度衰减的优势</span></span></code></pre>
<hr>
<h2 id="单元格-66-70标签平滑技术---对抗过拟合的智慧">单元格 66-70：标签平滑技术 - 对抗过拟合的智慧</h2>
<h3 id="是什么-概率分布的平滑化器"><strong>是什么：</strong> 概率分布的平滑化器</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> LabelSmoothing</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">nn</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Module</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Implement label smoothing."</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, padding_idx, smoothing</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">        super</span><span style="color:#E1E4E8">(LabelSmoothing, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">).</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.criterion </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.KLDivLoss(</span><span style="color:#FFAB70">reduction</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"sum"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.padding_idx </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> padding_idx</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.confidence </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1.0</span><span style="color:#F97583"> -</span><span style="color:#E1E4E8"> smoothing</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.smoothing </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> smoothing</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.true_dist </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> None</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> forward</span><span style="color:#E1E4E8">(self, x, target):</span></span>
<span class="line"><span style="color:#F97583">        assert</span><span style="color:#E1E4E8"> x.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.size</span></span>
<span class="line"><span style="color:#E1E4E8">        true_dist </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.data.clone()</span></span>
<span class="line"><span style="color:#E1E4E8">        true_dist.fill_(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.smoothing </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">        true_dist.scatter_(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, target.data.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">), </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.confidence)</span></span>
<span class="line"><span style="color:#E1E4E8">        true_dist[:, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.padding_idx] </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#E1E4E8">        mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.nonzero(target.data </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.padding_idx)</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> mask.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            true_dist.index_fill_(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, mask.squeeze(), </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.true_dist </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> true_dist</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.criterion(x, true_dist.clone().detach())</span></span></code></pre>
<p><strong>标签平滑的核心使命：</strong></p>
<ul>
<li><strong>概率软化器</strong>：将硬性的one-hot标签转换为平滑的概率分布</li>
<li><strong>过拟合抑制器</strong>：防止模型过度自信于训练数据</li>
<li><strong>泛化增强器</strong>：提高模型在未见数据上的表现</li>
</ul>
<h3 id="为什么需要标签平滑-对抗过度自信的陷阱"><strong>为什么需要标签平滑：</strong> 对抗过度自信的陷阱</h3>
<p><strong>传统硬标签的问题：</strong></p>
<ol>
<li><strong>过度自信</strong>：模型被训练为对正确答案给出100%的置信度</li>
<li><strong>泛化差</strong>：在训练数据上表现很好，但在新数据上表现差</li>
<li><strong>脆弱性</strong>：对输入的小变化过于敏感</li>
<li><strong>校准问题</strong>：预测概率与实际准确率不匹配</li>
</ol>
<p><strong>硬标签的数学表示：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 传统one-hot编码，假设正确答案是词汇3</span></span>
<span class="line"><span style="color:#E1E4E8">hard_label </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># 只有位置3是1，其他都是0</span></span></code></pre>
<p><strong>平滑标签的改进思路：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 标签平滑后，假设smoothing=0.1</span></span>
<span class="line"><span style="color:#E1E4E8">smooth_label </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.86</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.02</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]  </span></span>
<span class="line"><span style="color:#6A737D"># 正确答案仍然概率最高，但其他位置也有小概率</span></span></code></pre>
<p><strong>深层原理解析：</strong></p>
<ol>
<li><strong>不确定性建模</strong>：承认训练标签可能不是唯一正确答案</li>
<li><strong>正则化效应</strong>：阻止模型在训练集上过度拟合</li>
<li><strong>概率校准</strong>：使预测概率更接近真实的置信度</li>
<li><strong>鲁棒性提升</strong>：提高对噪声和变化的抗干扰能力</li>
</ol>
<h3 id="怎么做-标签平滑的精确实现"><strong>怎么做：</strong> 标签平滑的精确实现</h3>
<p><strong>初始化参数的深度含义：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, size, padding_idx, smoothing</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.criterion </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> nn.KLDivLoss(</span><span style="color:#FFAB70">reduction</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"sum"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># KL散度损失</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.padding_idx </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> padding_idx                   </span><span style="color:#6A737D"># 填充符索引</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.confidence </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1.0</span><span style="color:#F97583"> -</span><span style="color:#E1E4E8"> smoothing               </span><span style="color:#6A737D"># 正确答案的概率</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.smoothing </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> smoothing                      </span><span style="color:#6A737D"># 平滑参数</span></span>
<span class="line"><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> size                                </span><span style="color:#6A737D"># 词汇表大小</span></span></code></pre>
<p><strong>KL散度损失的选择：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">nn.KLDivLoss(</span><span style="color:#FFAB70">reduction</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"sum"</span><span style="color:#E1E4E8">)</span></span></code></pre>
<ul>
<li><strong>KL散度定义</strong>：D_KL(P||Q) = Σ P(x) * log(P(x)/Q(x))</li>
<li><strong>适用性</strong>：自然地处理概率分布间的差异</li>
<li><strong>数学性质</strong>：非对称，惩罚模型与目标分布的偏差</li>
</ul>
<p><strong>平滑分布的构建过程：</strong></p>
<p><strong>步骤1：创建模板分布</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">true_dist </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.data.clone()  </span><span style="color:#6A737D"># 复制输入张量的形状</span></span></code></pre>
<ul>
<li>获得与模型输出相同的形状 [batch_size, vocab_size]</li>
<li>为每个样本创建概率分布模板</li>
</ul>
<p><strong>步骤2：填充均匀基础概率</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">true_dist.fill_(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.smoothing </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.size </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">))</span></span></code></pre>
<p><strong>为什么除以(size-2)而不是size：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># size - 2 的原因：</span></span>
<span class="line"><span style="color:#6A737D"># 1. 排除正确答案位置（它会被单独设置）</span></span>
<span class="line"><span style="color:#6A737D"># 2. 排除padding位置（它会被设为0）</span></span>
<span class="line"><span style="color:#6A737D"># 实际参与平滑分配的词汇数 = total_vocab - 1 - 1 = size - 2</span></span></code></pre>
<p><strong>数学推导：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 设词汇表大小为V，平滑参数为ε</span></span>
<span class="line"><span style="color:#6A737D"># 正确答案概率：1 - ε</span></span>
<span class="line"><span style="color:#6A737D"># 错误答案总概率：ε</span></span>
<span class="line"><span style="color:#6A737D"># 每个错误答案概率：ε / (V - 2)</span></span></code></pre>
<p><strong>步骤3：设置正确答案的高概率</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">true_dist.scatter_(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, target.data.unsqueeze(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">), </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.confidence)</span></span></code></pre>
<p><strong>scatter_操作详解：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># scatter_(dim, index, src)</span></span>
<span class="line"><span style="color:#6A737D"># dim=1: 在词汇维度上操作</span></span>
<span class="line"><span style="color:#6A737D"># index: 正确答案的位置索引</span></span>
<span class="line"><span style="color:#6A737D"># src: 要填入的值(confidence = 1.0 - smoothing)</span></span></code></pre>
<p><strong>具体示例：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 假设target=[3], confidence=0.9, smoothing=0.1, vocab_size=6</span></span>
<span class="line"><span style="color:#6A737D"># 初始: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025]  # (除了padding)</span></span>
<span class="line"><span style="color:#6A737D"># scatter后: [0.025, 0.025, 0.025, 0.9, 0.025, 0.025]</span></span></code></pre>
<p><strong>步骤4：处理填充位置</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">true_dist[:, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.padding_idx] </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # 填充符概率设为0</span></span></code></pre>
<p><strong>填充位置的特殊处理：</strong></p>
<ul>
<li>填充符不是真实词汇，不应参与概率分布</li>
<li>设为0确保这些位置不影响损失计算</li>
</ul>
<p><strong>步骤5：处理填充样本</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.nonzero(target.data </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.padding_idx)</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> mask.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    true_dist.index_fill_(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, mask.squeeze(), </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>边界情况处理：</strong></p>
<ul>
<li>当目标本身就是填充符时，整行概率都设为0</li>
<li>这种情况在序列末尾的填充位置会出现</li>
</ul>
<p><strong>概率分布的数学验证：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 对于非填充位置，概率分布应该和为1：</span></span>
<span class="line"><span style="color:#6A737D"># P(correct) + Σ P(incorrect) = confidence + (size-2) * smoothing/(size-2)</span></span>
<span class="line"><span style="color:#6A737D">#                              = (1-smoothing) + smoothing = 1 ✓</span></span></code></pre>
<p><strong>实际效果对比：</strong></p>
<p><strong>传统硬标签：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 目标: "I love you"，词汇表: ["I", "love", "you", "hate", "like"]</span></span>
<span class="line"><span style="color:#E1E4E8">hard_labels </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "I"</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "love"  </span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "you"</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span></code></pre>
<p><strong>标签平滑后：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># smoothing = 0.1</span></span>
<span class="line"><span style="color:#E1E4E8">smooth_labels </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "I" 更多选择可能性</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "love" </span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.025</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># "you"</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span></code></pre>
<p><strong>性能提升的量化分析：</strong></p>
<ol>
<li><strong>训练损失</strong>：稍微增加（因为目标概率降低）</li>
<li><strong>验证损失</strong>：通常降低（因为泛化能力提升）</li>
<li><strong>BLEU分数</strong>：在机器翻译任务上通常提升1-2分</li>
<li><strong>概率校准</strong>：预测概率与实际准确率更匹配</li>
</ol>
<p><strong>超参数选择指导：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 常用smoothing值：</span></span>
<span class="line"><span style="color:#E1E4E8">smoothing </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.0</span><span style="color:#6A737D">   # 无平滑（传统训练）</span></span>
<span class="line"><span style="color:#E1E4E8">smoothing </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.1</span><span style="color:#6A737D">   # 轻度平滑（推荐起点）</span></span>
<span class="line"><span style="color:#E1E4E8">smoothing </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.2</span><span style="color:#6A737D">   # 中度平滑（某些任务有效）</span></span>
<span class="line"><span style="color:#E1E4E8">smoothing </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.3</span><span style="color:#6A737D">   # 重度平滑（可能过度）</span></span></code></pre>
<p><strong>平滑参数的影响分析：</strong></p>
<ul>
<li><strong>过小（&#x3C;0.05）</strong>：效果有限，接近硬标签</li>
<li><strong>适中（0.1-0.2）</strong>：平衡性能和泛化，多数情况最优</li>
<li><strong>过大（>0.3）</strong>：可能损害学习效率，正确答案置信度过低</li>
</ul>
<hr>
<h2 id="单元格-71-77简单复制任务示例---模型能力的初步验证">单元格 71-77：简单复制任务示例 - 模型能力的初步验证</h2>
<h3 id="是什么-模型学习能力的基础测试"><strong>是什么：</strong> 模型学习能力的基础测试</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> data_gen</span><span style="color:#E1E4E8">(V, batch_size, nbatches):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Generate random data for a src-tgt copy task."</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(nbatches):</span></span>
<span class="line"><span style="color:#E1E4E8">        data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randint(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, V, </span><span style="color:#FFAB70">size</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(batch_size, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">        data[:, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span></span>
<span class="line"><span style="color:#E1E4E8">        src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">).clone().detach()</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">).clone().detach()</span></span>
<span class="line"><span style="color:#F97583">        yield</span><span style="color:#E1E4E8"> Batch(src, tgt, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> SimpleLossCompute</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#9ECBFF">    "A simple loss compute and train function."</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, generator, criterion):</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.generator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.criterion </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> criterion</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __call__</span><span style="color:#E1E4E8">(self, x, y, norm):</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.generator(x)</span></span>
<span class="line"><span style="color:#E1E4E8">        sloss </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.criterion(</span></span>
<span class="line"><span style="color:#E1E4E8">                x.contiguous().view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, x.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)), y.contiguous().view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">            )</span></span>
<span class="line"><span style="color:#F97583">            /</span><span style="color:#E1E4E8"> norm</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> sloss.data </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> norm, sloss</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> greedy_decode</span><span style="color:#E1E4E8">(model, src, src_mask, max_len, start_symbol):</span></span>
<span class="line"><span style="color:#E1E4E8">    memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.encode(src, src_mask)</span></span>
<span class="line"><span style="color:#E1E4E8">    ys </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">).fill_(start_symbol).type_as(src.data)</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(max_len </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        out </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.decode(</span></span>
<span class="line"><span style="color:#E1E4E8">            memory, src_mask, ys, subsequent_mask(ys.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)).type_as(src.data)</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#E1E4E8">        prob </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generator(out[:, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">        _, next_word </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.max(prob, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        next_word </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> next_word.data[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">        ys </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cat(</span></span>
<span class="line"><span style="color:#E1E4E8">            [ys, torch.zeros(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">).type_as(src.data).fill_(next_word)], </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> ys</span></span></code></pre>
<p><strong>复制任务的核心价值：</strong></p>
<ul>
<li><strong>原理验证器</strong>：测试模型是否具备基本的序列学习能力</li>
<li><strong>架构调试器</strong>：快速发现模型实现中的问题</li>
<li><strong>性能基准</strong>：为更复杂任务建立性能底线</li>
</ul>
<h3 id="为什么从复制任务开始-循序渐进的学习策略"><strong>为什么从复制任务开始：</strong> 循序渐进的学习策略</h3>
<p><strong>复制任务的独特优势：</strong></p>
<ol>
<li><strong>明确性</strong>：输入输出关系完全确定，容易验证正确性</li>
<li><strong>简洁性</strong>：排除语言学复杂性，专注于模型架构测试</li>
<li><strong>快速性</strong>：训练时间短，可以快速迭代和调试</li>
<li><strong>必要性</strong>：连复制都学不会，更复杂任务必然失败</li>
</ol>
<p><strong>从复制到翻译的能力阶梯：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 能力层次：复制 &#x3C; 重排 &#x3C; 转换 &#x3C; 翻译</span></span>
<span class="line"><span style="color:#6A737D"># 复制：[1,2,3] → [1,2,3]  (完全记忆)</span></span>
<span class="line"><span style="color:#6A737D"># 重排：[1,2,3] → [3,1,2]  (位置变换)</span></span>
<span class="line"><span style="color:#6A737D"># 转换：[1,2,3] → [4,5,6]  (符号映射)</span></span>
<span class="line"><span style="color:#6A737D"># 翻译：["I","love"] → ["ich","liebe"]  (语言转换)</span></span></code></pre>
<h3 id="怎么做-复制任务的精确实现"><strong>怎么做：</strong> 复制任务的精确实现</h3>
<p><strong>数据生成的巧妙设计：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> data_gen</span><span style="color:#E1E4E8">(V, batch_size, nbatches):</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(nbatches):</span></span>
<span class="line"><span style="color:#E1E4E8">        data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.randint(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, V, </span><span style="color:#FFAB70">size</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(batch_size, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">))  </span><span style="color:#6A737D"># 随机序列</span></span>
<span class="line"><span style="color:#E1E4E8">        data[:, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span><span style="color:#6A737D">                                     # 固定起始符</span></span>
<span class="line"><span style="color:#E1E4E8">        src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">).clone().detach()  </span><span style="color:#6A737D"># 源序列</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">).clone().detach()  </span><span style="color:#6A737D"># 目标序列</span></span>
<span class="line"><span style="color:#F97583">        yield</span><span style="color:#E1E4E8"> Batch(src, tgt, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)                          </span><span style="color:#6A737D"># 返回批次</span></span></code></pre>
<p><strong>设计细节的深层考虑：</strong></p>
<p><strong>1. 词汇范围控制：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">torch.randint(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, V, </span><span style="color:#FFAB70">size</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(batch_size, </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">))  </span><span style="color:#6A737D"># 从1到V-1随机选择</span></span></code></pre>
<ul>
<li><strong>避免0索引</strong>：通常保留给特殊符号（如填充）</li>
<li><strong>固定长度</strong>：简化实验，专注于学习机制验证</li>
<li><strong>随机性</strong>：确保模型学习通用模式，而非记忆特定序列</li>
</ul>
<p><strong>2. 起始符的统一设置：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">data[:, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span><span style="color:#6A737D">  # 所有序列都以符号1开始</span></span></code></pre>
<ul>
<li><strong>一致性</strong>：为解码器提供统一的起始点</li>
<li><strong>可预测性</strong>：简化解码过程的初始状态</li>
<li><strong>实用性</strong>：模拟真实场景中的句子开始标记</li>
</ul>
<p><strong>3. 梯度管理：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data.requires_grad_(</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">).clone().detach()</span></span></code></pre>
<ul>
<li><strong>内存优化</strong>：数据不需要梯度，节省计算资源</li>
<li><strong>安全性</strong>：避免意外的梯度传播到数据生成过程</li>
</ul>
<p><strong>SimpleLossCompute的封装智慧：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> SimpleLossCompute</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __call__</span><span style="color:#E1E4E8">(self, x, y, norm):</span></span>
<span class="line"><span style="color:#E1E4E8">        x </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.generator(x)                           </span><span style="color:#6A737D"># 生成词汇概率</span></span>
<span class="line"><span style="color:#E1E4E8">        sloss </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.criterion(                         </span><span style="color:#6A737D"># 计算损失</span></span>
<span class="line"><span style="color:#E1E4E8">            x.contiguous().view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, x.size(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)),      </span><span style="color:#6A737D"># 展平为2D</span></span>
<span class="line"><span style="color:#E1E4E8">            y.contiguous().view(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)                    </span><span style="color:#6A737D"># 展平为1D</span></span>
<span class="line"><span style="color:#E1E4E8">        ) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> norm                                       </span><span style="color:#6A737D"># 归一化</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> sloss.data </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> norm, sloss               </span><span style="color:#6A737D"># 返回值和节点</span></span></code></pre>
<p><strong>形状变换的必要性：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 原始形状：</span></span>
<span class="line"><span style="color:#6A737D"># x: [batch_size, seq_len, vocab_size]</span></span>
<span class="line"><span style="color:#6A737D"># y: [batch_size, seq_len]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 展平后：</span></span>
<span class="line"><span style="color:#6A737D"># x: [batch_size * seq_len, vocab_size]  # 每行是一个词的概率分布</span></span>
<span class="line"><span style="color:#6A737D"># y: [batch_size * seq_len]              # 每个元素是正确的词汇ID</span></span></code></pre>
<p><strong>返回双值的设计：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">return</span><span style="color:#E1E4E8"> sloss.data </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> norm, sloss  </span><span style="color:#6A737D"># (标量损失, 张量损失)</span></span></code></pre>
<ul>
<li><strong>标量损失</strong>：用于监控和日志记录</li>
<li><strong>张量损失</strong>：保留计算图，用于反向传播</li>
</ul>
<p><strong>贪婪解码的逐步实现：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> greedy_decode</span><span style="color:#E1E4E8">(model, src, src_mask, max_len, start_symbol):</span></span></code></pre>
<p><strong>解码的四个关键阶段：</strong></p>
<p><strong>阶段1：编码源序列</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.encode(src, src_mask)  </span><span style="color:#6A737D"># 获得源序列的表示</span></span></code></pre>
<ul>
<li><strong>一次编码</strong>：编码器只需运行一次</li>
<li><strong>记忆存储</strong>：存储源序列的完整信息</li>
</ul>
<p><strong>阶段2：初始化解码</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">ys </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">).fill_(start_symbol).type_as(src.data)</span></span></code></pre>
<ul>
<li><strong>起始状态</strong>：从特殊的开始符号启动</li>
<li><strong>类型匹配</strong>：确保设备和数据类型一致</li>
</ul>
<p><strong>阶段3：逐步生成</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(max_len </span><span style="color:#F97583">-</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">    out </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.decode(memory, src_mask, ys, subsequent_mask(ys.size(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)).type_as(src.data))</span></span>
<span class="line"><span style="color:#E1E4E8">    prob </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generator(out[:, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">])      </span><span style="color:#6A737D"># 只看最后位置的输出</span></span>
<span class="line"><span style="color:#E1E4E8">    _, next_word </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.max(prob, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)   </span><span style="color:#6A737D"># 选择概率最大的词</span></span>
<span class="line"><span style="color:#E1E4E8">    next_word </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> next_word.data[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]           </span><span style="color:#6A737D"># 提取标量值</span></span>
<span class="line"><span style="color:#E1E4E8">    ys </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cat([ys, torch.zeros(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">).type_as(src.data).fill_(next_word)], </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>贪婪策略的实现细节：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">torch.max(prob, </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 返回 (最大值, 最大值索引)</span></span></code></pre>
<ul>
<li><strong>贪婪选择</strong>：总是选择当前步骤概率最大的词</li>
<li><strong>简单高效</strong>：无需复杂的搜索算法</li>
<li><strong>确定性</strong>：相同输入总是产生相同输出</li>
</ul>
<p><strong>序列拼接的动态过程：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">ys </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cat([ys, new_word_tensor], </span><span style="color:#FFAB70">dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 在序列维度拼接</span></span></code></pre>
<ul>
<li><strong>动态扩展</strong>：序列长度逐步增长</li>
<li><strong>历史保持</strong>：保留之前生成的所有词汇</li>
</ul>
<p><strong>复制任务的成功标准：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 输入：[1, 3, 7, 2, 9]</span></span>
<span class="line"><span style="color:#6A737D"># 期望输出：[1, 3, 7, 2, 9]</span></span>
<span class="line"><span style="color:#6A737D"># 成功指标：100%的词汇级别准确率</span></span></code></pre>
<p><strong>训练进度的典型模式：</strong></p>
<ol>
<li><strong>初期混乱</strong>：随机输出，准确率接近1/vocab_size</li>
<li><strong>起始符学习</strong>：快速学会复制第一个符号</li>
<li><strong>模式识别</strong>：逐渐理解复制任务的本质</li>
<li><strong>完美复制</strong>：最终实现100%准确率</li>
</ol>
<p>这种精心设计的复制任务，为模型的复杂应用奠定了坚实的基础，体现了**“大道至简”**的深度学习训练哲学。</p>
<hr>
<h2 id="单元格-78-85实际机器翻译任务---真实应用的挑战">单元格 78-85：实际机器翻译任务 - 真实应用的挑战</h2>
<h3 id="是什么-从人工数据到真实语言的跨越"><strong>是什么：</strong> 从人工数据到真实语言的跨越</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Load the dataset using datasets library</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> datasets </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> load_dataset</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Load IWSLT 2017 German-English dataset</span></span>
<span class="line"><span style="color:#E1E4E8">raw_datasets </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> load_dataset(</span><span style="color:#9ECBFF">"iwslt2017"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"iwslt2017-de-en"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Tokenization using spaCy</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> spacy</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">spacy_de </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> spacy.load(</span><span style="color:#9ECBFF">"de_core_news_sm"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">spacy_en </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> spacy.load(</span><span style="color:#9ECBFF">"en_core_web_sm"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> tokenize_de</span><span style="color:#E1E4E8">(text):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> [tok.text </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> tok </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> spacy_de.tokenizer(text)]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> tokenize_en</span><span style="color:#E1E4E8">(text):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> [tok.text </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> tok </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> spacy_en.tokenizer(text)]</span></span></code></pre>
<p><strong>机器翻译的核心挑战：</strong></p>
<ul>
<li><strong>语言复杂性</strong>：处理真实语言的语法、语义、语用等多层面复杂性</li>
<li><strong>知识迁移</strong>：将在复制任务上验证的架构应用到复杂的双语映射</li>
<li><strong>评估标准</strong>：使用BLEU等标准指标衡量翻译质量</li>
</ul>
<h3 id="为什么选择机器翻译-transformer的经典应用场景"><strong>为什么选择机器翻译：</strong> Transformer的经典应用场景</h3>
<p><strong>机器翻译的代表性意义：</strong></p>
<ol>
<li><strong>序列到序列本质</strong>：体现Transformer encoder-decoder架构的完整能力</li>
<li><strong>历史意义</strong>：Transformer最初就是为机器翻译而设计</li>
<li><strong>实用价值</strong>：具有直接的商业和社会应用价值</li>
<li><strong>评估完善</strong>：有成熟的评估体系和基准数据集</li>
</ol>
<p><strong>IWSLT数据集的优势：</strong></p>
<ol>
<li><strong>适中规模</strong>：数据量足够训练，但不会过度消耗计算资源</li>
<li><strong>高质量</strong>：人工标注，质量可靠</li>
<li><strong>标准基准</strong>：研究社区广泛使用，便于对比</li>
<li><strong>语言对经典</strong>：德英翻译是NLP研究的经典语言对</li>
</ol>
<h3 id="怎么做-真实数据处理的工程挑战"><strong>怎么做：</strong> 真实数据处理的工程挑战</h3>
<p><strong>分词处理的深层考虑：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">spacy_de </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> spacy.load(</span><span style="color:#9ECBFF">"de_core_news_sm"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 德语分词模型</span></span>
<span class="line"><span style="color:#E1E4E8">spacy_en </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> spacy.load(</span><span style="color:#9ECBFF">"en_core_web_sm"</span><span style="color:#E1E4E8">)   </span><span style="color:#6A737D"># 英语分词模型</span></span></code></pre>
<p><strong>为什么使用spaCy：</strong></p>
<ol>
<li><strong>语言特异性</strong>：针对不同语言的专门优化</li>
<li><strong>工业级质量</strong>：经过大规模数据训练和验证</li>
<li><strong>一致性</strong>：确保分词结果的可重复性</li>
<li><strong>便利性</strong>：与深度学习框架良好集成</li>
</ol>
<p><strong>分词策略的影响：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 例如德语复合词：</span></span>
<span class="line"><span style="color:#6A737D"># "Bundesrepublik" → ["Bundes", "republik"] 或 ["Bundesrepublik"]</span></span>
<span class="line"><span style="color:#6A737D"># 不同策略影响词汇表大小和翻译质量</span></span></code></pre>
<p>这种从简单复制任务到复杂机器翻译的渐进式设计，体现了深度学习项目中**“从简到繁、步步为营”**的工程智慧。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span></span></span>
<span class="line"><span>**分词的重要性：**</span></span>
<span class="line"><span>1. **语言理解**：将文本分解成有意义的单元</span></span>
<span class="line"><span>2. **标准化**：处理标点、大小写等</span></span>
<span class="line"><span>3. **多语言支持**：不同语言有不同的分词规则</span></span>
<span class="line"><span></span></span>
<span class="line"><span>**spaCy 工具：**</span></span>
<span class="line"><span>- 业界标准的自然语言处理库</span></span>
<span class="line"><span>- 提供多语言支持</span></span>
<span class="line"><span>- 包含预训练的语言模型</span></span>
<span class="line"><span></span></span>
<span class="line"><span>### 词汇表构建：</span></span>
<span class="line"><span></span></span>
<span class="line"><span>```python</span></span>
<span class="line"><span>from torchtext.data.utils import get_tokenizer</span></span>
<span class="line"><span>from collections import Counter</span></span>
<span class="line"><span>from torchtext.vocab import vocab</span></span>
<span class="line"><span></span></span>
<span class="line"><span>def build_vocabulary(spacy_tokenizer, data_iter):</span></span>
<span class="line"><span>    def yield_tokens(data_iter):</span></span>
<span class="line"><span>        for data_sample in data_iter:</span></span>
<span class="line"><span>            yield spacy_tokenizer(data_sample)</span></span>
<span class="line"><span>    </span></span>
<span class="line"><span>    counter = Counter()</span></span>
<span class="line"><span>    for tokens in yield_tokens(data_iter):</span></span>
<span class="line"><span>        counter.update(tokens)</span></span>
<span class="line"><span>    </span></span>
<span class="line"><span>    return vocab(counter, specials=["&#x3C;unk>", "&#x3C;pad>", "&#x3C;bos>", "&#x3C;eos>"])</span></span></code></pre>
<p><strong>词汇表的作用：</strong></p>
<ol>
<li><strong>词汇映射</strong>：将词汇转换为数字ID</li>
<li><strong>处理未知词</strong>：使用 <code>&#x3C;unk></code> 标记</li>
<li><strong>特殊标记</strong>：
<ul>
<li><code>&#x3C;pad></code>：填充短句子</li>
<li><code>&#x3C;bos></code>：句子开始</li>
<li><code>&#x3C;eos></code>：句子结束</li>
<li><code>&#x3C;unk></code>：未知词汇</li>
</ul>
</li>
</ol>
<h3 id="数据加载器">数据加载器：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> collate_batch</span><span style="color:#E1E4E8">(batch, src_pipeline, tgt_pipeline, src_vocab, tgt_vocab, device, max_padding</span><span style="color:#F97583">=</span><span style="color:#79B8FF">128</span><span style="color:#E1E4E8">, pad_id</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">    bs_id </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.tensor([</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">device</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">device)  </span><span style="color:#6A737D"># &#x3C;bos> token id</span></span>
<span class="line"><span style="color:#E1E4E8">    eos_id </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.tensor([</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">device</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">device)  </span><span style="color:#6A737D"># &#x3C;eos> token id</span></span>
<span class="line"><span style="color:#E1E4E8">    src_list, tgt_list </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [], []</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> (_src, _tgt) </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> batch:</span></span>
<span class="line"><span style="color:#E1E4E8">        processed_src </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cat([bs_id, torch.tensor(src_vocab(src_pipeline(_src)), </span><span style="color:#FFAB70">dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">torch.int64, </span><span style="color:#FFAB70">device</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">device), eos_id], </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        processed_tgt </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.cat([bs_id, torch.tensor(tgt_vocab(tgt_pipeline(_tgt)), </span><span style="color:#FFAB70">dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">torch.int64, </span><span style="color:#FFAB70">device</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">device), eos_id], </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        src_list.append(processed_src[:max_padding])</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_list.append(processed_tgt[:max_padding])</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> pad_sequence(src_list, </span><span style="color:#FFAB70">padding_value</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">pad_id), pad_sequence(tgt_list, </span><span style="color:#FFAB70">padding_value</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">pad_id)</span></span></code></pre>
<p><strong>批次处理的挑战：</strong></p>
<ol>
<li><strong>变长序列</strong>：不同句子长度不同</li>
<li><strong>填充对齐</strong>：需要填充到相同长度</li>
<li><strong>特殊标记</strong>：添加句子边界标记</li>
<li><strong>内存管理</strong>：限制最大长度</li>
</ol>
<hr>
<h2 id="单元格-86-92模型训练和评估">单元格 86-92：模型训练和评估</h2>
<h3 id="训练配置">训练配置：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> create_model</span><span style="color:#E1E4E8">(src_vocab_size, tgt_vocab_size, N</span><span style="color:#F97583">=</span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">, d_model</span><span style="color:#F97583">=</span><span style="color:#79B8FF">512</span><span style="color:#E1E4E8">, d_ff</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2048</span><span style="color:#E1E4E8">, h</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">, dropout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Helper: Construct a model from hyperparameters."</span></span>
<span class="line"><span style="color:#E1E4E8">    c </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> copy.deepcopy</span></span>
<span class="line"><span style="color:#E1E4E8">    attn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> MultiHeadedAttention(h, d_model)</span></span>
<span class="line"><span style="color:#E1E4E8">    ff </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionwiseFeedForward(d_model, d_ff, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8">    position </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> PositionalEncoding(d_model, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8">    model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> EncoderDecoder(</span></span>
<span class="line"><span style="color:#E1E4E8">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, src_vocab_size), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        nn.Sequential(Embeddings(d_model, tgt_vocab_size), c(position)),</span></span>
<span class="line"><span style="color:#E1E4E8">        Generator(d_model, tgt_vocab_size))</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#6A737D">    # Initialize parameters with Glorot / fan_avg.</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> p </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> model.parameters():</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> p.dim() </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            nn.init.xavier_uniform_(p)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> model</span></span></code></pre>
<p><strong>超参数的选择：</strong></p>
<ul>
<li><code>N=6</code>：编码器和解码器各6层（原论文配置）</li>
<li><code>d_model=512</code>：模型维度</li>
<li><code>d_ff=2048</code>：前馈网络维度</li>
<li><code>h=8</code>：注意力头数</li>
<li><code>dropout=0.1</code>：防止过拟合</li>
</ul>
<p><strong>参数初始化：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">nn.init.xavier_uniform_(p)</span></span></code></pre>
<ul>
<li>Xavier初始化保持激活值的方差</li>
<li>有助于训练稳定性</li>
<li>避免梯度爆炸或消失</li>
</ul>
<h3 id="训练循环">训练循环：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> train_worker</span><span style="color:#E1E4E8">(gpu, ngpus_per_node, config, model, criterion, opt):</span></span>
<span class="line"><span style="color:#E1E4E8">    torch.distributed.init_process_group(</span></span>
<span class="line"><span style="color:#FFAB70">        backend</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"nccl"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">init_method</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"env://"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">rank</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">gpu, </span><span style="color:#FFAB70">world_size</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">ngpus_per_node</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    torch.cuda.set_device(gpu)</span></span>
<span class="line"><span style="color:#E1E4E8">    device </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.device(</span><span style="color:#9ECBFF">"cuda:</span><span style="color:#79B8FF">{}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">.format(gpu))</span></span>
<span class="line"><span style="color:#E1E4E8">    model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.to(device)</span></span>
<span class="line"><span style="color:#E1E4E8">    model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DDP(model, </span><span style="color:#FFAB70">device_ids</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[gpu])</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    train_dataloader, valid_dataloader </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> create_dataloaders(device, config[</span><span style="color:#9ECBFF">"vocab_src"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"vocab_tgt"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"spacy_de"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"spacy_en"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"batch_size"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"max_padding"</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"is_distributed"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    optimizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.optim.Adam(model.parameters(), </span><span style="color:#FFAB70">lr</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">config[</span><span style="color:#9ECBFF">"base_lr"</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">betas</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.98</span><span style="color:#E1E4E8">), </span><span style="color:#FFAB70">eps</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1e-9</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    lr_scheduler </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LambdaLR(</span><span style="color:#FFAB70">optimizer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">optimizer, </span><span style="color:#FFAB70">lr_lambda</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8"> step: rate(step, config[</span><span style="color:#9ECBFF">"d_model"</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">factor</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">warmup</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">config[</span><span style="color:#9ECBFF">"warmup"</span><span style="color:#E1E4E8">]))</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    train_state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> TrainState()</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> epoch </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(config[</span><span style="color:#9ECBFF">"num_epochs"</span><span style="color:#E1E4E8">]):</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> config[</span><span style="color:#9ECBFF">"is_distributed"</span><span style="color:#E1E4E8">]:</span></span>
<span class="line"><span style="color:#E1E4E8">            train_dataloader.sampler.set_epoch(epoch)</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#E1E4E8">        model.train()</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"[GPU</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">gpu</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">] Epoch </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">epoch</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> Training ===="</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">flush</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        _, train_state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> run_epoch(</span></span>
<span class="line"><span style="color:#E1E4E8">            (Batch(b[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], b[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], config[</span><span style="color:#9ECBFF">"pad_id"</span><span style="color:#E1E4E8">]) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> b </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> train_dataloader),</span></span>
<span class="line"><span style="color:#E1E4E8">            model,</span></span>
<span class="line"><span style="color:#E1E4E8">            SimpleLossCompute(model.module.generator, criterion),</span></span>
<span class="line"><span style="color:#E1E4E8">            optimizer,</span></span>
<span class="line"><span style="color:#E1E4E8">            lr_scheduler,</span></span>
<span class="line"><span style="color:#FFAB70">            mode</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"train+log"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">            accum_iter</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">config[</span><span style="color:#9ECBFF">"accum_iter"</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#FFAB70">            train_state</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">train_state,</span></span>
<span class="line"><span style="color:#E1E4E8">        )</span></span></code></pre>
<p><strong>分布式训练：</strong></p>
<ul>
<li>使用多个GPU并行训练</li>
<li><code>DDP</code>（DistributedDataParallel）：PyTorch的分布式训练</li>
<li>提高训练速度和处理能力</li>
</ul>
<p><strong>优化器配置：</strong></p>
<ul>
<li><strong>Adam</strong>：自适应学习率优化器</li>
<li><strong>betas=(0.9, 0.98)</strong>：动量参数</li>
<li><strong>eps=1e-9</strong>：数值稳定性</li>
</ul>
<h3 id="bleu评估">BLEU评估：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> check_outputs</span><span style="color:#E1E4E8">(valid_dataloader, model, vocab_src, vocab_tgt, n_examples</span><span style="color:#F97583">=</span><span style="color:#79B8FF">15</span><span style="color:#E1E4E8">, pad_idx</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, eos_string</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"&#x3C;/s>"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">    results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [()] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> n_examples</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> idx </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(n_examples):</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">Example </span><span style="color:#79B8FF">%d</span><span style="color:#9ECBFF"> ========</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#F97583"> %</span><span style="color:#E1E4E8"> idx)</span></span>
<span class="line"><span style="color:#E1E4E8">        b </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> next</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">iter</span><span style="color:#E1E4E8">(valid_dataloader))</span></span>
<span class="line"><span style="color:#E1E4E8">        rb </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Batch(b[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], b[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], pad_idx)</span></span>
<span class="line"><span style="color:#E1E4E8">        greedy_decode_result, _ </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> greedy_decode(model, rb.src, rb.src_mask, </span><span style="color:#79B8FF">64</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#E1E4E8">        src_tokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_src.get_itos()[x] </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> rb.src[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad_idx]</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_tokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_tgt.get_itos()[x] </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> rb.tgt[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad_idx]</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Source Text (Input)        : "</span><span style="color:#F97583"> +</span><span style="color:#9ECBFF"> " "</span><span style="color:#E1E4E8">.join(src_tokens).replace(</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Target Text (Ground Truth) : "</span><span style="color:#F97583"> +</span><span style="color:#9ECBFF"> " "</span><span style="color:#E1E4E8">.join(tgt_tokens).replace(</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Model Output               : "</span><span style="color:#F97583"> +</span><span style="color:#9ECBFF"> " "</span><span style="color:#E1E4E8">.join([vocab_tgt.get_itos()[x] </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> greedy_decode_result[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">!=</span><span style="color:#E1E4E8"> pad_idx]).replace(</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">        results[idx] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (rb, greedy_decode_result)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> results</span></span></code></pre>
<p><strong>模型评估的重要性：</strong></p>
<ol>
<li><strong>定量评估</strong>：BLEU分数等指标</li>
<li><strong>定性分析</strong>：人工检查翻译质量</li>
<li><strong>错误分析</strong>：理解模型的局限性</li>
</ol>
<p><strong>BLEU分数：</strong></p>
<ul>
<li>比较机器翻译和参考翻译的n-gram重叠</li>
<li>分数范围0-100，越高越好</li>
<li>业界标准的机器翻译评估指标</li>
</ul>
<hr>
<h2 id="单元格-93-100注意力可视化">单元格 93-100：注意力可视化</h2>
<h3 id="注意力权重提取">注意力权重提取：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> draw_attention</span><span style="color:#E1E4E8">(data, layer, head, row, col, ax):</span></span>
<span class="line"><span style="color:#9ECBFF">    "Draw attention weights"</span></span>
<span class="line"><span style="color:#E1E4E8">    attention </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> data[layer][</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, head].data</span></span>
<span class="line"><span style="color:#E1E4E8">    ax.matshow(attention, </span><span style="color:#FFAB70">cmap</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">'Blues'</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    ax.set_xticks(</span><span style="color:#79B8FF">range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(col)))</span></span>
<span class="line"><span style="color:#E1E4E8">    ax.set_yticks(</span><span style="color:#79B8FF">range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(row)))</span></span>
<span class="line"><span style="color:#E1E4E8">    ax.set_xticklabels(col, </span><span style="color:#FFAB70">rotation</span><span style="color:#F97583">=</span><span style="color:#79B8FF">90</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    ax.set_yticklabels(row)</span></span></code></pre>
<p><strong>注意力可视化的价值：</strong></p>
<ol>
<li><strong>模型解释性</strong>：理解模型在关注什么</li>
<li><strong>调试工具</strong>：发现模型的问题</li>
<li><strong>语言学洞察</strong>：揭示语言现象</li>
</ol>
<h3 id="多头注意力分析">多头注意力分析：</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> visualize_attention</span><span style="color:#E1E4E8">(model, vocab_src, vocab_tgt, sentence_idx</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#6A737D">    # Set model to evaluation mode</span></span>
<span class="line"><span style="color:#E1E4E8">    model.eval()</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#F97583">    with</span><span style="color:#E1E4E8"> torch.no_grad():</span></span>
<span class="line"><span style="color:#6A737D">        # Get a batch of data</span></span>
<span class="line"><span style="color:#E1E4E8">        example_data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> check_outputs(valid_dataloader, model, vocab_src, vocab_tgt, </span><span style="color:#FFAB70">n_examples</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        example_data_idx </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sentence_idx</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#6A737D">        # Get the source and target sentences</span></span>
<span class="line"><span style="color:#E1E4E8">        src_sent </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> example_data[example_data_idx][</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">].src[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_sent </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> example_data[example_data_idx][</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">][</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#6A737D">        # Convert to tokens</span></span>
<span class="line"><span style="color:#E1E4E8">        src_tokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_src.get_itos()[x] </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> src_sent </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">!=</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># Remove padding</span></span>
<span class="line"><span style="color:#E1E4E8">        tgt_tokens </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [vocab_tgt.get_itos()[x] </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> tgt_sent </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">!=</span><span style="color:#79B8FF"> 2</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># Remove padding</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Source:"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">" "</span><span style="color:#E1E4E8">.join(src_tokens))</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Target:"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">" "</span><span style="color:#E1E4E8">.join(tgt_tokens))</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#6A737D">        # Get attention weights for each layer and head</span></span>
<span class="line"><span style="color:#E1E4E8">        attns </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> run_model_extract_attentions(example_data[example_data_idx][</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], model)</span></span>
<span class="line"><span style="color:#E1E4E8">        </span></span>
<span class="line"><span style="color:#6A737D">        # Visualize different attention heads</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> layer </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># 6 layers</span></span>
<span class="line"><span style="color:#E1E4E8">            fig, axes </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> plt.subplots(</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">figsize</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">16</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> head </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># 8 heads</span></span>
<span class="line"><span style="color:#E1E4E8">                draw_attention(attns, layer, head, tgt_tokens, src_tokens, axes[head</span><span style="color:#F97583">//</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">, head</span><span style="color:#F97583">%</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">                axes[head</span><span style="color:#F97583">//</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">, head</span><span style="color:#F97583">%</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">].set_title(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Layer </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">layer</span><span style="color:#F97583">+</span><span style="color:#79B8FF">1}</span><span style="color:#9ECBFF"> Head </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">head</span><span style="color:#F97583">+</span><span style="color:#79B8FF">1}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span></code></pre>
<p><strong>不同注意力头的专门化：</strong>
研究发现不同的注意力头学会了不同的语言现象：</p>
<ul>
<li><strong>语法关系</strong>：主谓关系、修饰关系</li>
<li><strong>长距离依赖</strong>：跨越多个词的关系</li>
<li><strong>位置信息</strong>：相邻词的关系</li>
</ul>
<h3 id="编码器-解码器注意力">编码器-解码器注意力：</h3>
<p>编码器-解码器注意力显示了：</p>
<ol>
<li><strong>对齐关系</strong>：源语言和目标语言词汇的对应</li>
<li><strong>翻译策略</strong>：模型如何处理不同的语言结构</li>
<li><strong>语言差异</strong>：德语和英语的语序差异</li>
</ol>
<hr>
<h2 id="总结完整的transformer实现">总结：完整的Transformer实现</h2>
<h3 id="我们学到了什么">我们学到了什么：</h3>
<p><strong>1. 模型架构：</strong></p>
<ul>
<li><strong>注意力机制</strong>：Self-attention和Cross-attention</li>
<li><strong>多头注意力</strong>：并行处理不同类型的关系</li>
<li><strong>位置编码</strong>：为模型提供位置信息</li>
<li><strong>残差连接和层归一化</strong>：稳定训练</li>
</ul>
<p><strong>2. 训练技术：</strong></p>
<ul>
<li><strong>Teacher Forcing</strong>：训练时使用真实标签</li>
<li><strong>学习率调度</strong>：Warmup和衰减策略</li>
<li><strong>标签平滑</strong>：提高泛化能力</li>
<li><strong>梯度累积</strong>：模拟大批次训练</li>
</ul>
<p><strong>3. 实际应用：</strong></p>
<ul>
<li><strong>数据处理</strong>：分词、词汇表构建</li>
<li><strong>批次处理</strong>：填充和掩码</li>
<li><strong>分布式训练</strong>：多GPU并行</li>
<li><strong>模型评估</strong>：BLEU分数和可视化</li>
</ul>
<h3 id="为什么transformer如此重要">为什么Transformer如此重要：</h3>
<p><strong>1. 并行化：</strong></p>
<ul>
<li>不像RNN需要顺序处理</li>
<li>可以并行计算所有位置</li>
<li>训练和推理都更快</li>
</ul>
<p><strong>2. 长距离依赖：</strong></p>
<ul>
<li>直接连接任意两个位置</li>
<li>避免了RNN的梯度问题</li>
<li>更好地处理长序列</li>
</ul>
<p><strong>3. 可解释性：</strong></p>
<ul>
<li>注意力权重提供了解释</li>
<li>可以看到模型关注什么</li>
<li>有助于调试和理解</li>
</ul>
<p><strong>4. 可扩展性：</strong></p>
<ul>
<li>架构简单但功能强大</li>
<li>可以轻松调整层数和维度</li>
<li>为GPT、BERT等奠定基础</li>
</ul>
<h3 id="后续发展">后续发展：</h3>
<p>这个基础的Transformer架构启发了：</p>
<ul>
<li><strong>BERT</strong>：双向编码器表示</li>
<li><strong>GPT</strong>：生成式预训练Transformer</li>
<li><strong>T5</strong>：Text-to-Text Transfer Transformer</li>
<li><strong>更多变体</strong>：优化效率和性能</li>
</ul>
<p>通过这个详细的解释，你应该对Transformer有了深入的理解。从基础的Python语法到复杂的注意力机制，每一个组件都有其重要的作用。这为理解现代自然语言处理的基础打下了坚实的基础！</p>
<hr>
<p><strong>建议的学习路径：</strong></p>
<ol>
<li><strong>先理解概念</strong>：每个组件的作用和原理</li>
<li><strong>动手实践</strong>：运行代码，修改参数</li>
<li><strong>可视化分析</strong>：观察注意力权重</li>
<li><strong>尝试改进</strong>：调整架构或训练策略</li>
<li><strong>应用扩展</strong>：尝试其他任务和数据集</li>
</ol>
<p>记住，深度学习是一个实践性很强的领域，理论理解和动手实践同样重要！</p> </div> </article> <div class="max-xl:hidden  "> <div id="nav-content" class="bg-white dark:bg-transparent sticky w-72 mt-8 rounded-2xl dark:border-0 border border-neutral-100 top-14 max-h-[calc(100svh-3.5rem)] overflow-x-hidden px-6 pt-8 pb-12"> <div class="flex flex-col gap-4 pl-0"> <div> <h3 class="dark:text-zinc-400 text-blacktext/90 font-black tracking-wide text-md uppercase">Table of Contents</h3> </div> <div class="flex flex-col gap-2 pr-6 text-neutral-500 dark:text-neutral-300 "> <ul id="toc-list" class="leading-loose text-base gap-2 border-l dark:border-neutral-500/20 border-blacktext/20"> <li class="leading-loose"> <a class="inline-block leading-5 pl-4 font-bold text-white border-l dark:border-white border-blacktext dark:hover:border-white hover:border-blacktext" href="#">Transformer代码深入理解</a> </li> </ul> </div> </div> </div> </div> <script type="module">document.addEventListener("DOMContentLoaded",function(){const d=document.getElementById("toc-list"),c=document.getElementById("content");if(!d||!c)return;const s=c.querySelectorAll("h2, h3");let r=d;s.forEach((e,n)=>{e.id||(e.id=e.textContent?.trim().toLowerCase().replace(/\s+/g,"-")+"-"+n);const l=document.createElement("li"),t=document.createElement("a");if(t.href=`#${e.id}`,t.textContent=e.textContent?.trim()||e.id,t.classList.add("inline-block","leading-5","hover:text-mint-400","py-2","border-l","border-transparent","dark:hover:border-white","hover:border-blacktext"),t.classList.add(e.tagName==="H2"?"pl-6":"pl-12"),console.log("classes removed 2"),l.appendChild(t),e.tagName==="H2"){r=document.createElement("ul"),r.classList.add("border-neutral-400","dark:hover:border-white","hover:border-blacktext","pl-0"),console.log("classes removed 3");const o=document.createElement("li");o.appendChild(t),o.appendChild(r),d.appendChild(o)}else r.appendChild(l);t.addEventListener("click",function(o){o.preventDefault(),document.getElementById(e.id)?.scrollIntoView({behavior:"smooth",block:"start"})})});const i=new IntersectionObserver(e=>{e.forEach(n=>{const l=n.target.getAttribute("id"),t=document.querySelector(`a[href="#${l}"]`);n.isIntersecting&&(document.querySelectorAll("#toc-list a").forEach(o=>{o.classList.remove("font-semibold","dark:text-mint-300!","text-blacktext!","dark:border-white!","border-blacktext!"),o.classList.add("dark:text-neutral-300","text-neutral-500"),console.log("classes removed 4")}),t?.classList.add("font-semibold","dark:text-mint-300!","text-blacktext!","border-l","dark:border-white!","border-blacktext!"))})},{rootMargin:"-30% 0px -65% 0px",threshold:.1});s.forEach(e=>i.observe(e))});</script> </section> <div class="flex flex-col gap-6 max-w-4xl max-lg:py-2 py-3 max-xl:py-2 mx-auto"> <div class="px-8"> <nav class="mt-8 flex flex-row gap-2 w-full p-6 max-xl:p-3 max-lg:p-2"> <a href="/blog/posts/TransformerFromScratch" style="width: -webkit-fill-available;" class="relative flex min-w-1/2 items-center justify-start gap-2 font-semibold dark:text-white text-blacktext text-left text-pretty max-sm:text-xs max-md:text-sm max-md:leading-4 hover:text-mint-300 hover:[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)] transition-all before:absolute before:-top-5 before:left-0 before:text-sm before:font-light before:content-['Previous_Post']"> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.85355 3.85355C7.04882 3.65829 7.04882 3.34171 6.85355 3.14645C6.65829 2.95118 6.34171 2.95118 6.14645 3.14645L2.14645 7.14645C1.95118 7.34171 1.95118 7.65829 2.14645 7.85355L6.14645 11.8536C6.34171 12.0488 6.65829 12.0488 6.85355 11.8536C7.04882 11.6583 7.04882 11.3417 6.85355 11.1464L3.20711 7.5L6.85355 3.85355ZM12.8536 3.85355C13.0488 3.65829 13.0488 3.34171 12.8536 3.14645C12.6583 2.95118 12.3417 2.95118 12.1464 3.14645L8.14645 7.14645C7.95118 7.34171 7.95118 7.65829 8.14645 7.85355L12.1464 11.8536C12.3417 12.0488 12.6583 12.0488 12.8536 11.8536C13.0488 11.6583 13.0488 11.3417 12.8536 11.1464L9.20711 7.5L12.8536 3.85355Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> Transformer From Scratch </a> <a href="/blog/posts/condatrick" style="width: -webkit-fill-available;" class="relative flex min-w-1/2 items-center justify-end gap-2 font-semibold  dark:text-white text-blacktext  text-right text-pretty max-sm:text-xs max-md:text-sm max-md:leading-4 hover:text-mint-300 hover:[text-shadow:1px_1px_11px_rgba(208,251,229,0.7)] transition-all before:absolute before:-top-5 before:right-0 before:text-sm before:font-light before:content-['Next_Post']"> Conda 环境管理 <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.14645 11.1464C1.95118 11.3417 1.95118 11.6583 2.14645 11.8536C2.34171 12.0488 2.65829 12.0488 2.85355 11.8536L6.85355 7.85355C7.04882 7.65829 7.04882 7.34171 6.85355 7.14645L2.85355 3.14645C2.65829 2.95118 2.34171 2.95118 2.14645 3.14645C1.95118 3.34171 1.95118 3.65829 2.14645 3.85355L5.79289 7.5L2.14645 11.1464ZM8.14645 11.1464C7.95118 11.3417 7.95118 11.6583 8.14645 11.8536C8.34171 12.0488 8.65829 12.0488 8.85355 11.8536L12.8536 7.85355C13.0488 7.65829 13.0488 7.34171 12.8536 7.14645L8.85355 3.14645C8.65829 2.95118 8.34171 2.95118 8.14645 3.14645C7.95118 3.34171 7.95118 3.65829 8.14645 3.85355L11.7929 7.5L8.14645 11.1464Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> </a> </nav> <hr class="text-mint-300/50"> </div> <div class="px-8 max-sm:px-4"> <section class="py-8 max-lg:px-4 max-md:px-8 max-sm:px-0 max-md:py-4 max-w-4xl mx-auto">   <div class="flex flex-col gap-8 w-full mx-auto"> <article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/KeYan/survey" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> October 5, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">论文-医嘱生成与健康预测</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" class="size-4 rotate-180" data-icon="arrow-left">   <symbol id="ai:local:arrow-left" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M6.854 3.146a.5.5 0 0 1 0 .708L3.707 7H12.5a.5.5 0 0 1 0 1H3.707l3.147 3.146a.5.5 0 0 1-.708.708l-4-4a.5.5 0 0 1 0-.708l4-4a.5.5 0 0 1 .708 0" clip-rule="evenodd"/></symbol><use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/markdown" aria-label="View articles about Markdown" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Markdown icon"><svg width="1.63em" height="1em" class="w-full!" data-icon="markdown">   <symbol id="ai:local:markdown" viewBox="0 0 208 128"><path fill="none" stroke="#FFF" stroke-width="10" d="M15 5h178a10 10 0 0 1 10 10v98a10 10 0 0 1-10 10H15a10 10 0 0 1-10-10V15A10 10 0 0 1 15 5z"/><path fill="#FFF" d="M30 98V30h20l20 25 20-25h20v68H90V59L70 84 50 59v39zm125 0-30-33h20V30h20v35h20z"/></symbol><use href="#ai:local:markdown"></use>  </svg></div>Markdown</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> AI </a><a href="/blog/tags/Clinical AI" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Clinical AI </a> </div> </div> </div> <a href="/blog/posts/KeYan/survey" style="background-image:url(image-8.png)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/CO/copre" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> September 30, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">COpre上机</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/verilog" aria-label="View articles about Verilog" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Verilog icon"><svg width="1em" height="1em" class="w-full!" data-icon="verilog">   <symbol id="ai:local:verilog" viewBox="0 0 32 32"><path fill="#c5c5c5" d="M29.007 17.4h.037a1.45 1.45 0 0 0 .938-.316 1.47 1.47 0 0 0 .519-1.031V15.9a1.413 1.413 0 0 0-1.376-1.3H25.5v-2.644h3.547A1.41 1.41 0 0 0 30.5 10.6v-.122a1.41 1.41 0 0 0-.646-1.1 1.46 1.46 0 0 0-.835-.225h-3.563a3 3 0 0 0-.278-1.034 2.9 2.9 0 0 0-1.7-1.461 2.7 2.7 0 0 0-.629-.13V2.884A1.414 1.414 0 0 0 21.481 1.5h-.116a1.4 1.4 0 0 0-1.319 1.388V6.5H17.4V2.894A1.41 1.41 0 0 0 16.053 1.5H15.9a1.41 1.41 0 0 0-1.3 1.383v3.625h-2.639V2.899a1.37 1.37 0 0 0-.4-.975 1.4 1.4 0 0 0-.984-.424H10.5a1.41 1.41 0 0 0-1.341 1.381V6.53a2.93 2.93 0 0 0-2.466 1.9 3 3 0 0 0-.161.726H2.876A1.413 1.413 0 0 0 1.5 10.5v.095a1.42 1.42 0 0 0 .575 1.091 1.46 1.46 0 0 0 .887.273h3.539V14.6h-3.61A1.43 1.43 0 0 0 1.5 15.913v.187a1.4 1.4 0 0 0 1.386 1.3H6.5v2.65h-.292c-.613-.007-1.226 0-1.838 0H3.087a1.63 1.63 0 0 0-.918.211A1.44 1.44 0 0 0 1.5 21.4v.1a1.417 1.417 0 0 0 1.375 1.337h3.671a2.83 2.83 0 0 0 1.978 2.5 2.7 2.7 0 0 0 .631.128v3.645a1.4 1.4 0 1 0 2.8-.092v-3.527H14.6v3.516a1.414 1.414 0 0 0 1.347 1.493h.153a1.41 1.41 0 0 0 1.3-1.385q.006-1.066 0-2.131V25.5h2.644v3.6a1.423 1.423 0 0 0 1.356 1.4h.119a1.41 1.41 0 0 0 1.16-.741 1.64 1.64 0 0 0 .167-.833v-3.452a2.7 2.7 0 0 0 .62-.128 2.93 2.93 0 0 0 1.886-1.888 3 3 0 0 0 .123-.613h3.646a1.42 1.42 0 0 0 1.379-1.364v-.118a1.41 1.41 0 0 0-1.382-1.318H25.5V17.4Z" opacity=".3"/><path fill="#1a348f" d="M10.515 2h.056a.91.91 0 0 1 .886.893c.006 1.108 0 2.216 0 3.324h-1.8V2.884A.907.907 0 0 1 10.515 2m5.422 0h.116a.91.91 0 0 1 .846.889c.006 1.109 0 2.219 0 3.329H15.1v-3.33A.91.91 0 0 1 15.937 2m5.455 0h.089a.907.907 0 0 1 .859.881c.007 1.112 0 2.225 0 3.337h-1.8V2.892A.91.91 0 0 1 21.392 2M2 10.518a.91.91 0 0 1 .882-.859c1.112-.007 2.223 0 3.334 0v1.8H2.979a1 1 0 0 1-.6-.173.92.92 0 0 1-.379-.715Zm23.786-.863h3.233a1 1 0 0 1 .561.143.92.92 0 0 1 .42.716v.058a.93.93 0 0 1-.3.651.96.96 0 0 1-.677.234h-3.237zM2 15.944a.913.913 0 0 1 .888-.842c1.109-.007 2.219 0 3.328 0v1.8H2.89a.915.915 0 0 1-.89-.839Zm23.786-.844h3.326a.914.914 0 0 1 .889.835v.117a.95.95 0 0 1-.331.641.97.97 0 0 1-.651.207h-3.233zM2.433 20.678a1.1 1.1 0 0 1 .643-.136c1.043.006 2.086-.006 3.129.006.028.6 0 1.2.012 1.8H2.886A.91.91 0 0 1 2 21.483V21.4a.93.93 0 0 1 .433-.722m23.353-.135h3.324a.914.914 0 0 1 .89.847v.09a.91.91 0 0 1-.888.859c-1.109.007-2.218 0-3.326 0zm-16.13 5.238h1.8v3.243a.96.96 0 0 1-.235.677.93.93 0 0 1-.653.3h-.052a.907.907 0 0 1-.86-.882c-.005-1.113.002-2.225 0-3.338m5.444 0h1.8v3.33a.914.914 0 0 1-.836.889h-.116a.94.94 0 0 1-.619-.306.96.96 0 0 1-.228-.673zm5.447 3.325v-3.325h1.8v3.144a1.2 1.2 0 0 1-.105.59.92.92 0 0 1-.756.484H21.4a.91.91 0 0 1-.853-.893m4.185-20.757a2.43 2.43 0 0 0-1.422-1.218A3.3 3.3 0 0 0 22.254 7c-4.3.008-8.6-.011-12.905.01a2.44 2.44 0 0 0-2.192 1.596A3.6 3.6 0 0 0 7 9.934v12.5a2.7 2.7 0 0 0 .268 1.219 2.43 2.43 0 0 0 1.42 1.217 3.8 3.8 0 0 0 1.239.13H22.07a3.8 3.8 0 0 0 1.243-.127 2.43 2.43 0 0 0 1.562-1.573A3.8 3.8 0 0 0 25 22.071v-12.5a2.7 2.7 0 0 0-.268-1.222"/><path fill="#c5c2ff" d="m7.5 8.307.084-.2q.968 0 1.935.024h.394q.179 0 .394-.012.776-.036 2.807-.036.406 0 .788.006t.741.018a.23.23 0 0 1 .108.108l-.024.191q-.072.119-.358.119h-.1a10 10 0 0 0-1.1.054 2.2 2.2 0 0 0-1 .245.43.43 0 0 0-.155.346 4 4 0 0 0 .394 1.4l1.818 4.217.634 1.4q.921 2.222 1.819 4.539l.179.454.8-1.959q.62-1.481 1.252-3.154l.823-2.246q.024-.072.131-.478a17 17 0 0 0 1.157-4 .56.56 0 0 0-.161-.364 1.17 1.17 0 0 0-.52-.3 8 8 0 0 0-.932-.143l-.884-.108a.34.34 0 0 1-.108-.2l.036-.1 3.87-.036H23.6a3.1 3.1 0 0 1 .885.084l.012.1a.6.6 0 0 1-.072.251 2.5 2.5 0 0 1-.526.119 3.6 3.6 0 0 0-1.244.281 1.8 1.8 0 0 0-.5.687l-3.134 7.765q-1.466 3.5-2.086 5.423l-.345 1.039a.53.53 0 0 1-.263.1 1.4 1.4 0 0 1-.3-.036q-1.422-3.676-2.318-5.681L10.055 9.92a3.3 3.3 0 0 0-.591-.89 5.5 5.5 0 0 0-1.379-.424 4 4 0 0 1-.466-.119.33.33 0 0 1-.119-.18"/></symbol><use href="#ai:local:verilog"></use>  </svg></div>Verilog</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/Logisim" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Logisim </a><a href="/blog/tags/Verilog" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> Verilog </a><a href="/blog/tags/MIPS" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> MIPS </a> </div> </div> </div>  </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/SomethingToWrite/suisui" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> September 30, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">一些碎碎念</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/suiji" aria-label="View articles about 随记" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="随记 icon"><svg width="1em" height="1em" class="w-full!" data-icon="memory">   <symbol id="ai:local:memory" viewBox="0 0 24 24"><g fill="none" stroke="#1C274D" stroke-width="1.5"><path d="M4 8c0-2.828 0-4.243.879-5.121C5.757 2 7.172 2 10 2h4c2.828 0 4.243 0 5.121.879C20 3.757 20 5.172 20 8v8c0 2.828 0 4.243-.879 5.121C18.243 22 16.828 22 14 22h-4c-2.828 0-4.243 0-5.121-.879C4 20.243 4 18.828 4 16z"/><path d="M19.898 16h-12c-.93 0-1.395 0-1.777.102A3 3 0 0 0 4 18.224"/><path stroke-linecap="round" d="M8 7h8m-8 3.5h5m0 5.5v3.53c0 .276 0 .414-.095.47s-.224-.006-.484-.13l-1.242-.59c-.088-.04-.132-.062-.179-.062s-.091.021-.179.063l-1.242.59c-.26.123-.39.185-.484.129C9 19.944 9 19.806 9 19.53v-3.08" opacity=".5"/></g></symbol><use href="#ai:local:memory"></use>  </svg></div>随记</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/随记" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> 随记 </a> </div> </div> </div> <a href="/blog/posts/SomethingToWrite/suisui" style="background-image:url(/images/posts/1756644786600.jpeg)" class="shrink-0 rounded-2xl bg-center bg-cover aspect-video max-md:aspect-video w-2/6 max-md:w-full"></a> </article><article class="bg-white dark:bg-zinc-900/25 dark:border dark:border-zinc-800 dark:hover:border-mint-300 hover:backdrop-blur-none backdrop-blur-lg shadow-sm overflow-auto hover:shadow-[5px_5px_rgba(0,98,90,0.4),10px_10px_rgba(0,98,90,0.3),15px_15px_rgba(0,98,90,0.2),20px_20px_rgba(0,98,90,0.1),25px_25px_rgba(0,98,90,0.05)] p-8 max-md:p-6 w-full flex justify-between items-center bg-linear-to-r hover:from-teal-200 hover:to-emerald-200 dark:hover:from-riptide-500 dark:hover:to-mint-500 transition-all hover:scale-105  duration-200 ease-in-out gap-8 max-md:gap-4 rounded-3xl max-md:flex-col-reverse"> <div class="flex flex-col"> <a href="/blog/posts/KL" class="flex flex-col gap-4 w-full"> <span class="flex flex-row center text-sm font-semibold items-center gap-3 text-blacktext dark:text-riptide-50 "> <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 1C4.77614 1 5 1.22386 5 1.5V2H10V1.5C10 1.22386 10.2239 1 10.5 1C10.7761 1 11 1.22386 11 1.5V2H12.5C13.3284 2 14 2.67157 14 3.5V12.5C14 13.3284 13.3284 14 12.5 14H2.5C1.67157 14 1 13.3284 1 12.5V3.5C1 2.67157 1.67157 2 2.5 2H4V1.5C4 1.22386 4.22386 1 4.5 1ZM10 3V3.5C10 3.77614 10.2239 4 10.5 4C10.7761 4 11 3.77614 11 3.5V3H12.5C12.7761 3 13 3.22386 13 3.5V5H2V3.5C2 3.22386 2.22386 3 2.5 3H4V3.5C4 3.77614 4.22386 4 4.5 4C4.77614 4 5 3.77614 5 3.5V3H10ZM2 6V12.5C2 12.7761 2.22386 13 2.5 13H12.5C12.7761 13 13 12.7761 13 12.5V6H2ZM7 7.5C7 7.22386 7.22386 7 7.5 7C7.77614 7 8 7.22386 8 7.5C8 7.77614 7.77614 8 7.5 8C7.22386 8 7 7.77614 7 7.5ZM9.5 7C9.22386 7 9 7.22386 9 7.5C9 7.77614 9.22386 8 9.5 8C9.77614 8 10 7.77614 10 7.5C10 7.22386 9.77614 7 9.5 7ZM11 7.5C11 7.22386 11.2239 7 11.5 7C11.7761 7 12 7.22386 12 7.5C12 7.77614 11.7761 8 11.5 8C11.2239 8 11 7.77614 11 7.5ZM11.5 9C11.2239 9 11 9.22386 11 9.5C11 9.77614 11.2239 10 11.5 10C11.7761 10 12 9.77614 12 9.5C12 9.22386 11.7761 9 11.5 9ZM9 9.5C9 9.22386 9.22386 9 9.5 9C9.77614 9 10 9.22386 10 9.5C10 9.77614 9.77614 10 9.5 10C9.22386 10 9 9.77614 9 9.5ZM7.5 9C7.22386 9 7 9.22386 7 9.5C7 9.77614 7.22386 10 7.5 10C7.77614 10 8 9.77614 8 9.5C8 9.22386 7.77614 9 7.5 9ZM5 9.5C5 9.22386 5.22386 9 5.5 9C5.77614 9 6 9.22386 6 9.5C6 9.77614 5.77614 10 5.5 10C5.22386 10 5 9.77614 5 9.5ZM3.5 9C3.22386 9 3 9.22386 3 9.5C3 9.77614 3.22386 10 3.5 10C3.77614 10 4 9.77614 4 9.5C4 9.22386 3.77614 9 3.5 9ZM3 11.5C3 11.2239 3.22386 11 3.5 11C3.77614 11 4 11.2239 4 11.5C4 11.7761 3.77614 12 3.5 12C3.22386 12 3 11.7761 3 11.5ZM5.5 11C5.22386 11 5 11.2239 5 11.5C5 11.7761 5.22386 12 5.5 12C5.77614 12 6 11.7761 6 11.5C6 11.2239 5.77614 11 5.5 11ZM7 11.5C7 11.2239 7.22386 11 7.5 11C7.77614 11 8 11.2239 8 11.5C8 11.7761 7.77614 12 7.5 12C7.22386 12 7 11.7761 7 11.5ZM9.5 11C9.22386 11 9 11.2239 9 11.5C9 11.7761 9.22386 12 9.5 12C9.77614 12 10 11.7761 10 11.5C10 11.2239 9.77614 11 9.5 11Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg> September 17, 2025</span> <h2 class="dark:text-mint-50 text-blacktext text-3xl font-bold text-pretty">KL散度</h2> <div class="flex justify-start items-center gap-2 text-blacktext dark:text-mint-50 "> <span class="font-medium tracking-wider">Read more </span> <svg width="1em" height="1em" viewBox="0 0 15 15" class="size-4 rotate-180" data-icon="arrow-left">   <use href="#ai:local:arrow-left"></use>  </svg> </div> </a> <div class="gap-3 mt-3 flex flex-col"> <div class="flex gap-2 flex-wrap"> <a class="cursor-pointer" href="/blog/techs/markdown" aria-label="View articles about Markdown" role="link"><span class="flex items-center w-fit pl-2 pr-2 py-0.5 gap-1 text-sm font-semibold leading-3 bg-white shadow rounded-full transition-all duration-300 ease-in-out hover:bg-zinc-800 hover:text-white max-sm:pl-1 max-sm:pr-1.5 max-sm:text-xs max-sm:gap-0.5 text-base" role="presentation" aria-hidden="true"><div class="flex items-center justify-center aspect-square bg-black rounded-full p-1 size-7 max-lg:size-6 max-sm:size-5 " role="img" aria-label="Markdown icon"><svg width="1.63em" height="1em" viewBox="0 0 208 128" class="w-full!" data-icon="markdown">   <use href="#ai:local:markdown"></use>  </svg></div>Markdown</span></a> </div> <div class="gap-2 flex flex-wrap justify-start items-center"> <a href="/blog/tags/machine learning" class="max-md:text-xs text-sm font-medium text-zinc-500 dark:text-neutral-400 hover:text-blacktext transition-all ease-in-out duration-300 px-4 py-1 max-md:px-3 rounded-full bg-mint-950/5 dark:bg-zinc-800 hover:bg-mint-200"> machine learning </a> </div> </div> </div>  </article> </div> <div id="morePosts" class="w-full flex justify-center text-center my-12"> <a href="/blog/posts/" class="font-bold cursor-pointer text-mint-400 dark:text-mint-100 hover:text-mint-500 dark:hover:text-mint-300 transition-all">
View all posts...
</a> </div> </section> </div> </div>  </div> <footer class="relative bottom-0 w-full px-4 py-8 font-medium text-blacktext dark:bg-transparent dark:border-b-2 dark:border-zinc-800 dark:text-zinc-300 max-lg:mt-3" role="contentinfo" aria-label="Site footer"> <nav class="mx-auto flex max-w-7xl flex-row items-center justify-between gap-4 text-xl max-xl:px-6 max-sm:flex-col" aria-label="Footer navigation"> <div class="relative h-6 cursor-pointer before:absolute before:left-1/2 before:top-1/2 before:h-full before:w-[40%] before:-translate-x-1/2 before:-translate-y-1/2 before:rounded-full before:bg-[#50fd8f25] before:blur-3xl before:opacity-80 before:-z-1 hover:text-mint-500 transition-all [text-shadow:0_1px_2px_#000]"> <a href="/" aria-label="Return to homepage"> <!-- This icon represents the logo --> <svg width="80" height="24" viewBox="0 0 80 24" fill="none" xmlns="http://www.w3.org/2000/svg"> <defs> <linearGradient id="logoGradient" x1="0%" y1="0%" x2="100%" y2="0%"> <stop offset="0%" style="stop-color:#38bdf8;stop-opacity:1"></stop> <stop offset="100%" style="stop-color:#3b82f6;stop-opacity:1"></stop> </linearGradient> </defs> <text x="0" y="18" font-family="Arial, sans-serif" font-weight="bold" font-size="16" fill="url(#logoGradient)">
oGYCo
</text> </svg> </a> </div> <div class="flex items-center justify-center gap-5" role="list" aria-label="Social media links"> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="mailto:473815274@qq.com" rel="noopener noreferrer" aria-label="Send email to 473815274@qq.com"> <svg width="1em" height="1em" aria-hidden="true" data-icon="envelope">   <symbol id="ai:local:envelope" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M1 2a1 1 0 0 0-1 1v9a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1zm0 1h13v.925a.45.45 0 0 0-.241.07L7.5 7.967 1.241 3.995A.45.45 0 0 0 1 3.925zm0 1.908V12h13V4.908L7.741 8.88a.45.45 0 0 1-.482 0z" clip-rule="evenodd"/></symbol><use href="#ai:local:envelope"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://instagram.com/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on Instagram"> <svg width="1em" height="1em" aria-hidden="true" data-icon="instagram">   <symbol id="ai:local:instagram" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M12.91 12.909c.326-.327.582-.72.749-1.151.16-.414.27-.886.302-1.578s.04-.915.04-2.68-.008-1.987-.04-2.68c-.032-.692-.142-1.164-.302-1.578a3.2 3.2 0 0 0-.75-1.151 3.2 3.2 0 0 0-1.151-.75c-.414-.16-.886-.27-1.578-.302S9.265 1 7.5 1s-1.987.007-2.68.04c-.692.03-1.164.14-1.578.301a3.2 3.2 0 0 0-1.151.75 3.2 3.2 0 0 0-.75 1.151c-.16.414-.27.886-.302 1.578S1 5.735 1 7.5s.007 1.987.04 2.68c.03.692.14 1.164.301 1.578.164.434.42.826.75 1.151.325.33.718.586 1.151.75.414.16.886.27 1.578.302S5.735 14 7.5 14s1.987-.008 2.68-.04c.692-.03 1.164-.14 1.578-.301a3.3 3.3 0 0 0 1.151-.75M2 6.735v1.53c-.002.821-.002 1.034.02 1.5.026.586.058 1.016.156 1.34.094.312.199.63.543 1.012.344.383.675.556 1.097.684.423.127.954.154 1.415.175.522.024.73.024 1.826.024H8.24c.842.001 1.054.002 1.526-.02.585-.027 1.015-.059 1.34-.156.311-.094.629-.2 1.011-.543.383-.344.556-.676.684-1.098.127-.422.155-.953.176-1.414C13 9.247 13 9.04 13 7.947v-.89c0-1.096 0-1.303-.023-1.826-.021-.461-.049-.992-.176-1.414-.127-.423-.3-.754-.684-1.098-.383-.344-.7-.449-1.011-.543-.325-.097-.755-.13-1.34-.156A27 27 0 0 0 8.24 2H7.057c-1.096 0-1.304 0-1.826.023-.461.021-.992.049-1.415.176-.422.128-.753.301-1.097.684s-.45.7-.543 1.012c-.098.324-.13.754-.156 1.34-.022.466-.022.679-.02 1.5M7.5 5.25a2.25 2.25 0 1 0 0 4.5 2.25 2.25 0 0 0 0-4.5M4.25 7.5a3.25 3.25 0 1 1 6.5 0 3.25 3.25 0 0 1-6.5 0m6.72-2.72a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" clip-rule="evenodd"/></symbol><use href="#ai:local:instagram"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://youtube.com/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on YouTube"> <svg width="1em" height="1em" aria-hidden="true" data-icon="youtube">   <symbol id="ai:local:youtube" viewBox="0 0 15 15"><path fill="currentColor" fill-rule="evenodd" d="M4.764 3.122A33 33 0 0 1 7.5 3c.94 0 1.868.049 2.736.122 1.044.088 1.72.148 2.236.27.47.111.733.258.959.489.024.025.06.063.082.09.2.23.33.518.405 1.062.08.583.082 1.343.082 2.492 0 1.135-.002 1.885-.082 2.46-.074.536-.204.821-.405 1.054l-.083.09c-.23.234-.49.379-.948.487-.507.12-1.168.178-2.194.264-.869.072-1.812.12-2.788.12s-1.92-.048-2.788-.12c-1.026-.086-1.687-.144-2.194-.264-.459-.108-.719-.253-.948-.487l-.083-.09c-.2-.233-.33-.518-.405-1.054C1.002 9.41 1 8.66 1 7.525c0-1.149.002-1.91.082-2.492.075-.544.205-.832.405-1.062.023-.027.058-.065.082-.09.226-.231.489-.378.959-.489.517-.122 1.192-.182 2.236-.27M0 7.525c0-2.242 0-3.363.73-4.208.036-.042.085-.095.124-.135.78-.799 1.796-.885 3.826-1.056C5.57 2.05 6.527 2 7.5 2s1.93.05 2.82.126c2.03.171 3.046.257 3.826 1.056.039.04.087.093.124.135.73.845.73 1.966.73 4.208 0 2.215 0 3.323-.731 4.168a3 3 0 0 1-.125.135c-.781.799-1.778.882-3.773 1.048C9.48 12.951 8.508 13 7.5 13s-1.98-.05-2.87-.124c-1.996-.166-2.993-.25-3.774-1.048a3 3 0 0 1-.125-.135C0 10.848 0 9.74 0 7.525m5.25-2.142a.25.25 0 0 1 .35-.23l4.828 2.118c.2.088.2.37 0 .458L5.6 9.846a.25.25 0 0 1-.35-.229z" clip-rule="evenodd"/></symbol><use href="#ai:local:youtube"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://github.com/oGYCo" rel="noopener noreferrer" aria-label="Visit oGYCo on GitHub"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="github">   <use href="#ai:local:github"></use>  </svg> </a> <a class="hover:text-mint-300 hover:scale-150 transition-all" target="_blank" href="https://linkedin.com/in/placeholder" rel="noopener noreferrer" aria-label="Visit oGYCo on LinkedIn"> <svg width="1em" height="1em" viewBox="0 0 15 15" aria-hidden="true" data-icon="linkedin">   <use href="#ai:local:linkedin"></use>  </svg> </a> </div> </nav> </footer> <script type="module">document.querySelector(".hamburger").addEventListener("click",()=>{const e=document.querySelector(".nav-links"),t=document.querySelector(".hamburger");e.classList.toggle("expanded"),t.classList.toggle("active")});</script> </body> </html>